{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "534fd815-3b10-4304-bf5b-b7faa4c04dfc",
   "metadata": {},
   "source": [
    "### Phase-4: Model Loading and Inference Testing\n",
    "\n",
    "Load the fine-tuned model from shared storage and test it to verify training worked:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cd99a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers>=4.36.0\n",
    "%pip install peft>=0.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "050bd15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model paths in shared storage\n",
    "trained_model_path = \"/opt/app-root/src/shared/models/granite-3.1-2b-instruct-synthetic2\"\n",
    "base_model_cache_path = \"/opt/app-root/src/shared/huggingface_cache/hub/models--ibm-granite--granite-3.1-2b-instruct\"\n",
    "base_model_name = \"ibm-granite/granite-3.1-2b-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b217e739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models for comparison...\n",
      "Using device: cuda\n",
      "Loading tokenizer...\n",
      "   Using cached model at: /opt/app-root/src/shared/huggingface_cache/hub/models--ibm-granite--granite-3.1-2b-instruct/snapshots/bbc2aed595bd38bd770263dc3ab831db9794441d\n",
      "Tokenizer loaded from local cache\n",
      "\n",
      "1. Loading original untrained model...\n",
      "   Loading from cached path: /opt/app-root/src/shared/huggingface_cache/hub/models--ibm-granite--granite-3.1-2b-instruct/snapshots/bbc2aed595bd38bd770263dc3ab831db9794441d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model loaded from local cache\n"
     ]
    }
   ],
   "source": [
    "# Load both original and fine-tuned models from shared storage\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import os\n",
    "import glob\n",
    "\n",
    "print(\"Loading models for comparison...\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Function to find the actual model path in HuggingFace cache\n",
    "def find_model_snapshot_path(cache_path):\n",
    "    \"\"\"Find the actual model files in HuggingFace cache structure\"\"\"\n",
    "    if not os.path.exists(cache_path):\n",
    "        return None\n",
    "    \n",
    "    # Look for snapshots directory\n",
    "    snapshots_dir = os.path.join(cache_path, \"snapshots\")\n",
    "    if not os.path.exists(snapshots_dir):\n",
    "        return None\n",
    "    \n",
    "    # Get the latest snapshot (usually there's only one)\n",
    "    snapshot_dirs = [d for d in os.listdir(snapshots_dir) if os.path.isdir(os.path.join(snapshots_dir, d))]\n",
    "    if not snapshot_dirs:\n",
    "        return None\n",
    "    \n",
    "    # Use the first (or only) snapshot\n",
    "    snapshot_path = os.path.join(snapshots_dir, snapshot_dirs[0])\n",
    "    \n",
    "    # Verify it contains model files\n",
    "    if os.path.exists(os.path.join(snapshot_path, \"config.json\")):\n",
    "        return snapshot_path\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Load tokenizer from local cache\n",
    "print(f\"Loading tokenizer...\")\n",
    "local_model_path = find_model_snapshot_path(base_model_cache_path)\n",
    "\n",
    "if local_model_path:\n",
    "    print(f\"   Using cached model at: {local_model_path}\")\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            local_model_path,\n",
    "            trust_remote_code=True,\n",
    "            local_files_only=True\n",
    "        )\n",
    "        print(\"Tokenizer loaded from local cache\")\n",
    "    except Exception as e:\n",
    "        print(f\"   Cache loading failed: {e}\")\n",
    "        print(\"   Falling back to HuggingFace download...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            base_model_name,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        print(\"Tokenizer loaded from HuggingFace\")\n",
    "else:\n",
    "    print(\"   Local cache not found, downloading from HuggingFace...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        base_model_name,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    print(\"Tokenizer loaded from HuggingFace\")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 1. Load original untrained model from local cache\n",
    "print(\"\\n1. Loading original untrained model...\")\n",
    "\n",
    "if local_model_path:\n",
    "    print(f\"   Loading from cached path: {local_model_path}\")\n",
    "    try:\n",
    "        original_model = AutoModelForCausalLM.from_pretrained(\n",
    "            local_model_path,\n",
    "            dtype=torch.bfloat16 if device == \"cuda\" else torch.float32,\n",
    "            device_map=\"auto\" if device == \"cuda\" else None,\n",
    "            trust_remote_code=True,\n",
    "            local_files_only=True\n",
    "        )\n",
    "        print(\"Original model loaded from local cache\")\n",
    "    except Exception as e:\n",
    "        print(f\"Cache loading failed: {e}\")\n",
    "        print(\"Falling back to HuggingFace download...\")\n",
    "        try:\n",
    "            original_model = AutoModelForCausalLM.from_pretrained(\n",
    "                base_model_name,\n",
    "                dtype=torch.bfloat16 if device == \"cuda\" else torch.float32,\n",
    "                device_map=\"auto\" if device == \"cuda\" else None,\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            print(\"Original model loaded from HuggingFace\")\n",
    "        except Exception as e2:\n",
    "            print(f\"Failed to load original model: {e2}\")\n",
    "            original_model = None\n",
    "else:\n",
    "    print(\"Local cache not found, downloading from HuggingFace...\")\n",
    "    try:\n",
    "        original_model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_name,\n",
    "            dtype=torch.bfloat16 if device == \"cuda\" else torch.float32,\n",
    "            device_map=\"auto\" if device == \"cuda\" else None,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        print(\"Original model loaded from HuggingFace\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load original model: {e}\")\n",
    "        original_model = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90ffbb12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Loading fine-tuned model...\n",
      "   Checking path: /opt/app-root/src/shared/models/granite-3.1-2b-instruct-synthetic2\n",
      "   Fine-tuned model path exists\n",
      "   Loading base model from cache for LoRA...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned model loaded from: /opt/app-root/src/shared/models/granite-3.1-2b-instruct-synthetic2\n",
      "LoRA weights merged successfully\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2. Load fine-tuned model with LoRA adapter\n",
    "print(\"\\n2. Loading fine-tuned model...\")\n",
    "print(f\"   Checking path: {trained_model_path}\")\n",
    "\n",
    "if os.path.exists(trained_model_path):\n",
    "    print(\"   Fine-tuned model path exists\")\n",
    "    try:\n",
    "        # Load base model for LoRA (use same logic as original model)\n",
    "        if local_model_path:\n",
    "            print(\"   Loading base model from cache for LoRA...\")\n",
    "            base_for_lora = AutoModelForCausalLM.from_pretrained(\n",
    "                local_model_path,\n",
    "                torch_dtype=torch.bfloat16 if device == \"cuda\" else torch.float32,\n",
    "                device_map=\"auto\" if device == \"cuda\" else None,\n",
    "                trust_remote_code=True,\n",
    "                local_files_only=True\n",
    "            )\n",
    "        else:\n",
    "            print(\"   Loading base model from HuggingFace for LoRA...\")\n",
    "            base_for_lora = AutoModelForCausalLM.from_pretrained(\n",
    "                base_model_name,\n",
    "                torch_dtype=torch.bfloat16 if device == \"cuda\" else torch.float32,\n",
    "                device_map=\"auto\" if device == \"cuda\" else None,\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "        \n",
    "        # Load LoRA adapter and merge\n",
    "        trained_model = PeftModel.from_pretrained(base_for_lora, trained_model_path)\n",
    "        trained_model = trained_model.merge_and_unload()  # Merge LoRA weights for inference\n",
    "        \n",
    "        print(f\"Fine-tuned model loaded from: {trained_model_path}\")\n",
    "        print(\"LoRA weights merged successfully\")\n",
    "        \n",
    "        # Clean up base model used for LoRA loading\n",
    "        del base_for_lora\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading fine-tuned model: {e}\")\n",
    "        trained_model = None\n",
    "        \n",
    "else:\n",
    "    print(f\"Fine-tuned model not found at: {trained_model_path}\")\n",
    "    print(\"Training may not have completed or path is incorrect\")\n",
    "    trained_model = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d4c2d675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Models loaded successfully!\n",
      "   Original model: Ready\n",
      "   Fine-tuned model: Ready\n",
      "   Device: cuda\n",
      "   Cache path used: /opt/app-root/src/shared/huggingface_cache/hub/models--ibm-granite--granite-3.1-2b-instruct/snapshots/bbc2aed595bd38bd770263dc3ab831db9794441d\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nModels loaded successfully!\")\n",
    "print(f\"   Original model: {'Ready' if original_model else 'Not available'}\")\n",
    "print(f\"   Fine-tuned model: {'Ready' if trained_model else 'Not available'}\")\n",
    "print(f\"   Device: {device}\")\n",
    "print(f\"   Cache path used: {local_model_path if local_model_path else 'HuggingFace download'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e846ff92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 20 test samples\n",
      "   Source: /opt/app-root/src/shared/synthetic_data_v2/synthetic_dataset.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "import re\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def generate_response(question: str, model, tokenizer, max_length: int = 512) -> str:\n",
    "    \"\"\"Generate response using the model\"\"\"\n",
    "    # Format as chat message\n",
    "    messages = [{\"role\": \"user\", \"content\": question}]\n",
    "    \n",
    "    # Apply chat template\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_length)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # Decode response (remove input prompt)\n",
    "    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "    return response.strip()\n",
    "\n",
    "def load_test_data(file_path: str, num_samples: int = 5) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Load test samples from synthetic dataset\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Take a subset for testing\n",
    "        if len(data) > num_samples:\n",
    "            # Take samples from different parts of the dataset\n",
    "            step = len(data) // num_samples\n",
    "            test_samples = [data[i] for i in range(0, len(data), step)][:num_samples]\n",
    "        else:\n",
    "            test_samples = data[:num_samples]\n",
    "            \n",
    "        return test_samples\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading test data: {e}\")\n",
    "        return []\n",
    "\n",
    "def extract_final_number(text: str) -> str:\n",
    "    \"\"\"Extract the final numerical answer from text\"\"\"\n",
    "    numbers = re.findall(r'\\d+', text)\n",
    "    return numbers[-1] if numbers else None\n",
    "\n",
    "def evaluate_accuracy(expected: str, response: str) -> bool:\n",
    "    \"\"\"Simple accuracy check based on final number\"\"\"\n",
    "    expected_num = extract_final_number(expected)\n",
    "    response_num = extract_final_number(response)\n",
    "    \n",
    "    if expected_num and response_num:\n",
    "        return expected_num == response_num\n",
    "    return False\n",
    "\n",
    "test_data_path = \"/opt/app-root/src/shared/synthetic_data_v2/synthetic_dataset.json\"\n",
    "test_samples = load_test_data(test_data_path, num_samples=20)\n",
    "\n",
    "if test_samples:\n",
    "    print(f\"Loaded {len(test_samples)} test samples\")\n",
    "    print(f\"   Source: {test_data_path}\")\n",
    "else:\n",
    "    print(\"No test data available\")\n",
    "    # Create fallback test samples\n",
    "    test_samples = [\n",
    "        {\n",
    "            \"question\": \"A bakery sold 45 cupcakes in the morning and 38 cupcakes in the afternoon. How many cupcakes did they sell in total?\",\n",
    "            \"answer\": \"The bakery sold 45 + 38 = 83 cupcakes in total.\",\n",
    "            \"source\": \"fallback\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"Sarah has 24 stickers. She gives 8 stickers to her friend and buys 15 more stickers. How many stickers does Sarah have now?\",\n",
    "            \"answer\": \"Sarah had 24 stickers, gave away 8, so she had 24 - 8 = 16 stickers. Then she bought 15 more, so she has 16 + 15 = 31 stickers now.\",\n",
    "            \"source\": \"fallback\"\n",
    "        }\n",
    "    ]\n",
    "    print(f\"Using {len(test_samples)} fallback test samples\")\n",
    "print(test_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d17dd825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PERFORMANCE COMPARISON SUMMARY\n",
      "==================================================\n",
      "Original Model Performance:\n",
      "   Correct Answers: 7/20\n",
      "   Accuracy: 35.0%\n",
      "\n",
      "Fine-tuned Model Performance:\n",
      "   Correct Answers: 10/20\n",
      "   Accuracy: 50.0%\n",
      "\n",
      "Training Impact:\n",
      "   Accuracy Change: +15.0%\n",
      "   Good! Training showed positive results\n",
      "\n",
      "DETAILED ANALYSIS\n",
      "========================================\n",
      "Key improvements to look for in fine-tuned model:\n",
      "   Step-by-step mathematical reasoning\n",
      "   Correct arithmetic calculations\n",
      "   Clear explanation of the process\n",
      "   Consistent answer format\n",
      "   Better handling of word problems\n",
      "\n",
      "CLEANING UP RESOURCES\n",
      "==============================\n",
      "Original model cleared from memory\n",
      "Fine-tuned model cleared from memory\n",
      "Tokenizer cleared from memory\n",
      "GPU memory cache cleared\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SIDE-BY-SIDE MODEL COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if original_model and tokenizer:\n",
    "    original_correct = 0\n",
    "    trained_correct = 0\n",
    "    \n",
    "    for i, sample in enumerate(test_samples, 1):\n",
    "        question = sample[\"question\"]\n",
    "        expected_answer = sample[\"answer\"]\n",
    "        \n",
    "        print(f\"\\nTest Problem {i}/{len(test_samples)}:\")\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Expected: {expected_answer}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Test original model\n",
    "        print(\"ORIGINAL MODEL (Untrained):\")\n",
    "        try:\n",
    "            original_response = generate_response(question, original_model, tokenizer)\n",
    "            print(f\"   Response: {original_response}\")\n",
    "            \n",
    "            # Check accuracy\n",
    "            is_correct = evaluate_accuracy(expected_answer, original_response)\n",
    "            if is_correct:\n",
    "                original_correct += 1\n",
    "                print(\"   Correct\")\n",
    "            else:\n",
    "                print(\"   Incorrect\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   Error: {e}\")\n",
    "        \n",
    "        print()\n",
    "        \n",
    "        # Test fine-tuned model\n",
    "        if trained_model:\n",
    "            print(\"FINE-TUNED MODEL (After Training):\")\n",
    "            try:\n",
    "                trained_response = generate_response(question, trained_model, tokenizer)\n",
    "                print(f\"   Response: {trained_response}\")\n",
    "                \n",
    "                # Check accuracy\n",
    "                is_correct = evaluate_accuracy(expected_answer, trained_response)\n",
    "                if is_correct:\n",
    "                    trained_correct += 1\n",
    "                    print(\"   Correct\")\n",
    "                else:\n",
    "                    print(\"   Incorrect\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"   Error: {e}\")\n",
    "        else:\n",
    "            print(\"FINE-TUNED MODEL: Not available\")\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "    \n",
    "    print(f\"\\nPERFORMANCE COMPARISON SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    total_samples = len(test_samples)\n",
    "    original_accuracy = (original_correct / total_samples) * 100\n",
    "    \n",
    "    print(f\"Original Model Performance:\")\n",
    "    print(f\"   Correct Answers: {original_correct}/{total_samples}\")\n",
    "    print(f\"   Accuracy: {original_accuracy:.1f}%\")\n",
    "    \n",
    "    if trained_model:\n",
    "        trained_accuracy = (trained_correct / total_samples) * 100\n",
    "        improvement = trained_accuracy - original_accuracy\n",
    "        \n",
    "        print(f\"\\nFine-tuned Model Performance:\")\n",
    "        print(f\"   Correct Answers: {trained_correct}/{total_samples}\")\n",
    "        print(f\"   Accuracy: {trained_accuracy:.1f}%\")\n",
    "        \n",
    "        print(f\"\\nTraining Impact:\")\n",
    "        print(f\"   Accuracy Change: {improvement:+.1f}%\")\n",
    "        \n",
    "        if improvement > 20:\n",
    "            print(\"   Excellent! Training significantly improved performance\")\n",
    "        elif improvement > 0:\n",
    "            print(\"   Good! Training showed positive results\")\n",
    "        elif improvement == 0:\n",
    "            print(\"   No change. Consider adjusting training parameters\")\n",
    "        else:\n",
    "            print(\"   Performance decreased. Check training setup\")\n",
    "    else:\n",
    "        print(f\"\\nFine-tuned Model: Not available for comparison\")\n",
    "    \n",
    "    # ============================================================================\n",
    "    # DETAILED ANALYSIS\n",
    "    # ============================================================================\n",
    "    \n",
    "    print(f\"\\nDETAILED ANALYSIS\")\n",
    "    print(\"=\" * 40)\n",
    "    print(\"Key improvements to look for in fine-tuned model:\")\n",
    "    print(\"   Step-by-step mathematical reasoning\")\n",
    "    print(\"   Correct arithmetic calculations\")\n",
    "    print(\"   Clear explanation of the process\")\n",
    "    print(\"   Consistent answer format\")\n",
    "    print(\"   Better handling of word problems\")\n",
    "    \n",
    "else:\n",
    "    print(\"Cannot run comparison - models not loaded\")\n",
    "\n",
    "# ============================================================================\n",
    "# CLEANUP\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\nCLEANING UP RESOURCES\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Clear models from memory\n",
    "if 'original_model' in locals() and original_model:\n",
    "    del original_model\n",
    "    print(\"Original model cleared from memory\")\n",
    "\n",
    "if 'trained_model' in locals() and trained_model:\n",
    "    del trained_model\n",
    "    print(\"Fine-tuned model cleared from memory\")\n",
    "\n",
    "if 'tokenizer' in locals():\n",
    "    del tokenizer\n",
    "    print(\"Tokenizer cleared from memory\")\n",
    "\n",
    "# Clear CUDA cache if using GPU\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"GPU memory cache cleared\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
