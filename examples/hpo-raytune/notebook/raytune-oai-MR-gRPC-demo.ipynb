{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70693f27-63b4-42e2-a9e4-0467bc25e146",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import pieces from codeflare-sdk\n",
    "from codeflare_sdk import Cluster, ClusterConfiguration, TokenAuthentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2c9eff-4c36-44d2-af5e-5e083fd02b08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create authentication object for user permissions\n",
    "# IF unused, SDK will automatically check for default kubeconfig, then in-cluster config\n",
    "# KubeConfigFileAuthentication can also be used to specify kubeconfig path manually\n",
    "auth = TokenAuthentication(\n",
    "    token = 'TOKEN',\n",
    "    server = 'SERVER',\n",
    "    skip_tls=True\n",
    ")\n",
    "auth.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cfa953-b068-49d9-bc68-6fa0c8d9bf22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create and configure our cluster object (and appwrapper)\n",
    "# NOTE: If running outside of RHOAI notebooks, add the following line to the cluster configuration:\n",
    "# namespace=\"rhods-notebooks\"\n",
    "cluster_name='terrestrial-raytest'\n",
    "cluster = Cluster(ClusterConfiguration(\n",
    "    name=cluster_name,\n",
    "    num_workers=2,\n",
    "    worker_cpu_requests=1,\n",
    "    worker_cpu_limits=1,\n",
    "    worker_memory_requests=4,\n",
    "    worker_memory_limits=4,\n",
    "    head_extended_resource_requests={'nvidia.com/gpu':0},\n",
    "    worker_extended_resource_requests={'nvidia.com/gpu':0},\n",
    "    image='quay.io/modh/ray:2.35.0-py311-cu121'\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc0a8f1-ab8b-4afa-81b7-47592a359413",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Bring up the cluster\n",
    "cluster.apply()\n",
    "cluster.wait_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238b81fb-01f2-4225-936d-81568b44bf90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster.status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1a5c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from codeflare_sdk import generate_cert\n",
    "# Create required TLS cert and export the environment variables to enable TLS\n",
    "generate_cert.generate_tls_cert(cluster_name, cluster.config.namespace)\n",
    "generate_cert.export_env(cluster_name, cluster.config.namespace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c30fb0-01fd-417b-811c-f97070f367b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy\n",
    "!pip install torch\n",
    "!pip install ray[default]==2.35.0\n",
    "!pip install onnxruntime\n",
    "!pip install ml_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc5a9720-4938-454b-979f-8bf1bb748090",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "ray_cluster_uri = cluster.cluster_uri()\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c01f36e-465d-4a4f-9a2c-25be8d8b6c4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Additional libs\n",
    "runtime_env = {\"pip\": [\"ipython\", \"torch\" , \"onnx\", \"ray[train]\", \"ml_metadata\" ,\"protobuf\"]}\n",
    "\n",
    "ray.init(address=ray_cluster_uri, runtime_env=runtime_env,_system_config={\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\": \"python\"} )\n",
    "\n",
    "print(\"Ray cluster is up and running: \", ray.is_initialized())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50076c0b-e09b-4b3a-8653-46b60dffa728",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Create all the required metadata types required for HPO PoC\n",
    "from grpc import insecure_channel\n",
    "from ml_metadata.proto import metadata_store_pb2\n",
    "from ml_metadata.proto import metadata_store_service_pb2\n",
    "from ml_metadata.proto import metadata_store_service_pb2_grpc\n",
    "\n",
    "channel = insecure_channel(\"modelregistry-sample:9090\")\n",
    "mds = metadata_store_service_pb2_grpc.MetadataStoreServiceStub(channel)\n",
    "\n",
    "def create_mdtypes():\n",
    "        # Create ArtifactTypes for HPOConfig\n",
    "        hpo_config = metadata_store_pb2.ArtifactType()\n",
    "        hpo_config.name = \"odh.HPOConfig\"\n",
    "        request = metadata_store_service_pb2.PutArtifactTypeRequest()\n",
    "        request.all_fields_match = True\n",
    "        request.artifact_type.CopyFrom(hpo_config)\n",
    "        response = mds.PutArtifactType(request)\n",
    "        hpo_config_type_id = response.type_id\n",
    "        \n",
    "        # Create a ContextType for HPO experiment\n",
    "        experiment_type = metadata_store_pb2.ContextType()\n",
    "        experiment_type.name = \"odh.HPOExperiment\"\n",
    "        request = metadata_store_service_pb2.PutContextTypeRequest()\n",
    "        request.context_type.CopyFrom(experiment_type)\n",
    "        response = mds.PutContextType(request)\n",
    "        hpo_experiment_type_id = response.type_id\n",
    "\n",
    "        # Create a ContextType for HPOTrial\n",
    "        trial_type = metadata_store_pb2.ContextType()\n",
    "        trial_type.name = \"odh.HPOTrial\"\n",
    "        trial_type.properties[\"trial_id\"] = metadata_store_pb2.STRING\n",
    "        request = metadata_store_service_pb2.PutContextTypeRequest()\n",
    "        request.context_type.CopyFrom(trial_type)\n",
    "        response = mds.PutContextType(request)\n",
    "        hpo_trial_type_id = response.type_id\n",
    "        \n",
    "        data_type = metadata_store_pb2.ArtifactType()\n",
    "        data_type.name = \"odh.Dataset\"\n",
    "        request = metadata_store_service_pb2.PutArtifactTypeRequest()\n",
    "        request.all_fields_match = True\n",
    "        request.artifact_type.CopyFrom(data_type)\n",
    "        response = mds.PutArtifactType(request)\n",
    "        data_type_id = response.type_id\n",
    "        \n",
    "        #  Register execution types for all steps in the ML workflow\n",
    "        # Create an ExecutionType, Trainer\n",
    "        trainer_type = metadata_store_pb2.ExecutionType()\n",
    "        trainer_type.name = \"odh.Train\"\n",
    "        request = metadata_store_service_pb2.PutExecutionTypeRequest()\n",
    "        request.execution_type.CopyFrom(trainer_type)\n",
    "        response = mds.PutExecutionType(request)\n",
    "        trainer_type_id = response.type_id\n",
    "        \n",
    "        data_type = metadata_store_pb2.ArtifactType()\n",
    "        data_type.name = \"odh.Metrics\"\n",
    "        request = metadata_store_service_pb2.PutArtifactTypeRequest()\n",
    "        request.all_fields_match = True\n",
    "        request.artifact_type.CopyFrom(data_type)\n",
    "        response = mds.PutArtifactType(request)\n",
    "        metrics_type_id = response.type_id\n",
    "\n",
    "        return hpo_config_type_id, hpo_experiment_type_id, hpo_trial_type_id, data_type_id, trainer_type_id, metrics_type_id\n",
    "    \n",
    "hpo_config_type_id, hpo_experiment_type_id, hpo_trial_type_id, data_type_id, trainer_type_id, metrics_type_id = create_mdtypes()\n",
    "print(hpo_config_type_id, hpo_experiment_type_id, hpo_trial_type_id, data_type_id, trainer_type_id, metrics_type_id)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7728142-e8f1-4100-a301-cdb5452ca26b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import ray\n",
    "from ray import tune\n",
    "import time\n",
    "import os\n",
    "\n",
    "from grpc import insecure_channel\n",
    "from ml_metadata.proto import metadata_store_pb2\n",
    "from ml_metadata.proto import metadata_store_service_pb2\n",
    "from ml_metadata.proto import metadata_store_service_pb2_grpc\n",
    "\n",
    "os.environ['RAY_PICKLE_VERBOSE_DEBUG'] = '1'\n",
    "\n",
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def get_metadata_store():\n",
    "    channel = insecure_channel(\"modelregistry-sample:9090\")\n",
    "    store = metadata_store_service_pb2_grpc.MetadataStoreServiceStub(channel)\n",
    "    yield store\n",
    "\n",
    "\n",
    "'''\n",
    "HPOExperiment: context type\n",
    "HPOTrial: context type\n",
    "HPOConfig: artifact type\n",
    "DataSet: artifact type\n",
    "Model: artifact type\n",
    "Metrics: artifact type\n",
    "'''\n",
    "\n",
    "global data_type_id , model_type_id , trainer_type_id , metrics_type_id\n",
    "global hpo_config_type_id, hpo_experiment_type_id, hpo_trial_type_id\n",
    "\n",
    "raytune_experiment_name = \"raytune_hyperparameter_tuning\"  #Replace the experiment name for a new run\n",
    "        \n",
    "def add_hpo_experiment(hpo_experiment_name):\n",
    "    with get_metadata_store() as mds:\n",
    "        # Create a Context for the experiment \n",
    "        experiment_context = metadata_store_pb2.Context()\n",
    "        experiment_context.type_id = hpo_experiment_type_id\n",
    "        experiment_context.name = hpo_experiment_name\n",
    "        request = metadata_store_service_pb2.PutContextsRequest()\n",
    "        request.contexts.extend([experiment_context])\n",
    "        response = mds.PutContexts(request)\n",
    "        experiment_id = response.context_ids[0]\n",
    "        return experiment_id\n",
    "\n",
    "def log_inputs(input_size,output_size,config,trial_name,trial_id,exp_name):\n",
    "    with get_metadata_store() as metadata_store0:\n",
    "        # Register the Execution of a Trainer run\n",
    "        trainer_run = metadata_store_pb2.Execution()\n",
    "        trainer_run.type_id = trainer_type_id\n",
    "        trainer_run.custom_properties[\"trainer_state\"].string_value = \"RUNNING\"\n",
    "        request = metadata_store_service_pb2.PutExecutionsRequest()\n",
    "        request.executions.extend([trainer_run])\n",
    "        response = metadata_store0.PutExecutions(request)\n",
    "        run_id = response.execution_ids[0]\n",
    "        \n",
    "        # Log metadata about the dataset\n",
    "        dataset_artifact = metadata_store_pb2.Artifact()\n",
    "        dataset_artifact.type_id = data_type_id\n",
    "        dataset_artifact.uri = \"\"  # Replace with the actual path or identifier\n",
    "        dataset_artifact.custom_properties[\"ds_input_size\"].int_value = input_size\n",
    "        dataset_artifact.custom_properties[\"ds_output_size\"].int_value = output_size\n",
    "        \n",
    "        # Log metadata about the hyperparameter tuning\n",
    "        hpo_config_artifact = metadata_store_pb2.Artifact()\n",
    "        hpo_config_artifact.type_id = hpo_config_type_id\n",
    "        hpo_config_artifact.uri = \"\"  # Replace with the actual path or identifier\n",
    "        hpo_config_artifact.custom_properties[\"hp_hidden_size\"].int_value = config[\"hidden_size\"]\n",
    "        hpo_config_artifact.custom_properties[\"hp_lr\"].double_value = config[\"lr\"]\n",
    "        hpo_config_artifact.custom_properties[\"trial_name\"].string_value = trial_name\n",
    "        hpo_config_artifact.custom_properties[\"trial_id\"].string_value = trial_id\n",
    "        hpo_config_artifact.custom_properties[\"experiment_name\"].string_value = exp_name\n",
    "        request = metadata_store_service_pb2.PutArtifactsRequest()\n",
    "        request.artifacts.extend([dataset_artifact, hpo_config_artifact])\n",
    "        response = metadata_store0.PutArtifacts(request)\n",
    "        dataset_artifact_id = response.artifact_ids[0]\n",
    "        hpo_config_artifact_id = response.artifact_ids[1]   \n",
    "\n",
    "        return run_id, trainer_run, dataset_artifact_id, hpo_config_artifact_id\n",
    "    \n",
    "# Update the raytune_experiment_name as parent context for the trials.\n",
    "hpo_experiment_id = add_hpo_experiment(raytune_experiment_name)\n",
    "        \n",
    "def log_output_artifacts(accuracy,run_id,trainer_run,trial_name, trial_id, dataset_artifact_id, hpo_config_artifact_id):\n",
    "    with get_metadata_store() as metadata_store:\n",
    "        experiment_contexts = []\n",
    "        # Log metadata about the model\n",
    "        model_artifact = metadata_store_pb2.Artifact()\n",
    "        model_artifact.type_id = model_type_id\n",
    "        model_artifact.uri = \"s3://hpomodels\"  # Replace with the actual path or identifier\n",
    "        model_artifact.custom_properties[\"model_name\"].string_value = \"SNN\"\n",
    "        request = metadata_store_service_pb2.PutArtifactsRequest()\n",
    "        request.artifacts.extend([model_artifact])\n",
    "        response = metadata_store.PutArtifacts(request)\n",
    "        model_artifact_id = response.artifact_ids[0]\n",
    "        \n",
    "        # Log metrics\n",
    "        metrics_artifact = metadata_store_pb2.Artifact()\n",
    "        metrics_artifact.type_id = metrics_type_id\n",
    "        metrics_artifact.custom_properties[\"accuracy\"].double_value = accuracy\n",
    "        request = metadata_store_service_pb2.PutArtifactsRequest()\n",
    "        request.artifacts.extend([metrics_artifact])\n",
    "        response = metadata_store.PutArtifacts(request)\n",
    "        metrics_artifact_id = response.artifact_ids[0]\n",
    "        \n",
    "        # Log Execution trainer state\n",
    "        trainer_run.id = run_id\n",
    "        trainer_run.custom_properties[\"trainer_state\"].string_value = \"COMPLETED\"\n",
    "        request = metadata_store_service_pb2.PutExecutionsRequest()\n",
    "        request.executions.extend([trainer_run])\n",
    "        response = metadata_store.PutExecutions(request)\n",
    "        run_id = response.execution_ids[0]\n",
    "\n",
    "        # Create a Context for the trial\n",
    "        trial_context = metadata_store_pb2.Context()\n",
    "        trial_context.type_id = hpo_trial_type_id\n",
    "        trial_context.name = trial_name # Use the trial name as the context name\n",
    "        trial_context.properties[\"trial_id\"].string_value = trial_id\n",
    "        request = metadata_store_service_pb2.PutContextsRequest()\n",
    "        request.contexts.extend([trial_context])\n",
    "        response = metadata_store.PutContexts(request)\n",
    "        context_trial_id = response.context_ids[0]\n",
    "        \n",
    "        # Associate the trial context with the experiment context\n",
    "        association = metadata_store_pb2.Association()\n",
    "        association.execution_id = run_id\n",
    "        association.context_id = context_trial_id\n",
    "        \n",
    "        attribution = metadata_store_pb2.Attribution()\n",
    "        attribution.artifact_id = model_artifact_id\n",
    "        attribution.context_id = context_trial_id\n",
    "        \n",
    "        attribution_hpo = metadata_store_pb2.Attribution()\n",
    "        attribution_hpo.artifact_id = hpo_config_artifact_id\n",
    "        attribution_hpo.context_id = context_trial_id\n",
    "        \n",
    "        attribution_metrics = metadata_store_pb2.Attribution()\n",
    "        attribution_metrics.artifact_id = metrics_artifact_id\n",
    "        attribution_metrics.context_id = context_trial_id\n",
    "        \n",
    "        request = metadata_store_service_pb2.PutAttributionsAndAssociationsRequest()\n",
    "        request.attributions.add().CopyFrom(attribution)\n",
    "        request.attributions.add().CopyFrom(attribution_hpo)\n",
    "        request.attributions.add().CopyFrom(attribution_metrics)\n",
    "        request.associations.add().CopyFrom(association)\n",
    "        response = metadata_store.PutAttributionsAndAssociations(request)\n",
    "\n",
    "        # Create a ParentContext to associate the trial context with the experiment context\n",
    "        experiment_context_obj = metadata_store_pb2.ParentContext()\n",
    "        experiment_context_obj.child_id = context_trial_id\n",
    "        experiment_context_obj.parent_id = hpo_experiment_id\n",
    "\n",
    "        request = metadata_store_service_pb2.PutParentContextsRequest()\n",
    "        request.parent_contexts.extend([experiment_context_obj])\n",
    "        response = metadata_store.PutParentContexts(request)\n",
    "\n",
    "\n",
    "# Define a simple neural network\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Define a function to train and evaluate the model\n",
    "def train_evaluate(config): \n",
    "    input_size = 10\n",
    "    output_size = 1\n",
    "\n",
    "    # Instantiate the neural network with the hyperparameters\n",
    "    model = SimpleNet(input_size, config[\"hidden_size\"], output_size)\n",
    "\n",
    "    # Define a dummy dataset for illustration purposes\n",
    "    X = torch.randn(100, input_size)\n",
    "    y = torch.randn(100, output_size)\n",
    "\n",
    "    # Dummy DataLoader\n",
    "    dataset = TensorDataset(X, y)\n",
    "    dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "    trial_id = ray.train.get_context().get_trial_id()\n",
    "    trial_name = ray.train.get_context().get_trial_name()\n",
    "    trial_dir = ray.train.get_context().get_trial_dir()\n",
    "    exp_name = ray.train.get_context().get_experiment_name()\n",
    "    mtdata = ray.train.get_context().get_metadata()\n",
    "\n",
    "    # Log input artfacts, events , Executions\n",
    "    run_id, trainer_run, dataset_artifact_id, hpo_config_artifact_id = log_inputs(input_size,output_size,config,trial_name,trial_id,exp_name)\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(10):  # Adjust as needed\n",
    "        for inputs, targets in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluate the model (for simplicity, just return a dummy accuracy)\n",
    "    accuracy = torch.rand(1).item()\n",
    "\n",
    "    # Log output artifacts, events, Executions\n",
    "    log_output_artifacts(accuracy,run_id,trainer_run,trial_name,trial_id,dataset_artifact_id,hpo_config_artifact_id)\n",
    "\n",
    "    # Return a dictionary containing the accuracy and the model\n",
    "    return {\"accuracy\": accuracy, \"model\": model}\n",
    "\n",
    "\n",
    "def custom_trial_name_creator(trial):\n",
    "    trial_index = trial.trial_id\n",
    "    experiment_name = raytune_experiment_name\n",
    "    trial_name = f\"{experiment_name}_{trial_index}\"    \n",
    "    return trial_name\n",
    "\n",
    "\n",
    "# Define the hyperparameter search space\n",
    "search_space = {\n",
    "    \"hidden_size\": tune.choice([5, 10, 20]),\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "}\n",
    "\n",
    "# Use the default metadata types for Model and other metadata types that are part of the metaData database.\n",
    "# Replace these values based on the MR database using the code from previous block.\n",
    "model_type_id = 12\n",
    "hpo_config_type_id = 17\n",
    "hpo_experiment_type_id = 18\n",
    "hpo_trial_type_id = 19\n",
    "data_type_id = 20\n",
    "trainer_type_id = 21\n",
    "metrics_type_id = 22\n",
    "\n",
    "# Perform hyperparameter tuning with Ray Tune\n",
    "analysis = tune.run(\n",
    "    train_evaluate,\n",
    "    config=search_space,\n",
    "    num_samples=1,  # Number of trials\n",
    "    resources_per_trial={\"cpu\": 1},\n",
    "    name=raytune_experiment_name,\n",
    "    trial_name_creator=custom_trial_name_creator, )\n",
    "\n",
    "\n",
    "# Get the best configuration and result\n",
    "best_trial = analysis.get_best_trial(\"accuracy\", \"max\", \"last\")\n",
    "best_config = best_trial.config\n",
    "best_accuracy = best_trial.last_result[\"accuracy\"]\n",
    "best_model = best_trial.last_result[\"model\"]\n",
    "trial_id = best_trial.trial_id \n",
    "\n",
    "print(f\"Best model from trial ID: {trial_id}\")\n",
    "print(f\"Best hyperparameters: {best_config}\")\n",
    "print(f\"Best accuracy: {best_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3e1db94-e135-4d44-83de-70bd8f6dd106",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "channel = insecure_channel(\"modelregistry-sample:9090\")\n",
    "mdstore = metadata_store_service_pb2_grpc.MetadataStoreServiceStub(channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73738dcc-b97b-4d25-a0a0-fb0126411326",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract unique artifact types from the artifacts\n",
    "request = metadata_store_service_pb2.GetArtifactsRequest()\n",
    "response = mdstore.GetArtifacts(request)\n",
    "unique_artifact_types = set(artifact.type for artifact in response.artifacts)\n",
    "\n",
    "print(\"Available Artifact Types:\")\n",
    "for artifact_type_id in unique_artifact_types:\n",
    "    print(artifact_type_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddec98f-8d3c-43d4-bafd-6c97f40b3a25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract artifact details of given\n",
    "def get_artifact_type(type_name):\n",
    "        request = metadata_store_service_pb2.GetArtifactTypeRequest()\n",
    "        request.type_name = type_name\n",
    "        response = mdstore.GetArtifactType(request)\n",
    "        return response.artifact_type\n",
    "\n",
    "# Example usage:\n",
    "data_type = get_artifact_type(\"odh.HPOConfig\")\n",
    "print(f\"Artifact Type: {data_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa78097-45b4-4ab4-a382-0ff40b5b6a46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract artifact details for an experiment which includes all the trials.\n",
    "request = metadata_store_service_pb2.GetContextsRequest()\n",
    "response = mdstore.GetContexts(request)\n",
    "\n",
    "# Filter the contexts based on the experiment name\n",
    "experiment_context_name = \"raytune_hyperparameter_tuning\"   #Replace the experiment name \n",
    "experiment_context = next(\n",
    "    (context for context in response.contexts if context.name == experiment_context_name),\n",
    "    None\n",
    ")\n",
    "trial_context=\"\"\n",
    "if experiment_context:\n",
    "    # Retrieve all contexts and filter them to find the child contexts (trial contexts)\n",
    "    child_contexts = [\n",
    "        context for context in response.contexts\n",
    "        if context.name.startswith(experiment_context_name) and context.name != experiment_context_name\n",
    "    ]\n",
    "\n",
    "    # For each child context, retrieve the artifacts associated with it\n",
    "    for child_context in child_contexts:\n",
    "        artifacts_request = metadata_store_service_pb2.GetArtifactsByContextRequest(\n",
    "            context_id=child_context.id\n",
    "        )\n",
    "        artifacts_response = mdstore.GetArtifactsByContext(artifacts_request)\n",
    "        trial_context=child_context.name\n",
    "        print(f\"Trial Context: {trial_context}\")\n",
    "        print(f\"Artifacts: {artifacts_response.artifacts}\")\n",
    "else:\n",
    "    print(f\"No context found with the name '{experiment_context_name}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7170acc-f391-4e10-b9ec-aa03deb64603",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Extract artifact details of given trial\n",
    "context_request = metadata_store_service_pb2.GetContextByTypeAndNameRequest()\n",
    "context_request.type_name = \"Trial\"\n",
    "context_request.context_name = trial_context\n",
    "\n",
    "context_response = mdstore.GetContextByTypeAndName(context_request)\n",
    "context_id = context_response.context.id\n",
    "artifacts_request = metadata_store_service_pb2.GetArtifactsByContextRequest()\n",
    "artifacts_request.context_id = context_id\n",
    "artifacts_response = mdstore.GetArtifactsByContext(artifacts_request)\n",
    "trial_artifacts = artifacts_response.artifacts\n",
    "\n",
    "print(trial_artifacts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44cbdda-2190-4d84-a83d-8108a71b2fd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Extract artifact details as per the conditions\n",
    "request = metadata_store_service_pb2.GetArtifactsRequest()\n",
    "response = mdstore.GetArtifacts(request)\n",
    "\n",
    "# Process the response and filter the artifacts manually\n",
    "artifacts_with_conditions = [\n",
    "    artifact for artifact in response.artifacts\n",
    "    if artifact.properties[\"accuracy\"].double_value >  0.1\n",
    "]\n",
    "\n",
    "print(artifacts_with_conditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3202456a-b301-47a4-a70c-7e710b4c09ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Save the best model\n",
    "# Create a directory to save the optimal HPO model\n",
    "hpo_folder = \"models/hpo/\"\n",
    "os.makedirs(hpo_folder, exist_ok=True)\n",
    "onnx_model_path = os.path.join(hpo_folder, \"model.onnx\")\n",
    "\n",
    "# Save the best model to a file in ONNX format\n",
    "dummy_input = torch.tensor([[0.3111400080477545, 1.9459399775518593, 1.0, 0.0, 0.0, 1.2, 3.4, -0.5, 0.8, -2.0]])\n",
    "torch.onnx.export(best_model, dummy_input, onnx_model_path, verbose=True)\n",
    "\n",
    "print(f\"Best model saved to {onnx_model_path} in ONNX format\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550b90f4-4fde-48a4-af4a-5a3d96dc762d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import botocore\n",
    "\n",
    "aws_access_key_id = os.environ.get('AWS_ACCESS_KEY_ID')\n",
    "aws_secret_access_key = os.environ.get('AWS_SECRET_ACCESS_KEY')\n",
    "endpoint_url = os.environ.get('AWS_S3_ENDPOINT')\n",
    "region_name = os.environ.get('AWS_DEFAULT_REGION')\n",
    "bucket_name = os.environ.get('AWS_S3_BUCKET')\n",
    "\n",
    "session = boto3.session.Session(aws_access_key_id=aws_access_key_id,\n",
    "                                aws_secret_access_key=aws_secret_access_key)\n",
    "\n",
    "s3_resource = session.resource(\n",
    "    's3',\n",
    "    config=botocore.client.Config(signature_version='s3v4'),\n",
    "    endpoint_url=endpoint_url,\n",
    "    region_name=region_name)\n",
    "\n",
    "bucket = s3_resource.Bucket(bucket_name)\n",
    "print(bucket)\n",
    "\n",
    "def upload_directory_to_s3(local_directory, s3_prefix):\n",
    "    for root, dirs, files in os.walk(local_directory):\n",
    "        for filename in files:\n",
    "            file_path = os.path.join(root, filename)\n",
    "            relative_path = os.path.relpath(file_path, local_directory)\n",
    "            s3_key = os.path.join(s3_prefix, relative_path)\n",
    "            print(f\"{file_path} -> {s3_key}\")\n",
    "            bucket.upload_file(file_path, s3_key)\n",
    "    return True\n",
    "\n",
    "def list_objects(prefix):\n",
    "    filter = bucket.objects.filter(Prefix=prefix)\n",
    "    for obj in filter.all():\n",
    "        print(obj.key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4b8caa-e69e-4c8f-9176-b8fd365df9bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List the objects from\n",
    "list_objects(\"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c53525-7228-4d08-8a00-51aee3dde112",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Upload the model to the S3 directory\n",
    "upload_directory_to_s3(\"models\", \"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e1008a-ec6e-4e44-b747-e590c89a021d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "## Get the input_names from the model\n",
    "# Load the ONNX model\n",
    "onnx_model_path=\"models/hpo/model.onnx\"\n",
    "onnx_model = onnx.load(onnx_model_path)\n",
    "\n",
    "# Print input names\n",
    "input_names = [input.name for input in onnx_model.graph.input]\n",
    "print(\"Input Names:\", input_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db79cd59-0cd0-418d-98a2-fee188c6342a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predict for the given data through REST\n",
    "data = [0.3111400080477545, 1.9459399775518593, 1.0, 2.0, 3.0, 1.2, 0.4, 0.5, 0.8, 2.0]\n",
    "input_array=np.array(data, dtype=np.float32).reshape(1,10)\n",
    "\n",
    "ort_session=ort.InferenceSession(onnx_model_path)\n",
    "input_name=input_names[0]\n",
    "outputs=ort_session.run(None, {input_name:input_array})\n",
    "\n",
    "print('Model Prediction:', outputs[0][0][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
