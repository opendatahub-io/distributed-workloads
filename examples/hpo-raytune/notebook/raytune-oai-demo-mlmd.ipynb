{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8f1c98-912c-41be-8f74-e1d144a82501",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import pieces from codeflare-sdk\n",
    "from codeflare_sdk import Cluster, ClusterConfiguration, TokenAuthentication\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2585dbab-f4e1-42cc-babc-b4cacd7b5431",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create authentication object for user permissions\n",
    "# IF unused, SDK will automatically check for default kubeconfig, then in-cluster config\n",
    "# KubeConfigFileAuthentication can also be used to specify kubeconfig path manually\n",
    "auth = TokenAuthentication(\n",
    "    token = \"TOKEN\",\n",
    "    server = \"SERVER\",\n",
    "    skip_tls=True\n",
    ")\n",
    "auth.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91affd0d-3500-432a-8ef9-2c1ddcd9ee98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create and configure our cluster object (and appwrapper)\n",
    "# NOTE: If running outside of RHOAI notebooks, add the following line to the cluster configuration:\n",
    "# namespace=\"rhods-notebooks\"\n",
    "cluster_name='terrestrial-raytest'\n",
    "cluster = Cluster(ClusterConfiguration(\n",
    "    name=cluster_name,\n",
    "    head_cpus=1,\n",
    "    head_memory=4,\n",
    "    num_workers=2,\n",
    "    min_cpus=1,\n",
    "    max_cpus=1,\n",
    "    min_memory=4,\n",
    "    max_memory=4,\n",
    "    num_gpus=0,\n",
    "    image=\"quay.io/modh/ray:2.35.0-py311-cu121\",                                           \n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc0a8f1-ab8b-4afa-81b7-47592a359413",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Bring up the cluster\n",
    "cluster.apply()\n",
    "cluster.wait_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5518074-1cc5-4fe5-bf7d-8c5087b6b65c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster.status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb7f911",
   "metadata": {},
   "outputs": [],
   "source": [
    "from codeflare_sdk import generate_cert\n",
    "# Create required TLS cert and export the environment variables to enable TLS\n",
    "generate_cert.generate_tls_cert(cluster_name, cluster.config.namespace)\n",
    "generate_cert.export_env(cluster_name, cluster.config.namespace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c30fb0-01fd-417b-811c-f97070f367b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy\n",
    "!pip install ray[default]==2.35.0\n",
    "!pip install onnxruntime\n",
    "!pip install ml_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "bc5a9720-4938-454b-979f-8bf1bb748090",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "ray_cluster_uri = cluster.cluster_uri()\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e1349a-5b08-44c3-a1c7-b4ea8733bbb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Additional libs\n",
    "runtime_env = {\"pip\": [\"ipython\", \"torch\" , \"onnx\", \"ray[train]\", \"ml_metadata\" ,\"protobuf==3.20.1\"]}\n",
    "\n",
    "ray.init(address=ray_cluster_uri, runtime_env=runtime_env,_system_config={\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\": \"python\"} )\n",
    "\n",
    "print(\"Ray cluster is up and running: \", ray.is_initialized())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f78b2c1-0c0e-4f73-9209-41dd80aae63f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import ray\n",
    "from ray import tune\n",
    "import time\n",
    "import os\n",
    "\n",
    "import grpc\n",
    "import ml_metadata as mlmd\n",
    "import ml_metadata.metadata_store.pywrap.metadata_store_extension.metadata_store\n",
    "from grpc import insecure_channel\n",
    "from ml_metadata.metadata_store import metadata_store\n",
    "from ml_metadata.proto import metadata_store_pb2\n",
    "from ml_metadata.proto import metadata_store_service_pb2\n",
    "from ml_metadata.proto import metadata_store_service_pb2_grpc\n",
    "from contextlib import contextmanager\n",
    "\n",
    "## **  SET UP THE CONNECTION CONFIGURATION FOR THE DATABASE ** ##\n",
    "# Set up the connection configuration for the MySQL database\n",
    "#connection_config = metadata_store_pb2.ConnectionConfig()\n",
    "#connection_config.mysql.host = 'mlmetadataservice'\n",
    "#connection_config.mysql.port = 3306\n",
    "#connection_config.mysql.database = 'mlmetadata'\n",
    "#connection_config.mysql.user = 'usermetadata'\n",
    "#connection_config.mysql.password = 'password'\n",
    "\n",
    "## ** USE CONTEXTMANAGER WHILE CREATING METADATA STORE CONNECTION TO AVOID SERIALIZATION ** ##\n",
    "@contextmanager\n",
    "def get_metadata_store():\n",
    "    # Create the Metadata Store with the connection configuration\n",
    "    mdstore = metadata_store.MetadataStore(connection_config, enable_upgrade_migration=True)\n",
    "    yield mdstore\n",
    "\n",
    "# Create a gRPC channel with the appropriate options\n",
    "channel = grpc.insecure_channel(\n",
    "    'mlmetadataservice:8080',\n",
    "    options=[('grpc.max_receive_message_length', 512 * 1024 * 1024),\n",
    "             ('grpc.max_send_message_length', 512 * 1024 * 1024)]\n",
    ")\n",
    "metadata_store_stub = metadata_store_service_pb2_grpc.MetadataStoreServiceStub(channel)\n",
    "\n",
    "\n",
    "global data_type_id, hpo_config_id, model_type_id, trainer_type_id\n",
    "global experiment_type_id \n",
    "global trial_type_id\n",
    "\n",
    "## ** CREATE ARTIFACT , CONTEXT AND EXECUTION TYPES FOR YOUR MODEL **##\n",
    "\n",
    "'''\n",
    "For the model in this code, below are created.\n",
    "\n",
    "HPOExperiment: context type\n",
    "HPOTrial: context type\n",
    "HPOConfig: artifact type\n",
    "DataSet: artifact type\n",
    "Model: artifact type\n",
    "'''\n",
    "\n",
    "def create_mdtypes():\n",
    "    with get_metadata_store() as mds:\n",
    "        # Create ArtifactTypes for DataSet\n",
    "        data_type = metadata_store_pb2.ArtifactType()\n",
    "        data_type.name = \"DataSet\"\n",
    "        data_type.properties[\"ds_input_size\"] = metadata_store_pb2.INT\n",
    "        data_type.properties[\"ds_output_size\"] = metadata_store_pb2.INT\n",
    "        data_type_id = mds.put_artifact_type(data_type) \n",
    "\n",
    "        # Create ArtifactTypes for HPOConfig\n",
    "        #hpo_config = metadata_store_pb2.ArtifactType()\n",
    "        #hpo_config.name = \"HPOConfig\"\n",
    "        #hpo_config.properties[\"hp_hidden_size\"] = metadata_store_pb2.INT\n",
    "        #hpo_config.properties[\"hp_lr\"] = metadata_store_pb2.DOUBLE\n",
    "        #hpo_config_id = mds.put_artifact_type(hpo_config)\n",
    "\n",
    "        # Create ArtifactTypes for Model\n",
    "        #model_type = metadata_store_pb2.ArtifactType()\n",
    "        #model_type.name = \"Model\"\n",
    "        #model_type.properties[\"model_version\"] = metadata_store_pb2.INT\n",
    "        #model_type.properties[\"model_name\"] = metadata_store_pb2.STRING\n",
    "        #model_type.properties[\"model_accuracy\"] = metadata_store_pb2.DOUBLE\n",
    "        #model_type_id = mds.put_artifact_type(model_type)\n",
    "\n",
    "        #  Register execution types for all steps in the ML workflow\n",
    "        # Create an ExecutionType, Trainer\n",
    "        trainer_type = metadata_store_pb2.ExecutionType()\n",
    "        trainer_type.name = \"Trainer\"\n",
    "        trainer_type.properties[\"trainer_state\"] = metadata_store_pb2.STRING\n",
    "        trainer_type_id = mds.put_execution_type(trainer_type)\n",
    "\n",
    "        # Create a ContextType for HPO experiment\n",
    "        #experiment_type = metadata_store_pb2.ContextType()\n",
    "        #experiment_type.name = \"Experiment\"\n",
    "        #experiment_type.properties[\"parent_exp\"] = metadata_store_pb2.STRING\n",
    "        #experiment_type_id = mds.put_context_type(experiment_type)\n",
    "\n",
    "        # Create a ContextType for HPOTrial\n",
    "        trial_type = metadata_store_pb2.ContextType()\n",
    "        trial_type.name = \"Trial\"\n",
    "        trial_type.properties[\"trial_id\"] = metadata_store_pb2.STRING\n",
    "        trial_type_id = mds.put_context_type(trial_type)    \n",
    "\n",
    "## ** CREATE EXPERIMENT AS PARENT AND TRIALS AS CHILD ** ##\n",
    "\n",
    "#def create_Parent_Context():\n",
    "#    with get_metadata_store() as mds:\n",
    "#        # Create a Context for the experiment \n",
    "#        experiment_context = metadata_store_pb2.Context()\n",
    "#        experiment_context.type_id = experiment_type_id\n",
    "#        experiment_context.name = raytune_experiment_name\n",
    "#        experiment_context.properties[\"parent_exp\"].string_value = \"Overall experiment details\"\n",
    "#        [experiment_id] = mds.put_contexts([experiment_context])\n",
    "#        return experiment_id\n",
    "\n",
    "\n",
    "## ** LOG INPUT EVENTS INTO MDSTORE , UPDATE EXECUTION STATUS ** ##\n",
    "def log_inputs(input_size,output_size,config,trial_name,trial_id,exp_name):\n",
    "    with get_metadata_store() as metadata_store0:\n",
    "        # Register the Execution of a Trainer run\n",
    "        trainer_run = metadata_store_pb2.Execution()\n",
    "        trainer_run.type_id = trainer_type_id\n",
    "        trainer_run.properties[\"trainer_state\"].string_value = \"RUNNING\"\n",
    "        [run_id] = metadata_store0.put_executions([trainer_run])\n",
    "        \n",
    "        # Log metadata about the dataset\n",
    "        dataset_artifact = metadata_store_pb2.Artifact()\n",
    "        dataset_artifact.type_id = data_type_id\n",
    "        dataset_artifact.uri = \"\"  # Replace with the actual path or identifier\n",
    "        dataset_artifact.properties[\"ds_input_size\"].int_value = input_size\n",
    "        dataset_artifact.properties[\"ds_output_size\"].int_value = output_size\n",
    "\n",
    "        # Log metadata about the hyperparameter tuning\n",
    "        #hpo_config_artifact = metadata_store_pb2.Artifact()\n",
    "        #hpo_config_artifact.type_id = hpo_config_id\n",
    "        #hpo_config_artifact.uri = \"\"  # Replace with the actual path or identifier\n",
    "        #hpo_config_artifact.properties[\"hp_hidden_size\"].int_value = config[\"hidden_size\"]\n",
    "        #hpo_config_artifact.properties[\"hp_lr\"].double_value = config[\"lr\"]\n",
    "        #hpo_config_artifact.custom_properties[\"trial_name\"].string_value = trial_name\n",
    "        #hpo_config_artifact.custom_properties[\"trial_id\"].string_value = trial_id\n",
    "        #hpo_config_artifact.custom_properties[\"experiment_name\"].string_value = exp_name\n",
    "\n",
    "        #artifact_ids = metadata_store0.put_artifacts([dataset_artifact, hpo_config_artifact])\n",
    "        artifact_ids = metadata_store0.put_artifacts([dataset_artifact])\n",
    "\n",
    "        # Define the input events\n",
    "        input_event_dataset = metadata_store_pb2.Event()\n",
    "        input_event_dataset.artifact_id = artifact_ids[0]\n",
    "        input_event_dataset.execution_id = run_id\n",
    "        input_event_dataset.type = metadata_store_pb2.Event.DECLARED_INPUT\n",
    "\n",
    "        #input_event_hpo_config = metadata_store_pb2.Event()\n",
    "        #input_event_hpo_config.artifact_id = artifact_ids[1]\n",
    "        #input_event_hpo_config.execution_id = run_id\n",
    "        #input_event_hpo_config.type = metadata_store_pb2.Event.DECLARED_INPUT\n",
    "\n",
    "        # Record the input events in the metadata store\n",
    "        #metadata_store0.put_events([input_event_dataset, input_event_hpo_config])\n",
    "        metadata_store0.put_events([input_event_dataset])\n",
    "        return run_id, trainer_run , artifact_ids\n",
    "\n",
    "## ** LOG OUTPUT EVENTS INTO MDSTORE , UPDATE EXECUTION STATUS ** ##\n",
    "def log_output_artifacts(accuracy,run_id,trainer_run,trial_name, artifact_ids):\n",
    "    with get_metadata_store() as metadata_store:\n",
    "        experiment_contexts = []\n",
    "\n",
    "        # Log metadata about the model\n",
    "        model_artifact = metadata_store_pb2.Artifact()\n",
    "        model_artifact.type_id = model_type_id\n",
    "        model_artifact.uri = \"\"  # Replace with the actual path or identifier\n",
    "        model_artifact.properties[\"model_accuracy\"].double_value = accuracy\n",
    "        [model_artifact_id] = metadata_store.put_artifacts([model_artifact])\n",
    "\n",
    "        # Declare the output event\n",
    "        output_event = metadata_store_pb2.Event()\n",
    "        output_event.artifact_id = model_artifact_id\n",
    "        output_event.execution_id = run_id\n",
    "        output_event.type = metadata_store_pb2.Event.DECLARED_OUTPUT\n",
    "\n",
    "        # Submit output event to the Metadata Store\n",
    "        metadata_store.put_events([output_event])\n",
    "\n",
    "        trainer_run.id = run_id\n",
    "        trainer_run.properties[\"trainer_state\"].string_value = \"COMPLETED\"\n",
    "        metadata_store.put_executions([trainer_run])\n",
    "\n",
    "        # Create a Context for the trial\n",
    "        trial_context = metadata_store_pb2.Context()\n",
    "        trial_context.type_id = trial_type_id\n",
    "        trial_context.name = trial_name # Use the trial name as the context name\n",
    "        trial_context.properties[\"trial_id\"].string_value = trial_id # Store the trial ID as a note\n",
    "        [context_trial_id] = metadata_store.put_contexts([trial_context])\n",
    "\n",
    "        #experiment_context_obj = metadata_store_pb2.ParentContext()\n",
    "        #experiment_context_obj.child_id = context_trial_id\n",
    "        #experiment_context_obj.parent_id = experiment_id\n",
    "\n",
    "        #experiment_contexts.append(experiment_context_obj)\n",
    "        #metadata_store.put_parent_contexts(experiment_contexts)\n",
    "\n",
    "        # Associate the trial context with the experiment context\n",
    "        association = metadata_store_pb2.Association()\n",
    "        association.execution_id = run_id\n",
    "        association.context_id = context_trial_id\n",
    "        \n",
    "        attribution = metadata_store_pb2.Attribution()\n",
    "        attribution.artifact_id = model_artifact_id\n",
    "        attribution.context_id = context_trial_id\n",
    "        \n",
    "        #attribution_hpo = metadata_store_pb2.Attribution()\n",
    "        #attribution_hpo.artifact_id = artifact_ids[1]\n",
    "        #attribution_hpo.context_id = context_trial_id\n",
    "\n",
    "        #metadata_store.put_attributions_and_associations([attribution,attribution_hpo ], [association])\n",
    "        metadata_store.put_attributions_and_associations([attribution], [association])\n",
    "\n",
    "\n",
    "# Define a simple neural network\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Define a function to train and evaluate the model\n",
    "def train_evaluate(config,trial_name=None, trial_id=None):\n",
    "    input_size = 10\n",
    "    output_size = 1\n",
    "\n",
    "    # Instantiate the neural network with the hyperparameters\n",
    "    model = SimpleNet(input_size, config[\"hidden_size\"], output_size)\n",
    "\n",
    "    # Define a dummy dataset for illustration purposes\n",
    "    X = torch.randn(100, input_size)\n",
    "    y = torch.randn(100, output_size)\n",
    "\n",
    "    # Dummy DataLoader\n",
    "    dataset = TensorDataset(X, y)\n",
    "    dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "    trial_id = ray.train.get_context().get_trial_id()\n",
    "    trial_name = ray.train.get_context().get_trial_name()\n",
    "    trial_dir = ray.train.get_context().get_trial_dir()\n",
    "    exp_name = ray.train.get_context().get_experiment_name()\n",
    "    mtdata = ray.train.get_context().get_metadata()\n",
    "    \n",
    "    # Log input artfacts, events , Executions\n",
    "    run_id, trainer_run, artifact_ids = log_inputs(input_size,output_size,config,trial_name,trial_id,exp_name)\n",
    "    \n",
    "    time.sleep(10)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(10):  # Adjust as needed\n",
    "        for inputs, targets in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "    # Evaluate the model (for simplicity, just return a dummy accuracy)\n",
    "    accuracy = torch.rand(1).item()\n",
    "    \n",
    "    # Log output artifacts, events, Executions\n",
    "    log_output_artifacts(accuracy,run_id,trainer_run,trial_name,artifact_ids)\n",
    "\n",
    "    # Return a dictionary containing the accuracy and the model\n",
    "    return {\"accuracy\": accuracy, \"model\": model}\n",
    "\n",
    "\n",
    "raytune_experiment_name = \"raytune_hyperparameter_tuning\"\n",
    "\n",
    "# Define the hyperparameter search space\n",
    "search_space = {\n",
    "    \"hidden_size\": tune.choice([5, 10, 20]),\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "}\n",
    "\n",
    "# Create MLMD Artifact/Context Types\n",
    "create_mdtypes()\n",
    "# Uncomment while adding ParentContext\n",
    "#experiment_id = create_Parent_Context()\n",
    "\n",
    "# Perform hyperparameter tuning with Ray Tune\n",
    "analysis = tune.run(\n",
    "    train_evaluate,\n",
    "    config=search_space,\n",
    "    num_samples=6,  # Number of trials\n",
    "    resources_per_trial={\"cpu\": 1},\n",
    "    name=raytune_experiment_name,)\n",
    "\n",
    "\n",
    "# Get the best configuration and result\n",
    "best_trial = analysis.get_best_trial(\"accuracy\", \"max\", \"last\")\n",
    "best_config = best_trial.config\n",
    "best_accuracy = best_trial.last_result[\"accuracy\"]\n",
    "best_model = best_trial.last_result[\"model\"]\n",
    "trial_id = best_trial.trial_id \n",
    "\n",
    "print(f\"Best model from trial ID: {trial_id}\")\n",
    "print(f\"Best hyperparameters: {best_config}\")\n",
    "print(f\"Best accuracy: {best_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cda0690-a274-40cb-9c62-eb52d452b025",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Save the best model\n",
    "# Create a directory to save the optimal HPO model\n",
    "hpo_folder = \"models/hpo/\"\n",
    "os.makedirs(hpo_folder, exist_ok=True)\n",
    "onnx_model_path = os.path.join(hpo_folder, \"model.onnx\")\n",
    "\n",
    "# Save the best model to a file in ONNX format\n",
    "dummy_input = torch.tensor([[0.3111400080477545, 1.9459399775518593, 1.0, 0.0, 0.0, 1.2, 3.4, -0.5, 0.8, -2.0]])\n",
    "torch.onnx.export(best_model, dummy_input, onnx_model_path, verbose=True)\n",
    "\n",
    "print(f\"Best model saved to {onnx_model_path} in ONNX format\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f3170d-7ff2-499a-94cb-14a6c024d507",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import botocore\n",
    "\n",
    "aws_access_key_id = os.environ.get('AWS_ACCESS_KEY_ID')\n",
    "aws_secret_access_key = os.environ.get('AWS_SECRET_ACCESS_KEY')\n",
    "endpoint_url = os.environ.get('AWS_S3_ENDPOINT')\n",
    "region_name = os.environ.get('AWS_DEFAULT_REGION')\n",
    "bucket_name = os.environ.get('AWS_S3_BUCKET')\n",
    "\n",
    "session = boto3.session.Session(aws_access_key_id=aws_access_key_id,\n",
    "                                aws_secret_access_key=aws_secret_access_key)\n",
    "\n",
    "s3_resource = session.resource(\n",
    "    's3',\n",
    "    config=botocore.client.Config(signature_version='s3v4'),\n",
    "    endpoint_url=endpoint_url,\n",
    "    region_name=region_name)\n",
    "\n",
    "bucket = s3_resource.Bucket(bucket_name)\n",
    "print(bucket)\n",
    "\n",
    "def upload_directory_to_s3(local_directory, s3_prefix):\n",
    "    for root, dirs, files in os.walk(local_directory):\n",
    "        for filename in files:\n",
    "            file_path = os.path.join(root, filename)\n",
    "            relative_path = os.path.relpath(file_path, local_directory)\n",
    "            s3_key = os.path.join(s3_prefix, relative_path)\n",
    "            print(f\"{file_path} -> {s3_key}\")\n",
    "            bucket.upload_file(file_path, s3_key)\n",
    "    return True\n",
    "\n",
    "def list_objects(prefix):\n",
    "    filter = bucket.objects.filter(Prefix=prefix)\n",
    "    for obj in filter.all():\n",
    "        print(obj.key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60693c02-93a8-416a-8625-3c5b5de230cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List the objects from\n",
    "list_objects(\"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1598d0f-0635-4f21-ac7b-1a4a2b3bb981",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Upload the model to the S3 directory\n",
    "upload_directory_to_s3(\"models\", \"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "d2160e15-2a32-4830-8905-cf06b3185545",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Details to access the model through REST API\n",
    "deployed_model_name = \"hpo\"\n",
    "rest_url = \"http://modelmesh-serving.pcelesti:8008\"\n",
    "infer_url = f\"{rest_url}/v2/models/{deployed_model_name}/infer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40c3d3b-84f8-4a4a-a67d-87b1aa9e8f09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Get the input_names from the model\n",
    "# Load the ONNX model\n",
    "onnx_model = onnx.load(\"models/hpo/model.onnx\")\n",
    "\n",
    "# Print input names\n",
    "input_names = [input.name for input in onnx_model.graph.input]\n",
    "print(\"Input Names:\", input_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685956e8-3a30-4b15-b542-aaea30f9b033",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "def onnx_rest_request(data, infer_url):\n",
    "    # Convert the input data to a numpy array\n",
    "    input_array = np.array(data, dtype=np.float32).reshape(1, 10)\n",
    "\n",
    "    # Convert the numpy array to a list for JSON serialization\n",
    "    input_list = input_array.tolist()\n",
    "\n",
    "    # Create the JSON payload for the REST request\n",
    "    json_data = {\n",
    "        \"inputs\": [\n",
    "            {\n",
    "                \"name\": \"onnx::Gemm_0\",\n",
    "                \"shape\": input_array.shape,\n",
    "                \"datatype\": \"FP32\",\n",
    "                \"data\": input_list\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Make the REST request\n",
    "    response = requests.post(infer_url, json=json_data)\n",
    "    print(response.content)\n",
    "\n",
    "    # Check for successful response (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        response_dict = response.json()\n",
    "        # Extract and return the predictions from the response\n",
    "        return response_dict['outputs'][0]['data']\n",
    "    else:\n",
    "        # Print an error message for unsuccessful requests\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d831a8c-69bd-409b-ba6f-09d103e44a2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predict for the given data through REST\n",
    "data = [0.3111400080477545, 1.9459399775518593, 1.0, 2.0, 3.0, 1.2, 0.4, 0.5, 0.8, 2.0]\n",
    "prediction = onnx_rest_request(data,infer_url)\n",
    "print(\"Model Prediction:\", prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d655151f-e3b1-4cba-98ac-db515c99f288",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
