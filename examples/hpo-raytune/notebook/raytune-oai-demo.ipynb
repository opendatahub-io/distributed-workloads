{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8f1c98-912c-41be-8f74-e1d144a82501",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import pieces from codeflare-sdk\n",
    "from codeflare_sdk import Cluster, ClusterConfiguration, TokenAuthentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae3b346-36d2-45a1-ae35-ac3971d471c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create authentication object for user permissions\n",
    "# IF unused, SDK will automatically check for default kubeconfig, then in-cluster config\n",
    "# KubeConfigFileAuthentication can also be used to specify kubeconfig path manually\n",
    "# Replace TOKEN and SERVER with the actual values\n",
    "auth = TokenAuthentication(\n",
    "    token = \"TOKEN\",\n",
    "    server = \"SERVER\",\n",
    "    skip_tls=True\n",
    ")\n",
    "auth.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea135d2b-e2c2-4a0a-8c65-7ca1b3355c2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create and configure our cluster object (and appwrapper)\n",
    "cluster_name=\"terrestrial-raytest\"\n",
    "cluster = Cluster(ClusterConfiguration(\n",
    "    name=cluster_name,\n",
    "    head_cpus=1,\n",
    "    head_memory=4,\n",
    "    num_workers=2,\n",
    "    min_cpus=1,\n",
    "    max_cpus=1,\n",
    "    min_memory=4,\n",
    "    max_memory=4,\n",
    "    num_gpus=0,\n",
    "    image=\"quay.io/modh/ray:2.35.0-py311-cu121\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc0a8f1-ab8b-4afa-81b7-47592a359413",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Bring up the cluster\n",
    "cluster.up()\n",
    "cluster.wait_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0a5759-2b37-4d36-bfb0-2dd89a146878",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster.status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b589f722",
   "metadata": {},
   "outputs": [],
   "source": [
    "from codeflare_sdk import generate_cert\n",
    "# Create required TLS cert and export the environment variables to enable TLS\n",
    "generate_cert.generate_tls_cert(cluster_name, cluster.config.namespace)\n",
    "generate_cert.export_env(cluster_name, cluster.config.namespace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878a059e-d267-4b3e-972f-cb3cf9557f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy\n",
    "!pip install ray[default]==2.35.0\n",
    "!pip install onnxruntime\n",
    "!pip install --upgrade pyarrow fsspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5a9720-4938-454b-979f-8bf1bb748090",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "ray_cluster_uri = cluster.cluster_uri()\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fffaa70-b6c7-40aa-9e7c-e7e2812303c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Additional libs\n",
    "runtime_env = {\"pip\": [\"ipython\", \"torch\" , \"onnx\", \"ray[train]\", \"protobuf==3.20.1\"]}\n",
    "\n",
    "ray.init(address=ray_cluster_uri, runtime_env=runtime_env,_system_config={\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\": \"python\"} )\n",
    "\n",
    "print(\"Ray cluster is up and running: \", ray.is_initialized())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebfaa96-023c-443b-a653-4ce3eb5e4272",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from ray import tune\n",
    "import time\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "# Import ONNX-related libraries\n",
    "import torch.onnx\n",
    "import onnx\n",
    "from onnxruntime import InferenceSession\n",
    "\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\n",
    "\n",
    "# Define a simple neural network\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Define a function to train and evaluate the model\n",
    "def train_evaluate(config):\n",
    "    input_size = 10\n",
    "    output_size = 1\n",
    "\n",
    "    # Instantiate the neural network with the hyperparameters\n",
    "    model = SimpleNet(input_size, config[\"hidden_size\"], output_size)\n",
    "\n",
    "    # Define a dummy dataset for illustration purposes\n",
    "    X = torch.randn(100, input_size)\n",
    "    y = torch.randn(100, output_size)\n",
    "\n",
    "    # Dummy DataLoader\n",
    "    dataset = TensorDataset(X, y)\n",
    "    dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "    \n",
    "    time.sleep(10)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(10):  # Adjust as needed\n",
    "        for inputs, targets in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluate the model (for simplicity, just return a dummy accuracy)\n",
    "    accuracy = torch.rand(1).item()\n",
    "\n",
    "    # Return a dictionary containing the accuracy and the model\n",
    "    return {\"accuracy\": accuracy, \"model\": model}\n",
    "\n",
    "# Define the hyperparameter search space\n",
    "search_space = {\n",
    "    \"hidden_size\": tune.choice([5, 10, 20]),\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "}\n",
    "\n",
    "# Run the raytune\n",
    "analysis = tune.run(\n",
    "    train_evaluate,\n",
    "    config=search_space,\n",
    "    num_samples=2,  # Number of trials\n",
    "    resources_per_trial={\"cpu\": 1},\n",
    "    name=\"raytune_hyperparameter_tuning_example\",\n",
    ")\n",
    "\n",
    "# Get the best configuration and result\n",
    "best_trial = analysis.get_best_trial(\"accuracy\", \"max\", \"last\")\n",
    "best_config = best_trial.config\n",
    "best_accuracy = best_trial.last_result[\"accuracy\"]\n",
    "best_model = best_trial.last_result[\"model\"]\n",
    "\n",
    "print(f\"Best hyperparameters: {best_config}\")\n",
    "print(f\"Best accuracy: {best_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cda0690-a274-40cb-9c62-eb52d452b025",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Save the best model\n",
    "# Create a directory to save the optimal HPO model\n",
    "hpo_folder = \"models/hpo/\"\n",
    "os.makedirs(hpo_folder, exist_ok=True)\n",
    "onnx_model_path = os.path.join(hpo_folder, \"model.onnx\")\n",
    "\n",
    "# Save the best model to a file in ONNX format\n",
    "dummy_input = torch.tensor([[0.3111400080477545, 1.9459399775518593, 1.0, 0.0, 0.0, 1.2, 3.4, -0.5, 0.8, -2.0]])\n",
    "torch.onnx.export(best_model, dummy_input, onnx_model_path, verbose=True)\n",
    "\n",
    "print(f\"Best model saved to {onnx_model_path} in ONNX format\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f3170d-7ff2-499a-94cb-14a6c024d507",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import botocore\n",
    "\n",
    "aws_access_key_id = os.environ.get('AWS_ACCESS_KEY_ID')\n",
    "aws_secret_access_key = os.environ.get('AWS_SECRET_ACCESS_KEY')\n",
    "endpoint_url = os.environ.get('AWS_S3_ENDPOINT')\n",
    "region_name = os.environ.get('AWS_DEFAULT_REGION')\n",
    "bucket_name = os.environ.get('AWS_S3_BUCKET')\n",
    "\n",
    "session = boto3.session.Session(aws_access_key_id=aws_access_key_id,\n",
    "                                aws_secret_access_key=aws_secret_access_key)\n",
    "\n",
    "s3_resource = session.resource(\n",
    "    's3',\n",
    "    config=botocore.client.Config(signature_version='s3v4'),\n",
    "    endpoint_url=endpoint_url,\n",
    "    region_name=region_name)\n",
    "\n",
    "bucket = s3_resource.Bucket(bucket_name)\n",
    "print(bucket)\n",
    "\n",
    "def upload_directory_to_s3(local_directory, s3_prefix):\n",
    "    for root, dirs, files in os.walk(local_directory):\n",
    "        for filename in files:\n",
    "            file_path = os.path.join(root, filename)\n",
    "            relative_path = os.path.relpath(file_path, local_directory)\n",
    "            s3_key = os.path.join(s3_prefix, relative_path)\n",
    "            print(f\"{file_path} -> {s3_key}\")\n",
    "            bucket.upload_file(file_path, s3_key)\n",
    "    return True\n",
    "\n",
    "def list_objects(prefix):\n",
    "    filter = bucket.objects.filter(Prefix=prefix)\n",
    "    for obj in filter.all():\n",
    "        print(obj.key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60693c02-93a8-416a-8625-3c5b5de230cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List the objects from\n",
    "list_objects(\"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1598d0f-0635-4f21-ac7b-1a4a2b3bb981",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Upload the model to the S3 directory\n",
    "upload_directory_to_s3(\"models\", \"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "d2160e15-2a32-4830-8905-cf06b3185545",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Details to access the model through REST API\n",
    "deployed_model_name = \"hpo\"\n",
    "rest_url = \"http://modelmesh-serving.pcelesti:8008\"\n",
    "infer_url = f\"{rest_url}/v2/models/{deployed_model_name}/infer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40c3d3b-84f8-4a4a-a67d-87b1aa9e8f09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Get the input_names from the model\n",
    "# Load the ONNX model\n",
    "onnx_model = onnx.load(\"models/hpo/model.onnx\")\n",
    "\n",
    "# Print input names\n",
    "input_names = [input.name for input in onnx_model.graph.input]\n",
    "print(\"Input Names:\", input_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685956e8-3a30-4b15-b542-aaea30f9b033",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "def onnx_rest_request(data, infer_url):\n",
    "    # Convert the input data to a numpy array\n",
    "    input_array = np.array(data, dtype=np.float32).reshape(1, 10)\n",
    "\n",
    "    # Convert the numpy array to a list for JSON serialization\n",
    "    input_list = input_array.tolist()\n",
    "\n",
    "    # Create the JSON payload for the REST request\n",
    "    json_data = {\n",
    "        \"inputs\": [\n",
    "            {\n",
    "                \"name\": \"onnx::Gemm_0\",\n",
    "                \"shape\": input_array.shape,\n",
    "                \"datatype\": \"FP32\",\n",
    "                \"data\": input_list\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Make the REST request\n",
    "    response = requests.post(infer_url, json=json_data)\n",
    "    print(response.content)\n",
    "\n",
    "    # Check for successful response (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        response_dict = response.json()\n",
    "        # Extract and return the predictions from the response\n",
    "        return response_dict['outputs'][0]['data']\n",
    "    else:\n",
    "        # Print an error message for unsuccessful requests\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d831a8c-69bd-409b-ba6f-09d103e44a2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predict for the given data\n",
    "data = [0.3111400080477545, 1.9459399775518593, 1.0, 2.0, 3.0, 1.2, 0.4, 0.5, 0.8, 2.0]\n",
    "prediction = onnx_rest_request(data,infer_url)\n",
    "print(\"Model Prediction:\", prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d655151f-e3b1-4cba-98ac-db515c99f288",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
