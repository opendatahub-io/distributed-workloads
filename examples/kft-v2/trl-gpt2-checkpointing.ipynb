{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec6b8bb7",
   "metadata": {},
   "source": [
    "## TRL(Transformer Reinforcement Learning) Training with Kubeflow SDK and Advanced Checkpointing\n",
    "\n",
    "This notebook demonstrates how to use the Kubeflow Trainer SDK to create and manage TrainJobs\n",
    "\n",
    "### Features Demonstrated\n",
    "- **Kubeflow SDK Integration**: Programmatic TrainJob creation and management\n",
    "- **Checkpointing**: Controller-managed resume/suspended compatibility for model checkpoints\n",
    "- **TRL SFTTrainer**: Supervised fine-tuning using Peft-LoRA with GPT-2 and Alpaca dataset for instruction following\n",
    "- **Distributed Training**: Multi-node Multi-GPU coordination\n",
    "- **Compute resource pre-requisite for this demo** : \n",
    "    This demo can run on -\n",
    "    - CPUs based training using GLOO backend (default configuration)\n",
    "    - GPUs(NVIDIA/AMD) based training using NCCL backend\n",
    "        - Respective training images (update in [torch-cuda-custom](./cluster_training_runtime.yaml)):\n",
    "            - quay.io/modh/training:py311-cuda124-torch251\n",
    "            - quay.io/modh/training:py311-rocm62-torch251\n",
    "    - Multi-node Multi-GPU distributed training using Trainer V2 MlPolicies (NumNodes/NProcPerNodes)\n",
    "\n",
    "### Prerequisites\n",
    "- Persistent volume storage with RWX(ReadWriteManyAccess) : [workspace](workspace-checkpoint-storage)\n",
    "- ClusterTrainingRuntime :  [torch-cuda-custom](./cluster_training_runtime.yaml)\n",
    "\n",
    "### Sample scripts\n",
    "- [mnist.py](./scripts/mnist.py)\n",
    "- [trl_training.py](./scripts/trl_training.py)\n",
    "- _oc apply -k examples/kft-v2/manifests_\n",
    "\n",
    "### References\n",
    "- [Kubeflow Trainer SDK](https://github.com/kubeflow/sdk)\n",
    "- [TRL Documentation](https://huggingface.co/docs/trl/)\n",
    "- [PEFT Documentation](https://huggingface.co/docs/peft/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c67d2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install Kubeflow SDK from source github main branch\n",
    "%pip install kubeflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa66b8bd-47f4-4f72-9332-50db5d827d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: kubeflow\n",
      "Version: 0.1.0\n",
      "Summary: Kubeflow Python SDK to manage ML workloads and to interact with Kubeflow APIs.\n",
      "Home-page: https://github.com/kubeflow/sdk\n",
      "Author: \n",
      "Author-email: The Kubeflow Authors <kubeflow-discuss@googlegroups.com>\n",
      "License: \n",
      "Location: /opt/app-root/lib64/python3.12/site-packages\n",
      "Requires: kubeflow-trainer-api, kubernetes, pydantic\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip show kubeflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f97773c",
   "metadata": {},
   "source": [
    "### Define TRL Training Function\n",
    "- Progress file writer (callbacks)\n",
    "- Distributed checkpoint coordination\n",
    "- Automated model checkpointing by SIGTERM signal handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c002e5e",
   "metadata": {},
   "source": [
    "### Create TrainJob Using Kubeflow SDK\n",
    "Now we'll use the Kubeflow SDK to create a TrainJob\n",
    "- Training arguments\n",
    "- *CustomTrainer* with the TRL training function\n",
    "- *Initializer* for dataset and model (V2 initializers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b94b783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training configuration initialised!\n"
     ]
    }
   ],
   "source": [
    "from kubeflow.trainer import CustomTrainer, Initializer\n",
    "\n",
    "training_env_args = {\n",
    "    \"PYTHONUNBUFFERED\": \"1\",\n",
    "    \"NCCL_DEBUG\": \"INFO\",\n",
    "    \"TORCH_DISTRIBUTED_DEBUG\": \"INFO\",\n",
    "    \"PYTHONPATH\": \"/tmp/lib:$PYTHONPATH\",\n",
    "\n",
    "    # Training hyperparameters\n",
    "    \"LEARNING_RATE\": \"5e-5\",\n",
    "    \"BATCH_SIZE\": \"1\",\n",
    "    \"MAX_EPOCHS\": \"3\",\n",
    "    \"WARMUP_STEPS\": \"5\",\n",
    "    \"EVAL_STEPS\": \"3\",\n",
    "    \"SAVE_STEPS\": \"2\",\n",
    "    \"LOGGING_STEPS\": \"2\",\n",
    "    \"GRADIENT_ACCUMULATION_STEPS\": \"2\",\n",
    "    \n",
    "    # Model configuration\n",
    "    \"MODEL_NAME\": \"gpt2\",\n",
    "    \"LORA_R\": \"16\",\n",
    "    \"LORA_ALPHA\": \"32\",\n",
    "    \"LORA_DROPOUT\": \"0.1\",\n",
    "    \"MAX_SEQ_LENGTH\": \"512\",\n",
    "    \n",
    "    # Dataset configuration\n",
    "    \"DATASET_NAME\": \"tatsu-lab/alpaca\",\n",
    "    \"DATASET_TRAIN_SPLIT\": \"train[:500]\",\n",
    "    \"DATASET_TEST_SPLIT\": \"train[500:520]\",\n",
    "    \n",
    "    # Checkpointing configuration\n",
    "    \"CHECKPOINT_URI\": \"/workspace/checkpoints\",\n",
    "    \"TRAINJOB_PROGRESSION_FILE_PATH\": \"/tmp/training_progression.json\",\n",
    "    \n",
    "    # Cache directories\n",
    "    \"PYTHONUNBUFFERED\": \"1\",\n",
    "    \"TRANSFORMERS_CACHE\": \"/workspace/cache/transformers\",\n",
    "    \"HF_HOME\": \"/workspace/cache\",\n",
    "    \"HF_DATASETS_CACHE\": \"/workspace/cache/datasets\",\n",
    "    \n",
    "    # Distributed training debug\n",
    "    \"NCCL_DEBUG\": \"INFO\",\n",
    "    \"NCCL_DEBUG_SUBSYS\": \"ALL\",\n",
    "    \"NCCL_SOCKET_IFNAME\": \"eth0\",\n",
    "    \"NCCL_IB_DISABLE\": \"1\",\n",
    "    \"NCCL_P2P_DISABLE\": \"1\",\n",
    "    \"NCCL_TREE_THRESHOLD\": \"0\",\n",
    "    \"TORCH_DISTRIBUTED_DEBUG\": \"INFO\",\n",
    "    \"TORCH_SHOW_CPP_STACKTRACES\": \"1\",\n",
    "}\n",
    "\n",
    "from trl_training import trl_train\n",
    "\n",
    "# Create CustomTrainer configuration\n",
    "custom_trainer = CustomTrainer(\n",
    "    func=trl_train,\n",
    "    num_nodes=2,  # Distributed training across 2 nodes\n",
    "    resources_per_node={\n",
    "        \"cpu\": \"2\",\n",
    "        \"memory\": \"4Gi\",\n",
    "        # Uncomment for GPU training:\n",
    "        # \"nvidia.com/gpu\": \"1\",\n",
    "    },\n",
    "    packages_to_install=[\n",
    "        \"transformers[torch]\",\n",
    "        \"trl\", \n",
    "        \"peft\", \n",
    "        \"datasets\", \n",
    "        \"accelerate\",\n",
    "        \"torch\",\n",
    "        \"numpy\"\n",
    "        \" --target=/tmp/lib\"\n",
    "        \" --verbose\"\n",
    "    ],\n",
    "    env=training_env_args\n",
    ")\n",
    "from kubeflow.trainer.types import types\n",
    "\n",
    "# Configure Initializers\n",
    "initializer = Initializer(\n",
    "    dataset=types.HuggingFaceDatasetInitializer(\n",
    "        storage_uri=\"hf://tatsu-lab/alpaca\"\n",
    "    ),\n",
    "    model=types.HuggingFaceModelInitializer(\n",
    "        storage_uri=\"hf://gpt2\"\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Training configuration initialised!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e632ece",
   "metadata": {},
   "source": [
    "### Initialize Trainer Client\n",
    "Use token authentication to intialize a training client and list available runtimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31077051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available runtimes : 5\n",
      "- torch-cuda-241\n",
      "- torch-cuda-251\n",
      "- torch-cuda-custom\n",
      "- torch-rocm-241\n",
      "- torch-rocm-251\n"
     ]
    }
   ],
   "source": [
    "from kubeflow.trainer import TrainerClient\n",
    "from kubeflow.trainer.backends.kubernetes.types import KubernetesBackendConfig\n",
    "from kubernetes import client\n",
    "\n",
    "api_server = \"<api-server-url>\"\n",
    "token = \"<auth-token>\"\n",
    "\n",
    "configuration = client.Configuration()\n",
    "configuration.host = api_server\n",
    "configuration.api_key = {\"authorization\": f\"Bearer {token}\"}\n",
    "\n",
    "# Un-comment if your cluster API server uses a self-signed certificate or an un-trusted CA\n",
    "configuration.verify_ssl = False\n",
    "\n",
    "api_client = client.ApiClient(configuration)\n",
    "trainer_client = TrainerClient(backend_config= KubernetesBackendConfig(client_configuration=api_client.configuration))\n",
    "\n",
    "print(\"Available runtimes :\", len(trainer_client.list_runtimes()))\n",
    "for r in trainer_client.list_runtimes():\n",
    "    print(f\"- {r.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373b1160",
   "metadata": {},
   "source": [
    "### Create TrainJob\n",
    "Create a TrainJob using resources declared above - \n",
    "- Custom trainer\n",
    "- Dataset & Model initailisers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab961c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainjob submitted!!\n"
     ]
    }
   ],
   "source": [
    "job_name = trainer_client.train(\n",
    "    trainer=custom_trainer,\n",
    "    initializer=initializer,\n",
    "    runtime=trainer_client.get_runtime(\"torch-cuda-custom\")\n",
    ")\n",
    "print(\"Trainjob submitted!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd299d02",
   "metadata": {},
   "source": [
    "![pods](./docs/trainjobs_jobsets.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440c7242",
   "metadata": {},
   "source": [
    "![jobs](./docs/jobs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805900d1",
   "metadata": {},
   "source": [
    "### Start monitoring - View Training Logs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5613e350",
   "metadata": {},
   "source": [
    "![trainjob_pods](./docs/trainjob_pods.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e2a0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training logs\n",
    "try:    \n",
    "    # Get logs from the training nodes\n",
    "    logs = trainer_client.get_job_logs(job_name, follow=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TRAINING LOGS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Display logs - logs is a generator, not a dict\n",
    "    for log_line in logs:\n",
    "        if log_line.strip():\n",
    "            print(log_line)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error getting logs: {e}\")\n",
    "    print(\"Note: Logs may not be available yet if training is still starting up\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dae52b",
   "metadata": {},
   "source": [
    "### Cleanup resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5a2b3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final TrainJob Status:\n",
      "   Name: d5648a3bd444\n",
      "   Status: Complete\n",
      "   Created: 2025-10-06 15:08:34+00:00\n",
      "   Nodes: 2\n",
      "   Runtime: torch-cuda-custom\n",
      "   Steps:\n",
      "     - dataset-initializer: Succeeded\n",
      "     - model-initializer: Succeeded\n",
      "     - node-0: Succeeded\n",
      "     - node-1: Succeeded\n",
      "\n",
      "TrainJob 'd5648a3bd444' deleted successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainJob 'hbfe180e23f8' deleted successfully\n"
     ]
    }
   ],
   "source": [
    "# Clean up the TrainJob when done\n",
    "def cleanup_trainjob():\n",
    "    \"\"\"Clean up the TrainJob using Kubeflow SDK\"\"\"\n",
    "    try:\n",
    "        trainer_client.delete_job(job_name)\n",
    "        print(f\"TrainJob '{job_name}' deleted successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting TrainJob: {e}\")\n",
    "\n",
    "# Get final job status before cleanup\n",
    "try:\n",
    "    final_job = trainer_client.get_job(job_name)\n",
    "    print(f\"Final TrainJob Status:\")\n",
    "    print(f\"   Name: {final_job.name}\")\n",
    "    print(f\"   Status: {final_job.status}\")\n",
    "    print(f\"   Created: {final_job.creation_timestamp}\")\n",
    "    print(f\"   Nodes: {final_job.num_nodes}\")\n",
    "    print(f\"   Runtime: {final_job.runtime.name}\")\n",
    "    \n",
    "    if final_job.steps:\n",
    "        print(f\"   Steps:\")\n",
    "        for step in final_job.steps:\n",
    "            print(f\"     - {step.name}: {step.status}\")\n",
    "        print()\n",
    "        cleanup_trainjob()\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"Error getting final job status: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
