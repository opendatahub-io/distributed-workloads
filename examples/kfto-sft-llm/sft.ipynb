{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1b048ce-ba25-455d-83d4-9ad7c5cc247b",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f240038b-794b-4532-8ef4-329dd8db3a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the YAML magic\n",
    "!pip install yamlmagic\n",
    "%load_ext yamlmagic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29aed0af-9136-4f2f-9f6d-45413cb9b0a8",
   "metadata": {},
   "source": [
    "# Training Configuration\n",
    "\n",
    "Edit the following training parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc2b06f-50e4-49a0-b170-17b69feeade6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%yaml parameters\n",
    "\n",
    "# Model\n",
    "model_name_or_path: Meta-Llama/Meta-Llama-3.1-8B-Instruct\n",
    "model_revision: main\n",
    "torch_dtype: bfloat16\n",
    "attn_implementation: flash_attention_2    # one of eager (default), sdpa or flash_attention_2\n",
    "use_liger: false                          # use Liger kernels\n",
    "\n",
    "# PEFT / LoRA\n",
    "use_peft: true\n",
    "lora_r: 16\n",
    "lora_alpha: 8\n",
    "lora_dropout: 0.05\n",
    "lora_target_modules: [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "lora_modules_to_save: []\n",
    "\n",
    "# QLoRA (BitsAndBytes)\n",
    "load_in_4bit: false                       # use 4 bit precision for the base model (only with LoRA)\n",
    "load_in_8bit: false                       # use 8 bit precision for the base model (only with LoRA)\n",
    "\n",
    "# Dataset\n",
    "dataset_name: gsm8k                       # id or path to the dataset\n",
    "dataset_config: main                      # name of the dataset configuration\n",
    "dataset_train_split: train                # dataset split to use for training\n",
    "dataset_test_split: test                  # dataset split to use for evaluation\n",
    "dataset_text_field: text                  # name of the text field of the dataset\n",
    "dataset_kwargs:\n",
    "  add_special_tokens: false               # template with special tokens\n",
    "  append_concat_token: false              # add additional separator token\n",
    "\n",
    "# SFT\n",
    "max_seq_length: 1024                      # max sequence length for model and packing of the dataset\n",
    "dataset_batch_size: 1000                  # samples to tokenize per batch\n",
    "packing: false\n",
    "\n",
    "# Training\n",
    "num_train_epochs: 10                      # number of training epochs\n",
    "\n",
    "per_device_train_batch_size: 32           # batch size per device during training\n",
    "per_device_eval_batch_size: 32            # batch size for evaluation\n",
    "auto_find_batch_size: false               # find a batch size that fits into memory automatically\n",
    "eval_strategy: epoch                      # evaluate every epoch\n",
    "\n",
    "bf16: true                                # use bf16 16-bit (mixed) precision\n",
    "tf32: false                               # use tf32 precision\n",
    "\n",
    "learning_rate: 2.0e-4                     # initial learning rate\n",
    "warmup_steps: 10                          # steps for a linear warmup from 0 to `learning_rate`\n",
    "lr_scheduler_type: inverse_sqrt           # learning rate scheduler (see transformers.SchedulerType)\n",
    "# lr_scheduler_type: reduce_lr_on_plateau\n",
    "# lr_scheduler_kwargs:\n",
    "  # patience: 1\n",
    "  # factor: 0.2\n",
    "# metric_for_best_model: eval_loss\n",
    "\n",
    "optim: adamw_torch_fused                  # optimizer (see transformers.OptimizerNames)\n",
    "max_grad_norm: 1.0                        # max gradient norm\n",
    "seed: 42\n",
    "\n",
    "gradient_accumulation_steps: 1            # number of steps before performing a backward/update pass\n",
    "gradient_checkpointing: false             # use gradient checkpointing to save memory\n",
    "gradient_checkpointing_kwargs:\n",
    "  use_reentrant: false\n",
    "\n",
    "# FSDP\n",
    "fsdp: \"full_shard auto_wrap\"              # add offload if not enough GPU memory\n",
    "fsdp_config:\n",
    "  activation_checkpointing: true\n",
    "  cpu_ram_efficient_loading: false\n",
    "  sync_module_states: true\n",
    "  use_orig_params: true\n",
    "  limit_all_gathers: false\n",
    "\n",
    "# Checkpointing\n",
    "save_strategy: epoch                      # save checkpoint every epoch\n",
    "save_total_limit: 1                       # limit the total amount of checkpoints\n",
    "resume_from_checkpoint: false             # load the last checkpoint in output_dir and resume from it\n",
    "\n",
    "# Logging\n",
    "log_level: warning                        # logging level (see transformers.logging)\n",
    "logging_strategy: steps\n",
    "logging_steps: 1                          # log every N steps\n",
    "report_to:\n",
    "- tensorboard                             # report metrics to tensorboard\n",
    "\n",
    "output_dir: /mnt/shared/Meta-Llama-3.1-8B-Instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231ad92c-753a-4c57-af75-7fcdb40c7e14",
   "metadata": {},
   "source": [
    "# Training Loop\n",
    "\n",
    "Review the training function. You can adjust the chat template if needed depending on the model you want to fine-tune:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9157499-ee71-4dae-a299-feeeacff23be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(parameters):\n",
    "    import random\n",
    "\n",
    "    from datasets import load_dataset\n",
    "    from transformers import (\n",
    "        AutoTokenizer,\n",
    "        set_seed,\n",
    "    )\n",
    "\n",
    "    from trl import (\n",
    "        ModelConfig,\n",
    "        ScriptArguments,\n",
    "        SFTConfig,\n",
    "        SFTTrainer,\n",
    "        TrlParser,\n",
    "        get_peft_config,\n",
    "        get_quantization_config,\n",
    "        get_kbit_device_map,\n",
    "    )\n",
    "\n",
    "    parser = TrlParser((ScriptArguments, SFTConfig, ModelConfig))\n",
    "    script_args, training_args, model_args = parser.parse_dict(parameters)\n",
    "\n",
    "    # Set seed for reproducibility\n",
    "    set_seed(training_args.seed)\n",
    "\n",
    "    # Model and tokenizer\n",
    "    quantization_config = get_quantization_config(model_args)\n",
    "    model_kwargs = dict(\n",
    "        revision=model_args.model_revision,\n",
    "        trust_remote_code=model_args.trust_remote_code,\n",
    "        attn_implementation=model_args.attn_implementation,\n",
    "        torch_dtype=model_args.torch_dtype,\n",
    "        use_cache=False if training_args.gradient_checkpointing or\n",
    "                           training_args.fsdp_config.get(\"activation_checkpointing\",\n",
    "                                                         False) else True,\n",
    "        device_map=get_kbit_device_map() if quantization_config is not None else None,\n",
    "        quantization_config=quantization_config,\n",
    "    )\n",
    "    training_args.model_init_kwargs = model_kwargs\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_args.model_name_or_path, trust_remote_code=model_args.trust_remote_code, use_fast=True\n",
    "    )\n",
    "    if tokenizer.pad_token is None:\n",
    "        # Models like Llama 3 use a dedicated padding token\n",
    "        right_pad_id = tokenizer.convert_tokens_to_ids('<|finetune_right_pad_id|>')\n",
    "        if right_pad_id is not None:\n",
    "            tokenizer.pad_token = '<|finetune_right_pad_id|>'\n",
    "        else:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Chat template\n",
    "    # You may need to provide your own chat template if the model does not have a default one\n",
    "    # or if you want to customize it\n",
    "    # Llama 3 instruct template, make sure to add \"lm_head\" and \"embed_tokens\" layers to lora_modules_to_save\n",
    "    # LLAMA_3_CHAT_TEMPLATE=\"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\"\n",
    "    # Anthropic/Vicuna like template without the need for special tokens\n",
    "    # LLAMA_3_CHAT_TEMPLATE = (\n",
    "    #     \"{% for message in messages %}\"\n",
    "    #     \"{% if message['role'] == 'system' %}\"\n",
    "    #     \"{{ message['content'] }}\"\n",
    "    #     \"{% elif message['role'] == 'user' %}\"\n",
    "    #     \"{{ '\\n\\nHuman: ' + message['content'] +  eos_token }}\"\n",
    "    #     \"{% elif message['role'] == 'assistant' %}\"\n",
    "    #     \"{{ '\\n\\nAssistant: '  + message['content'] +  eos_token  }}\"\n",
    "    #     \"{% endif %}\"\n",
    "    #     \"{% endfor %}\"\n",
    "    #     \"{% if add_generation_prompt %}\"\n",
    "    #     \"{{ '\\n\\nAssistant: ' }}\"\n",
    "    #     \"{% endif %}\"\n",
    "    # )\n",
    "    # tokenizer.chat_template = LLAMA_3_CHAT_TEMPLATE\n",
    "\n",
    "    # Datasets\n",
    "    train_dataset = load_dataset(\n",
    "        path=script_args.dataset_name,\n",
    "        name=script_args.dataset_config,\n",
    "        split=script_args.dataset_train_split,\n",
    "    )\n",
    "    test_dataset = None\n",
    "    if training_args.eval_strategy != \"no\":\n",
    "        test_dataset = load_dataset(\n",
    "            path=script_args.dataset_name,\n",
    "            name=script_args.dataset_config,\n",
    "            split=script_args.dataset_test_split,\n",
    "        )\n",
    "\n",
    "    # Templatize datasets\n",
    "    # You may need to adjust the mapping between columns and the chat template\n",
    "    def template_dataset(sample):\n",
    "        # return {\"text\": tokenizer.apply_chat_template(examples[\"messages\"], tokenize=False)}\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": sample['question']},\n",
    "            {\"role\": \"assistant\", \"content\": sample['answer']},\n",
    "        ]\n",
    "        return {\"text\": tokenizer.apply_chat_template(messages, tokenize=False)}\n",
    "\n",
    "    train_dataset = train_dataset.map(template_dataset, remove_columns=[\"question\", \"answer\"])\n",
    "    if training_args.eval_strategy != \"no\":\n",
    "        # test_dataset = test_dataset.map(template_dataset, remove_columns=[\"messages\"])\n",
    "        test_dataset = test_dataset.map(template_dataset, remove_columns=[\"question\", \"answer\"])\n",
    "\n",
    "    # Check random samples\n",
    "    with training_args.main_process_first(\n",
    "        desc=\"Log few samples from the training set\"\n",
    "    ):\n",
    "        for index in random.sample(range(len(train_dataset)), 2):\n",
    "            print(train_dataset[index][\"text\"])\n",
    "\n",
    "    # Training\n",
    "    trainer = SFTTrainer(\n",
    "        model=model_args.model_name_or_path,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        peft_config=get_peft_config(model_args),\n",
    "        processing_class=tokenizer,\n",
    "    )\n",
    "\n",
    "    if trainer.accelerator.is_main_process and hasattr(trainer.model, \"print_trainable_parameters\"):\n",
    "        trainer.model.print_trainable_parameters()\n",
    "\n",
    "    checkpoint = None\n",
    "    if training_args.resume_from_checkpoint is not None:\n",
    "        checkpoint = training_args.resume_from_checkpoint\n",
    "\n",
    "    trainer.train(resume_from_checkpoint=checkpoint)\n",
    "\n",
    "    trainer.save_model(training_args.output_dir)\n",
    "\n",
    "    with training_args.main_process_first(desc=\"Training completed\"):\n",
    "        print(f\"Training completed, model checkpoint written to {training_args.output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827164af-a257-44f9-900d-051e1baeac4f",
   "metadata": {},
   "source": [
    "# Training Client\n",
    "\n",
    "Configure the SDK client by providing the authentication token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b7677f-ca8b-4c33-8e0e-996befbd6b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubernetes import client\n",
    "from kubeflow.training import TrainingClient\n",
    "from kubeflow.training.models import V1Volume, V1VolumeMount, V1PersistentVolumeClaimVolumeSource\n",
    "\n",
    "api_server = \"<API_SERVER>\"\n",
    "token = \"<TOKEN>\"\n",
    "\n",
    "configuration = client.Configuration()\n",
    "configuration.host = api_server\n",
    "configuration.api_key = {\"authorization\": f\"Bearer {token}\"}\n",
    "# Un-comment if your cluster API server uses a self-signed certificate or an un-trusted CA\n",
    "#configuration.verify_ssl = False\n",
    "api_client = client.ApiClient(configuration)\n",
    "client = TrainingClient(client_configuration=api_client.configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbe5c82-6bd5-4965-a1f2-b5f21c78672c",
   "metadata": {},
   "source": [
    "# Training Job\n",
    "\n",
    "You're now almost ready to create the training job:\n",
    "* Fill the `HF_TOKEN` environment variable with your HuggingFace token if you fine-tune a gated model \n",
    "* Check the number of worker nodes\n",
    "* Amend the resources per worker according to the job requirements\n",
    "* If you use AMD accelerators:\n",
    "  * Change `nvidia.com/gpu` to `amd.com/gpu` in `resources_per_worker`\n",
    "  * Change `base_image` to `quay.io/modh/training:py311-rocm62-torch251`\n",
    "* Update the PVC name to the one you've attached to the workbench if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac71c341-fa35-4cad-ab7f-765dd1f11b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.create_job(\n",
    "    job_kind=\"PyTorchJob\",\n",
    "    name=\"sft\",\n",
    "    train_func=main,\n",
    "    num_workers=8,\n",
    "    num_procs_per_worker=\"1\",\n",
    "    resources_per_worker={\n",
    "        \"nvidia.com/gpu\": 1,\n",
    "        \"memory\": \"64Gi\",\n",
    "        \"cpu\": 4,\n",
    "    },\n",
    "    base_image=\"quay.io/modh/training:py311-cuda124-torch251\",\n",
    "    env_vars={\n",
    "        # HuggingFace\n",
    "        \"HF_HOME\": \"/mnt/shared/.cache\",\n",
    "        \"HF_TOKEN\": \"\",\n",
    "        # CUDA / ROCm (HIP)\n",
    "        \"PYTORCH_CUDA_ALLOC_CONF\": \"expandable_segments:True\",\n",
    "        \"PYTORCH_HIP_ALLOC_CONF\": \"expandable_segments:True\",\n",
    "        # NCCL / RCCL\n",
    "        \"NCCL_DEBUG\": \"INFO\",\n",
    "    },\n",
    "    parameters=parameters,\n",
    "    volumes=[\n",
    "        V1Volume(name=\"shared\",\n",
    "                 persistent_volume_claim=V1PersistentVolumeClaimVolumeSource(claim_name=\"shared\")),\n",
    "    ],\n",
    "    volume_mounts=[\n",
    "        V1VolumeMount(name=\"shared\", mount_path=\"/mnt/shared\"),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d410b3-0f1b-4aee-9d6b-89bf68e51de9",
   "metadata": {},
   "source": [
    "Once the training job is created, you can follow its progress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06782d41-a842-4377-8718-ef7c781d3c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.get_job_logs(\n",
    "    name=\"sft\",\n",
    "    job_kind=\"PyTorchJob\",\n",
    "    follow=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83769854-2614-4d4d-8708-213d89a0dd0d",
   "metadata": {},
   "source": [
    "# TensorBoard\n",
    "\n",
    "You can track your job runs and visualize the training metrics with TensorBoard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbfc9f0-43f6-4a4d-946a-ab543ceccca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TENSORBOARD_PROXY_URL\"]= os.environ[\"NB_PREFIX\"]+\"/proxy/6006/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d5675e-2534-437d-ab76-e74a78e5043e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26884537-fc13-4644-bd2b-c255874a4a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir /opt/app-root/src/shared"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cef079e-5541-439d-b7c6-a6234822da90",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bb7a94",
   "metadata": {},
   "source": [
    "## Testing the Pre-Trained Model\n",
    "\n",
    "If you've configured the workbench with a NVIDIA GPU or AMD accelerator, you can run inferences to validate the output generated by the fine-tuned model and compare it to the output of the pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2be9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install / upgrade dependencies\n",
    "!pip install --upgrade transformers peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52431fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import transformers\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from peft import LoraConfig, PeftModel\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955d1383",
   "metadata": {},
   "source": [
    "Check / update the paths to the pre-trained and fine-tuned model checkpoints prior to executing the cells below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac57e2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model\n",
    "pretrained_path = \"/opt/app-root/src/shared/.cache/hub/models--Meta-Llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/\"\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_path,\n",
    "    local_files_only=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7caa5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_path)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "if base_model.config.pad_token_id is None:\n",
    "    base_model.config.pad_token_id = base_model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f255d4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the pre-trained model\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=base_model,\n",
    "    tokenizer=tokenizer,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Janet's ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\",\n",
    "    }\n",
    "]\n",
    "\n",
    "outputs = pipeline(messages, max_new_tokens=256, temperature = 0.01)\n",
    "\n",
    "output1 = \"\"\n",
    "for turn in outputs:\n",
    "    for item in turn[\"generated_text\"]:\n",
    "        output1 += f\"# {item['role']}\\n\\n{item['content']}\\n\\n\"\n",
    "\n",
    "display(Markdown(output1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30a524d",
   "metadata": {},
   "source": [
    "## Merging the LoRA adapters\n",
    "\n",
    "If you've configured the training to use LoRA, then you can merge the fine-tuned LoRA adapters / layers into the pre-trained model.\n",
    "\n",
    "Check / update the path to the fine-tuned model checkpoint prior to executing the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb97e61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the fine-tuned adapters into the base model \n",
    "finetuned_path = \"/opt/app-root/src/shared/Meta-Llama-3.1-8B-Instruct/checkpoint-300/\"\n",
    "model = PeftModel.from_pretrained(base_model, finetuned_path)\n",
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c25e21",
   "metadata": {},
   "source": [
    "## Testing the Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9e45ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the fine-tuned model\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=base_model,\n",
    "    tokenizer=tokenizer,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Janet's ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\",\n",
    "    }\n",
    "]\n",
    "\n",
    "outputs = pipeline(messages, max_new_tokens=256, temperature = 0.01)\n",
    "\n",
    "output2 = \"\"\n",
    "for turn in outputs:\n",
    "    for item in turn[\"generated_text\"]:\n",
    "        output2 += f\"# {item['role']}\\n\\n{item['content']}\\n\\n\"\n",
    "\n",
    "display(Markdown(output2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1e7a7d",
   "metadata": {},
   "source": [
    "# Cleaning Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dd3a87",
   "metadata": {},
   "source": [
    "## Training Job\n",
    "\n",
    "Once you're done or want to re-create the training job, you can delete the existing one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345997d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.delete_job(name=\"sft\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556d637c",
   "metadata": {},
   "source": [
    "## GPU Memory\n",
    "\n",
    "If you want to start over and test the pre-trained model again, you can free the GPU / accelerator memory with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c60b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unload the model from GPU memory\n",
    "import gc\n",
    "\n",
    "del base_model, model\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
