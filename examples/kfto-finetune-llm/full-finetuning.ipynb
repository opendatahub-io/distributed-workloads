{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b001b673-bf3f-4a0f-a542-85c35ba08723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove once kubeflow-training SDK is upgraded to 1.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90ae07cf-2570-4492-b689-f07acc420ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kubeflow-training in /opt/app-root/lib64/python3.11/site-packages (1.8.1)\n",
      "Collecting kubeflow-training\n",
      "  Downloading kubeflow_training-1.9.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: certifi>=14.05.14 in /opt/app-root/lib64/python3.11/site-packages (from kubeflow-training) (2024.12.14)\n",
      "Requirement already satisfied: six>=1.10 in /opt/app-root/lib64/python3.11/site-packages (from kubeflow-training) (1.17.0)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /opt/app-root/lib64/python3.11/site-packages (from kubeflow-training) (74.1.3)\n",
      "Requirement already satisfied: urllib3>=1.15.1 in /opt/app-root/lib64/python3.11/site-packages (from kubeflow-training) (1.26.20)\n",
      "Requirement already satisfied: kubernetes>=27.2.0 in /opt/app-root/lib64/python3.11/site-packages (from kubeflow-training) (30.1.0)\n",
      "Requirement already satisfied: retrying>=1.3.3 in /opt/app-root/lib64/python3.11/site-packages (from kubeflow-training) (1.3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /opt/app-root/lib64/python3.11/site-packages (from kubernetes>=27.2.0->kubeflow-training) (2.9.0.post0)\n",
      "Requirement already satisfied: pyyaml>=5.4.1 in /opt/app-root/lib64/python3.11/site-packages (from kubernetes>=27.2.0->kubeflow-training) (6.0.2)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /opt/app-root/lib64/python3.11/site-packages (from kubernetes>=27.2.0->kubeflow-training) (2.37.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/app-root/lib64/python3.11/site-packages (from kubernetes>=27.2.0->kubeflow-training) (1.8.0)\n",
      "Requirement already satisfied: requests in /opt/app-root/lib64/python3.11/site-packages (from kubernetes>=27.2.0->kubeflow-training) (2.32.3)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/app-root/lib64/python3.11/site-packages (from kubernetes>=27.2.0->kubeflow-training) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /opt/app-root/lib64/python3.11/site-packages (from kubernetes>=27.2.0->kubeflow-training) (3.2.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/app-root/lib64/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=27.2.0->kubeflow-training) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/app-root/lib64/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=27.2.0->kubeflow-training) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/app-root/lib64/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=27.2.0->kubeflow-training) (4.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/app-root/lib64/python3.11/site-packages (from requests->kubernetes>=27.2.0->kubeflow-training) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/app-root/lib64/python3.11/site-packages (from requests->kubernetes>=27.2.0->kubeflow-training) (3.10)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/app-root/lib64/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=27.2.0->kubeflow-training) (0.6.1)\n",
      "Downloading kubeflow_training-1.9.0-py3-none-any.whl (111 kB)\n",
      "Installing collected packages: kubeflow-training\n",
      "  Attempting uninstall: kubeflow-training\n",
      "    Found existing installation: kubeflow-training 1.8.1\n",
      "    Uninstalling kubeflow-training-1.8.1:\n",
      "      Successfully uninstalled kubeflow-training-1.8.1\n",
      "Successfully installed kubeflow-training-1.9.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade kubeflow-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2981bccf-799e-4bd5-89b9-c5a7110db04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies for local inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e43b3cc2-9711-4218-8375-923bb439f1ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting s3fs\n",
      "  Downloading s3fs-2025.2.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: transformers in /opt/app-root/lib64/python3.11/site-packages (4.38.0)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: filelock in /opt/app-root/lib64/python3.11/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/app-root/lib64/python3.11/site-packages (from datasets) (2.1.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/app-root/lib64/python3.11/site-packages (from datasets) (18.1.0)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /opt/app-root/lib64/python3.11/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/app-root/lib64/python3.11/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/app-root/lib64/python3.11/site-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /opt/app-root/lib64/python3.11/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /opt/app-root/lib64/python3.11/site-packages (from datasets) (3.11.11)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /opt/app-root/lib64/python3.11/site-packages (from datasets) (0.27.1)\n",
      "Requirement already satisfied: packaging in /opt/app-root/lib64/python3.11/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/app-root/lib64/python3.11/site-packages (from datasets) (6.0.2)\n",
      "Collecting aiobotocore<3.0.0,>=2.5.4 (from s3fs)\n",
      "  Downloading aiobotocore-2.20.0-py3-none-any.whl.metadata (23 kB)\n",
      "INFO: pip is looking at multiple versions of s3fs to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting s3fs\n",
      "  Downloading s3fs-2024.12.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/app-root/lib64/python3.11/site-packages (from transformers) (2024.11.6)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/app-root/lib64/python3.11/site-packages (from transformers) (0.5.2)\n",
      "Collecting aioitertools<1.0.0,>=0.5.1 (from aiobotocore<3.0.0,>=2.5.4->s3fs)\n",
      "  Downloading aioitertools-0.12.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting botocore<1.36.24,>=1.36.20 (from aiobotocore<3.0.0,>=2.5.4->s3fs)\n",
      "  Downloading botocore-1.36.23-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/app-root/lib64/python3.11/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (2.9.0.post0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/app-root/lib64/python3.11/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (1.0.1)\n",
      "Requirement already satisfied: multidict<7.0.0,>=6.0.0 in /opt/app-root/lib64/python3.11/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (6.1.0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /opt/app-root/lib64/python3.11/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (1.26.20)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in /opt/app-root/lib64/python3.11/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (1.17.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/app-root/lib64/python3.11/site-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/app-root/lib64/python3.11/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/app-root/lib64/python3.11/site-packages (from aiohttp->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/app-root/lib64/python3.11/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/app-root/lib64/python3.11/site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/app-root/lib64/python3.11/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/app-root/lib64/python3.11/site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/app-root/lib64/python3.11/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/app-root/lib64/python3.11/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/app-root/lib64/python3.11/site-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/app-root/lib64/python3.11/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/app-root/lib64/python3.11/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/app-root/lib64/python3.11/site-packages (from python-dateutil<3.0.0,>=2.1->aiobotocore<3.0.0,>=2.5.4->s3fs) (1.17.0)\n",
      "Downloading datasets-3.3.2-py3-none-any.whl (485 kB)\n",
      "Downloading s3fs-2024.12.0-py3-none-any.whl (30 kB)\n",
      "Downloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m467.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiobotocore-2.20.0-py3-none-any.whl (78 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m617.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Downloading aioitertools-0.12.0-py3-none-any.whl (24 kB)\n",
      "Downloading botocore-1.36.23-py3-none-any.whl (13.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m645.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xxhash, dill, aioitertools, multiprocess, botocore, tokenizers, aiobotocore, transformers, s3fs, datasets\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.9\n",
      "    Uninstalling dill-0.3.9:\n",
      "      Successfully uninstalled dill-0.3.9\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.35.99\n",
      "    Uninstalling botocore-1.35.99:\n",
      "      Successfully uninstalled botocore-1.35.99\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.15.2\n",
      "    Uninstalling tokenizers-0.15.2:\n",
      "      Successfully uninstalled tokenizers-0.15.2\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.38.0\n",
      "    Uninstalling transformers-4.38.0:\n",
      "      Successfully uninstalled transformers-4.38.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "boto3 1.35.99 requires botocore<1.36.0,>=1.35.99, but you have botocore 1.36.23 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed aiobotocore-2.20.0 aioitertools-0.12.0 botocore-1.36.23 datasets-3.3.2 dill-0.3.8 multiprocess-0.70.16 s3fs-2024.12.0 tokenizers-0.21.0 transformers-4.49.0 xxhash-3.5.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade datasets s3fs transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1222715-691b-4f9e-81cf-ac8a4e300831",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_func(parameters):\n",
    "    import os\n",
    "    from transformers import (\n",
    "        AutoModelForCausalLM,\n",
    "        AutoTokenizer,\n",
    "    )\n",
    "    from trl import SFTTrainer, SFTConfig\n",
    "    from datasets import load_dataset\n",
    "    import s3fs\n",
    "\n",
    "    rank = int(os.environ[\"RANK\"])\n",
    "\n",
    "    model_name = parameters[\"model_name\"]\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        pretrained_model_name_or_path=model_name,\n",
    "    )\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    def format_dataset(example):\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": example['question']},\n",
    "            {\"role\": \"assistant\", \"content\": example['answer']}\n",
    "        ]\n",
    "        prompt = tokenizer.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=False\n",
    "        )\n",
    "        return {\"prompt\": prompt}\n",
    "\n",
    "    dataset = load_dataset(parameters[\"dataset_name\"], parameters[\"dataset_subset\"])\n",
    "    train_data = dataset[\"train\"].map(format_dataset, remove_columns=['question', 'answer'])\n",
    "    eval_data = dataset[\"test\"].map(format_dataset, remove_columns=['question', 'answer'])\n",
    "\n",
    "    training_args = SFTConfig(\n",
    "        dataset_text_field=\"prompt\",\n",
    "        max_seq_length=1024,\n",
    "        output_dir=\"/tmp\",\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=1,\n",
    "        learning_rate=5e-7,\n",
    "        logging_dir=\"/logs\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"no\",\n",
    "        fsdp=\"full_shard\",\n",
    "        fsdp_config={\n",
    "            \"fsdp_state_dict_type\": \"SHARDED_STATE_DICT\",\n",
    "            \"fsdp_sharding_strategy\": \"FULL_SHARD\",\n",
    "        },\n",
    "    )\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=model_name,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=eval_data,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_args,\n",
    "    )\n",
    "\n",
    "    # Train and save the model.\n",
    "    trainer.train()\n",
    "\n",
    "    # https://github.com/huggingface/transformers/issues/30491\n",
    "    if trainer.is_fsdp_enabled:\n",
    "        trainer.accelerator.state.fsdp_plugin.set_state_dict_type(\"FULL_STATE_DICT\")\n",
    "\n",
    "    if rank == 0:\n",
    "        save_model_path = \"./saved-model\"\n",
    "        trainer.save_model(save_model_path)\n",
    "        # Store trained model on AWS\n",
    "        s3 = s3fs.S3FileSystem()\n",
    "        s3_path = os.environ[\"AWS_S3_BUCKET\"] + '/saved-model'\n",
    "        s3.put(save_model_path, s3_path, recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba2fa8dc-9aaf-4c6b-a44f-500f40909f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubeflow.training import TrainingClient\n",
    "from kubernetes import client\n",
    "from kubernetes.client import (\n",
    "    V1EnvVar,\n",
    "    V1EnvVarSource,\n",
    "    V1SecretKeySelector\n",
    ")\n",
    "\n",
    "job_name = \"pytorch-fsdp\"\n",
    "\n",
    "# aws_connection_name value should be the same as connection name in Data science project where the Workbench is running\n",
    "aws_connection_name = \"workbench-aws\"\n",
    "\n",
    "parameters = {\n",
    "    'model_name': 'meta-llama/Llama-3.2-1B-Instruct',\n",
    "    'dataset_name': 'openai/gsm8k',\n",
    "    'dataset_subset': 'main',\n",
    "}\n",
    "\n",
    "# Provide URL and token with all needed rights\n",
    "# On OpenShift, you can retrieve the token by running `oc whoami -t`,\n",
    "# and the server with `oc cluster-info`.\n",
    "\n",
    "# token = \"\"\n",
    "# openshift_api_url = \"\"\n",
    "\n",
    "# api_key = {\"authorization\": \"Bearer \" + token}\n",
    "# config = client.Configuration(host=openshift_api_url, api_key=api_key)\n",
    "# config.verify_ssl = False\n",
    "# tc = TrainingClient(client_configuration=config)\n",
    "\n",
    "\n",
    "# Alternatively add edit role for user running this Notebook using oc CLI:\n",
    "# oc adm policy add-role-to-user edit system:serviceaccount:<namespace>:<workbench name> -n <namespace>\n",
    "tc = TrainingClient()\n",
    "\n",
    "tc.create_job(\n",
    "    job_kind=\"PyTorchJob\",\n",
    "    name=job_name,\n",
    "    train_func=train_func,\n",
    "    num_workers=2,\n",
    "    num_procs_per_worker=\"auto\",\n",
    "    resources_per_worker={\"gpu\": 2},\n",
    "    base_image=\"quay.io/modh/training:py311-cuda121-torch241\",\n",
    "    parameters=parameters,\n",
    "    env_vars=[\n",
    "        V1EnvVar(name=\"HF_TOKEN\", value_from=V1EnvVarSource(secret_key_ref=V1SecretKeySelector(key=\"HF_TOKEN\", name=\"hf-token\"))),\n",
    "        V1EnvVar(name=\"NCCL_DEBUG\", value=\"INFO\"),\n",
    "        V1EnvVar(name=\"AWS_ACCESS_KEY_ID\", value_from=V1EnvVarSource(secret_key_ref=V1SecretKeySelector(key=\"AWS_ACCESS_KEY_ID\", name=aws_connection_name))),\n",
    "        V1EnvVar(name=\"AWS_S3_BUCKET\", value_from=V1EnvVarSource(secret_key_ref=V1SecretKeySelector(key=\"AWS_S3_BUCKET\", name=aws_connection_name))),\n",
    "        V1EnvVar(name=\"AWS_S3_ENDPOINT\", value_from=V1EnvVarSource(secret_key_ref=V1SecretKeySelector(key=\"AWS_S3_ENDPOINT\", name=aws_connection_name))),\n",
    "        V1EnvVar(name=\"AWS_SECRET_ACCESS_KEY\", value_from=V1EnvVarSource(secret_key_ref=V1SecretKeySelector(key=\"AWS_SECRET_ACCESS_KEY\", name=aws_connection_name))),\n",
    "    ],\n",
    "    packages_to_install=[\n",
    "        \"s3fs\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d731d9d4-d8d4-47a9-b204-497c97b86ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pod pytorch-fsdp-master-0]: ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "[Pod pytorch-fsdp-master-0]: datasets 3.3.2 requires fsspec[http]<=2024.12.0,>=2023.1.0, but you have fsspec 2025.2.0 which is incompatible.\n",
      "[Pod pytorch-fsdp-master-0]: W0226 10:19:47.488000 140075600308032 torch/distributed/run.py:779] \n",
      "[Pod pytorch-fsdp-master-0]: W0226 10:19:47.488000 140075600308032 torch/distributed/run.py:779] *****************************************\n",
      "[Pod pytorch-fsdp-master-0]: W0226 10:19:47.488000 140075600308032 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "[Pod pytorch-fsdp-master-0]: W0226 10:19:47.488000 140075600308032 torch/distributed/run.py:779] *****************************************\n",
      "Generating train split: 100%|██████████| 7473/7473 [00:00<00:00, 479442.51 examples/s]\n",
      "Generating test split: 100%|██████████| 1319/1319 [00:00<00:00, 365577.68 examples/s]\n",
      "Map: 100%|██████████| 7473/7473 [00:00<00:00, 10323.28 examples/s]\n",
      "Map: 100%|██████████| 7473/7473 [00:00<00:00, 10105.60 examples/s]\n",
      "Map: 100%|██████████| 1319/1319 [00:00<00:00, 10881.05 examples/s]\n",
      "Map: 100%|██████████| 1319/1319 [00:00<00:00, 9848.40 examples/s] \n",
      "[Pod pytorch-fsdp-master-0]: /tmp/tmp.y62MiTeO8x/ephemeral_script.py:51: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "[Pod pytorch-fsdp-master-0]:   trainer = SFTTrainer(\n",
      "[Pod pytorch-fsdp-master-0]: /tmp/tmp.y62MiTeO8x/ephemeral_script.py:51: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "[Pod pytorch-fsdp-master-0]:   trainer = SFTTrainer(\n",
      "Converting train dataset to ChatML: 100%|██████████| 7473/7473 [00:00<00:00, 45609.83 examples/s]\n",
      "Applying chat template to train dataset: 100%|██████████| 7473/7473 [00:00<00:00, 45277.05 examples/s]\n",
      "Tokenizing train dataset: 100%|██████████| 7473/7473 [00:03<00:00, 2411.30 examples/s]\n",
      "Tokenizing train dataset: 100%|██████████| 7473/7473 [00:01<00:00, 4433.39 examples/s]\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:77:77 [0] NCCL INFO Bootstrap : Using eth0:10.129.5.55<0>\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:77:77 [0] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:77:77 [0] NCCL INFO cudaDriverVersion 12040\n",
      "[Pod pytorch-fsdp-master-0]: NCCL version 2.20.5+cuda12.4\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:78:78 [1] NCCL INFO cudaDriverVersion 12040\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:78:78 [1] NCCL INFO Bootstrap : Using eth0:10.129.5.55<0>\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:78:78 [1] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:77:189 [0] NCCL INFO Failed to open libibverbs.so[.1]\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:77:189 [0] NCCL INFO NET/Socket : Using [0]eth0:10.129.5.55<0>\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:77:189 [0] NCCL INFO Using non-device net plugin version 0\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:77:189 [0] NCCL INFO Using network Socket\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:78:190 [1] NCCL INFO Failed to open libibverbs.so[.1]\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:78:190 [1] NCCL INFO NET/Socket : Using [0]eth0:10.129.5.55<0>\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:78:190 [1] NCCL INFO Using non-device net plugin version 0\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:78:190 [1] NCCL INFO Using network Socket\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:78:190 [1] NCCL INFO comm 0x56474f4cda90 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId e080 commId 0xd78b2ca10cb2bfc7 - Init START\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:77:189 [0] NCCL INFO comm 0x55f72114f8b0 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId c050 commId 0xd78b2ca10cb2bfc7 - Init START\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:77:189 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ffffff00,00000000\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:78:190 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffff00,00000000\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:78:190 [1] NCCL INFO comm 0x56474f4cda90 rank 1 nRanks 4 nNodes 2 localRanks 2 localRank 1 MNNVL 0\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:78:190 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:78:190 [1] NCCL INFO P2P Chunksize set to 131072\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:77:189 [0] NCCL INFO comm 0x55f72114f8b0 rank 0 nRanks 4 nNodes 2 localRanks 2 localRank 0 MNNVL 0\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:77:189 [0] NCCL INFO Channel 00/02 :    0   1   2   3\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:77:189 [0] NCCL INFO Channel 01/02 :    0   1   2   3\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:77:189 [0] NCCL INFO Trees [0] 1/2/-1->0->-1 [1] 1/-1/-1->0->2\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:77:189 [0] NCCL INFO P2P Chunksize set to 131072\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:77:189 [0] NCCL INFO Channel 00/0 : 3[1] -> 0[0] [receive] via NET/Socket/0\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:78:190 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[0] [send] via NET/Socket/0\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:78:190 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[0] [send] via NET/Socket/0\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:77:189 [0] NCCL INFO Channel 01/0 : 3[1] -> 0[0] [receive] via NET/Socket/0\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:77:189 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/CUMEM/read\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:77:189 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/CUMEM/read\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:77:189 [0] NCCL INFO Connected all rings\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:77:189 [0] NCCL INFO Channel 00/0 : 2[0] -> 0[0] [receive] via NET/Socket/0\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:77:189 [0] NCCL INFO Channel 01/0 : 2[0] -> 0[0] [receive] via NET/Socket/0\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:77:189 [0] NCCL INFO Channel 00/0 : 0[0] -> 2[0] [send] via NET/Socket/0\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:77:189 [0] NCCL INFO Channel 01/0 : 0[0] -> 2[0] [send] via NET/Socket/0\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:78:190 [1] NCCL INFO Connected all rings\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:78:190 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:78:190 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:78:190 [1] NCCL INFO Connected all trees\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:77:189 [0] NCCL INFO Connected all trees\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:78:190 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:78:190 [1] NCCL INFO 2 coll channels, 0 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:77:189 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:77:189 [0] NCCL INFO 2 coll channels, 0 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:77:189 [0] NCCL INFO comm 0x55f72114f8b0 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId c050 commId 0xd78b2ca10cb2bfc7 - Init COMPLETE\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:78:190 [1] NCCL INFO comm 0x56474f4cda90 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId e080 commId 0xd78b2ca10cb2bfc7 - Init COMPLETE\n",
      "Converting eval dataset to ChatML: 100%|██████████| 1319/1319 [00:00<00:00, 42396.90 examples/s]\n",
      "Applying chat template to eval dataset: 100%|██████████| 1319/1319 [00:00<00:00, 40456.37 examples/s]\n",
      "Tokenizing eval dataset: 100%|██████████| 1319/1319 [00:00<00:00, 2427.73 examples/s]]\n",
      "Tokenizing eval dataset: 100%|██████████| 1319/1319 [00:00<00:00, 4569.68 examples/s]]\n",
      "Tokenizing train dataset: 100%|██████████| 7473/7473 [00:03<00:00, 2065.04 examples/s]\n",
      "Tokenizing train dataset: 100%|██████████| 7473/7473 [00:01<00:00, 4415.35 examples/s]\n",
      "Tokenizing eval dataset:  48%|████▊     | 631/1319 [00:00<00:00, 2507.26 examples/s][2025-02-26 10:21:34,585] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Tokenizing eval dataset:  67%|██████▋   | 888/1319 [00:00<00:00, 2527.97 examples/s]df: /opt/app-root/src/.triton/autotune: No such file or directory\n",
      "Tokenizing eval dataset: 100%|██████████| 1319/1319 [00:00<00:00, 2024.96 examples/s]\n",
      "Tokenizing eval dataset: 100%|██████████| 1319/1319 [00:00<00:00, 3831.72 examples/s]\n",
      "[Pod pytorch-fsdp-master-0]: [2025-02-26 10:21:35,899] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "100%|██████████| 234/234 [25:12<00:00,  6.63s/it]\n",
      "  0%|          | 0/42 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|▍         | 2/42 [00:00<00:08,  4.93it/s]\u001b[A\n",
      "  7%|▋         | 3/42 [00:00<00:11,  3.40it/s]\u001b[A\n",
      " 10%|▉         | 4/42 [00:01<00:13,  2.79it/s]\u001b[A\n",
      " 12%|█▏        | 5/42 [00:01<00:15,  2.44it/s]\u001b[A\n",
      " 14%|█▍        | 6/42 [00:02<00:14,  2.44it/s]\u001b[A\n",
      " 17%|█▋        | 7/42 [00:02<00:14,  2.48it/s]\u001b[A\n",
      " 19%|█▉        | 8/42 [00:03<00:14,  2.41it/s]\u001b[A\n",
      " 21%|██▏       | 9/42 [00:03<00:14,  2.26it/s]\u001b[A\n",
      " 24%|██▍       | 10/42 [00:03<00:13,  2.41it/s]\u001b[A\n",
      " 26%|██▌       | 11/42 [00:04<00:14,  2.16it/s]\u001b[A\n",
      " 29%|██▊       | 12/42 [00:04<00:12,  2.32it/s]\u001b[A\n",
      " 31%|███       | 13/42 [00:05<00:12,  2.35it/s]\u001b[A\n",
      " 33%|███▎      | 14/42 [00:05<00:11,  2.38it/s]\u001b[A\n",
      " 36%|███▌      | 15/42 [00:06<00:11,  2.39it/s]\u001b[A\n",
      " 38%|███▊      | 16/42 [00:06<00:10,  2.50it/s]\u001b[A\n",
      " 40%|████      | 17/42 [00:06<00:09,  2.59it/s]\u001b[A\n",
      " 43%|████▎     | 18/42 [00:07<00:09,  2.63it/s]\u001b[A\n",
      " 45%|████▌     | 19/42 [00:07<00:09,  2.54it/s]\u001b[A\n",
      " 48%|████▊     | 20/42 [00:07<00:08,  2.49it/s]\u001b[A\n",
      " 50%|█████     | 21/42 [00:08<00:08,  2.58it/s]\u001b[A\n",
      " 52%|█████▏    | 22/42 [00:08<00:07,  2.51it/s]\u001b[A\n",
      " 55%|█████▍    | 23/42 [00:09<00:07,  2.41it/s]\u001b[A\n",
      " 57%|█████▋    | 24/42 [00:09<00:07,  2.39it/s]\u001b[A\n",
      " 60%|█████▉    | 25/42 [00:10<00:07,  2.31it/s]\u001b[A\n",
      " 62%|██████▏   | 26/42 [00:10<00:06,  2.29it/s]\u001b[A\n",
      " 64%|██████▍   | 27/42 [00:10<00:06,  2.44it/s]\u001b[A\n",
      " 67%|██████▋   | 28/42 [00:11<00:06,  2.32it/s]\u001b[A\n",
      " 69%|██████▉   | 29/42 [00:11<00:05,  2.31it/s]\u001b[A\n",
      " 71%|███████▏  | 30/42 [00:12<00:05,  2.34it/s]\u001b[A\n",
      " 74%|███████▍  | 31/42 [00:12<00:04,  2.35it/s]\u001b[A\n",
      " 76%|███████▌  | 32/42 [00:13<00:04,  2.16it/s]\u001b[A\n",
      " 79%|███████▊  | 33/42 [00:13<00:04,  2.16it/s]\u001b[A\n",
      " 81%|████████  | 34/42 [00:14<00:03,  2.05it/s]\u001b[A\n",
      " 83%|████████▎ | 35/42 [00:14<00:03,  2.04it/s]\u001b[A\n",
      " 86%|████████▌ | 36/42 [00:15<00:02,  2.14it/s]\u001b[A\n",
      " 88%|████████▊ | 37/42 [00:15<00:02,  2.11it/s]\u001b[A\n",
      " 90%|█████████ | 38/42 [00:16<00:01,  2.06it/s]\u001b[A\n",
      " 93%|█████████▎| 39/42 [00:16<00:01,  2.22it/s]\u001b[A\n",
      " 95%|█████████▌| 40/42 [00:16<00:00,  2.20it/s]\u001b[A\n",
      " 98%|█████████▊| 41/42 [00:17<00:00,  2.30it/s]\u001b[A\n",
      "                                                 A\n",
      "\u001b[A{'eval_loss': 1.044976830482483, 'eval_runtime': 20.6766, 'eval_samples_per_second': 63.792, 'eval_steps_per_second': 2.031, 'eval_mean_token_accuracy': 0.7636893693951593, 'epoch': 1.0}\n",
      "100%|██████████| 234/234 [25:33<00:00,  6.63s/it]\n",
      "100%|██████████| 42/42 [00:17<00:00,  2.42it/s]\u001b[A\n",
      "                                               \u001b[A{'train_runtime': 1537.9405, 'train_samples_per_second': 4.859, 'train_steps_per_second': 0.152, 'train_loss': 1.0673255594367654, 'epoch': 1.0}\n",
      "100%|██████████| 234/234 [25:33<00:00,  6.55s/it]\n",
      "[Pod pytorch-fsdp-master-0]: /opt/app-root/lib64/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n",
      "[Pod pytorch-fsdp-master-0]:   warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "logs, _ = tc.get_job_logs(job_name, follow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cb4d19f-b172-45b7-bf30-418bd2922982",
   "metadata": {},
   "outputs": [],
   "source": [
    "tc.delete_job(name=job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7a30b92-b9b1-4c40-aacf-5023f70bc872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import s3fs\n",
    "import os\n",
    "\n",
    "# Download trained model into local filesystem\n",
    "s3 = s3fs.S3FileSystem()\n",
    "s3_path = os.environ[\"AWS_S3_BUCKET\"] + '/saved-model'\n",
    "s3.get(s3_path, \"./saved-model\", recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b211d79-aa45-4230-9e23-648e6c7c4b83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41577672827c41c0a9bae516bbc0c43a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\n",
      "\n",
      "Original answer:\n",
      "Janet sells 16 - 3 - 4 = <<16-3-4=9>>9 duck eggs a day.\n",
      "She makes 9 * 2 = $<<9*2=18>>18 every day at the farmer’s market.\n",
      "#### 18\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model answer:\n",
      "To find out how much Janet makes at the farmers' market, we need to calculate the number of eggs she sells and then multiply that by the price per egg.\n",
      "\n",
      "She lays 16 eggs per day. After eating 3 for breakfast, she has 16 - 3 = 13 eggs left.\n",
      "\n",
      "She bakes muffins for 4 eggs per day. So, she uses 13 - 4 = 9 eggs for baking muffins.\n",
      "\n",
      "She sells the remaining 9 eggs at the farmers' market for $2 per egg. So, she makes 9 x $2 = $18 per day.\n",
      "\n",
      "Therefore, Janet makes $18 every day at the farmers' market.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model answer:\n",
      "She eats 3 eggs for breakfast every morning, so she eats 3 eggs/day * 16 eggs/day = <<3*16=48>>48 eggs/day\n",
      "She bakes muffins for her friends every day with 4 muffins, so she bakes 4 muffins/day * 16 muffins/day = <<4*16=64>>64 muffins/day\n",
      "She sells the remainder at the farmers' market daily for $2 per fresh duck egg, so she sells 64 muffins/day - 48 eggs/day = <<64-48=16>>16 muffins/day\n",
      "She sells 16 muffins/day at the farmers' market for $2 per muffin, so she makes 16 muffins/day * $2/muffin = $<<16*2=32>>32/day\n",
      "#### 32\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run inference on base model and trained model\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    pipeline,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "def infer_answer(model, tokenizer, question):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": question},\n",
    "    ]\n",
    "    pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0, temperature = 0.01)\n",
    "    prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    outputs = pipe(messages, max_new_tokens=256)\n",
    "    return outputs[0]['generated_text'][-1]['content']\n",
    "\n",
    "\n",
    "dataset = load_dataset(parameters[\"dataset_name\"], parameters[\"dataset_subset\"])\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=parameters[\"model_name\"],\n",
    ").to(\"cuda\")\n",
    "\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path=parameters[\"model_name\"],\n",
    ")\n",
    "base_tokenizer.pad_token = base_tokenizer.eos_token\n",
    "\n",
    "trained_model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"./saved-model\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "trained_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"./saved-model\",\n",
    ")\n",
    "trained_tokenizer.pad_token = trained_tokenizer.eos_token\n",
    "\n",
    "print(f\"Query:\\n{dataset['test']['question'][0]}\\n\")\n",
    "print(f\"Original answer:\\n{dataset['test']['answer'][0]}\\n\")\n",
    "\n",
    "test_prompt_base_answer = infer_answer(base_model, base_tokenizer, dataset['test']['question'][0])\n",
    "print(f\"Base model answer:\\n{test_prompt_base_answer}\\n\")\n",
    "\n",
    "test_prompt_base_answer = infer_answer(trained_model, trained_tokenizer, dataset['test']['question'][0])\n",
    "print(f\"Trained model answer:\\n{test_prompt_base_answer}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "644fd89b-1d58-498d-b83d-80613c7c1474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unload the model from GPU memory\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "del base_model, trained_model\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79039eb0-ed85-453c-8c61-e8534f392d29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
