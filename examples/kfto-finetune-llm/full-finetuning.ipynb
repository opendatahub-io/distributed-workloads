{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b001b673-bf3f-4a0f-a542-85c35ba08723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove once kubeflow-training SDK is upgraded to 1.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90ae07cf-2570-4492-b689-f07acc420ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kubeflow-training in /opt/app-root/lib64/python3.11/site-packages (1.9.0)\n",
      "Requirement already satisfied: certifi>=14.05.14 in /opt/app-root/lib64/python3.11/site-packages (from kubeflow-training) (2024.12.14)\n",
      "Requirement already satisfied: six>=1.10 in /opt/app-root/lib64/python3.11/site-packages (from kubeflow-training) (1.17.0)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /opt/app-root/lib64/python3.11/site-packages (from kubeflow-training) (74.1.3)\n",
      "Requirement already satisfied: urllib3>=1.15.1 in /opt/app-root/lib64/python3.11/site-packages (from kubeflow-training) (1.26.20)\n",
      "Requirement already satisfied: kubernetes>=27.2.0 in /opt/app-root/lib64/python3.11/site-packages (from kubeflow-training) (30.1.0)\n",
      "Requirement already satisfied: retrying>=1.3.3 in /opt/app-root/lib64/python3.11/site-packages (from kubeflow-training) (1.3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /opt/app-root/lib64/python3.11/site-packages (from kubernetes>=27.2.0->kubeflow-training) (2.9.0.post0)\n",
      "Requirement already satisfied: pyyaml>=5.4.1 in /opt/app-root/lib64/python3.11/site-packages (from kubernetes>=27.2.0->kubeflow-training) (6.0.2)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /opt/app-root/lib64/python3.11/site-packages (from kubernetes>=27.2.0->kubeflow-training) (2.37.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/app-root/lib64/python3.11/site-packages (from kubernetes>=27.2.0->kubeflow-training) (1.8.0)\n",
      "Requirement already satisfied: requests in /opt/app-root/lib64/python3.11/site-packages (from kubernetes>=27.2.0->kubeflow-training) (2.32.3)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/app-root/lib64/python3.11/site-packages (from kubernetes>=27.2.0->kubeflow-training) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /opt/app-root/lib64/python3.11/site-packages (from kubernetes>=27.2.0->kubeflow-training) (3.2.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/app-root/lib64/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=27.2.0->kubeflow-training) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/app-root/lib64/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=27.2.0->kubeflow-training) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/app-root/lib64/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=27.2.0->kubeflow-training) (4.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/app-root/lib64/python3.11/site-packages (from requests->kubernetes>=27.2.0->kubeflow-training) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/app-root/lib64/python3.11/site-packages (from requests->kubernetes>=27.2.0->kubeflow-training) (3.10)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/app-root/lib64/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=27.2.0->kubeflow-training) (0.6.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade kubeflow-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2981bccf-799e-4bd5-89b9-c5a7110db04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies for local inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43b3cc2-9711-4218-8375-923bb439f1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade datasets s3fs transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a1222715-691b-4f9e-81cf-ac8a4e300831",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_func():\n",
    "    import os\n",
    "    import logging\n",
    "    from transformers import (\n",
    "        AutoModelForCausalLM,\n",
    "        AutoTokenizer,\n",
    "        TrainingArguments,\n",
    "        DataCollatorForLanguageModeling,\n",
    "        pipeline,\n",
    "    )\n",
    "    from trl import SFTTrainer, SFTConfig\n",
    "    from datasets import load_dataset\n",
    "    from datasets.distributed import split_dataset_by_node\n",
    "    from peft import LoraConfig, get_peft_model\n",
    "    from accelerate import PartialState\n",
    "    import s3fs\n",
    "\n",
    "    rank = int(os.environ[\"RANK\"])\n",
    "\n",
    "    model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        pretrained_model_name_or_path=model_name,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        pretrained_model_name_or_path=model_name,\n",
    "    )\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    def format_dataset(example):\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": example['question']},\n",
    "            {\"role\": \"assistant\", \"content\": example['answer']}\n",
    "        ]\n",
    "        prompt = tokenizer.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=False\n",
    "        )\n",
    "        return {\"prompt\": prompt}\n",
    "\n",
    "    # Use pipeline to retrieve sample response for test sample, to visually review training progress\n",
    "    def infer_answer(model, question):\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "        ]\n",
    "        pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0, temperature = 0.01)\n",
    "        prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        outputs = pipe(messages, max_new_tokens=256)\n",
    "        return outputs[0]['generated_text'][-1]['content']\n",
    "\n",
    "    dataset = load_dataset(\"openai/gsm8k\", \"main\")\n",
    "    train_data = dataset[\"train\"].map(format_dataset, remove_columns=['question', 'answer'])\n",
    "    eval_data = dataset[\"test\"].map(format_dataset, remove_columns=['question', 'answer'])\n",
    "\n",
    "    training_args = SFTConfig(\n",
    "        dataset_text_field=\"prompt\",\n",
    "        max_seq_length=1024,\n",
    "        output_dir=\"/tmp\",\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=3,\n",
    "        learning_rate=5e-7,\n",
    "        logging_dir=\"/logs\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"no\",\n",
    "        fsdp=\"full_shard\",\n",
    "        fsdp_config={\n",
    "            \"fsdp_state_dict_type\": \"SHARDED_STATE_DICT\",\n",
    "            \"fsdp_sharding_strategy\": \"FULL_SHARD\",\n",
    "        },\n",
    "    )\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=eval_data,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_args,\n",
    "    )\n",
    "\n",
    "    # Check model response for sample test data to see how untrained model responds\n",
    "    if rank == 0:\n",
    "        test_prompt_answer = infer_answer(model, dataset['test']['question'][0])\n",
    "        print(f\"Query:\\n{dataset['test']['question'][0]}\")\n",
    "        print(f\"Original Answer:\\n{dataset['test']['answer'][0]}\")\n",
    "        print(f\"Generated Answer:\\n{test_prompt_answer}\")\n",
    "\n",
    "    # Train and save the model.\n",
    "    trainer.train()\n",
    "\n",
    "    # https://github.com/huggingface/transformers/issues/30491\n",
    "    if trainer.is_fsdp_enabled:\n",
    "        trainer.accelerator.state.fsdp_plugin.set_state_dict_type(\"FULL_STATE_DICT\")\n",
    "\n",
    "    if rank == 0:\n",
    "        save_model_path = \"./saved-model\"\n",
    "        trainer.save_model(save_model_path)\n",
    "        # Store trained model on AWS\n",
    "        s3 = s3fs.S3FileSystem()\n",
    "        s3_path = os.environ[\"AWS_S3_BUCKET\"] + '/saved-model'\n",
    "        s3.put(save_model_path, s3_path, recursive=True)\n",
    "\n",
    "    # TODO remove before merging to examples\n",
    "    print(\"parallel_mode: \" + str(trainer.args.parallel_mode))\n",
    "    print(\"is_model_parallel: \" + str(trainer.is_model_parallel))\n",
    "    print(\"model_wrapped: \" + str(trainer.model_wrapped))\n",
    "\n",
    "    # Check model response for sample test data to see how trained model responds\n",
    "    if rank == 0:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            pretrained_model_name_or_path=\"./saved-model/\",\n",
    "        )\n",
    "        test_prompt_answer = infer_answer(model, dataset['test']['question'][0])\n",
    "        print(f\"Query:\\n{dataset['test']['question'][0]}\")\n",
    "        print(f\"Original Answer:\\n{dataset['test']['answer'][0]}\")\n",
    "        print(f\"Generated Answer:\\n{test_prompt_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba2fa8dc-9aaf-4c6b-a44f-500f40909f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubeflow.training import TrainingClient\n",
    "from kubernetes import client\n",
    "from kubernetes.client import (\n",
    "    V1EnvVar,\n",
    "    V1EnvVarSource,\n",
    "    V1SecretKeySelector\n",
    ")\n",
    "\n",
    "job_name = \"pytorch-fsdp\"\n",
    "\n",
    "# aws_connection_name value should be the same as connection name in Data science project where the Workbench is running\n",
    "aws_connection_name = \"workbench-aws\"\n",
    "\n",
    "# Provide URL and token with all needed rights\n",
    "# On OpenShift, you can retrieve the token by running `oc whoami -t`,\n",
    "# and the server with `oc cluster-info`.\n",
    "\n",
    "# token = \"\"\n",
    "# openshift_api_url = \"\"\n",
    "\n",
    "# api_key = {\"authorization\": \"Bearer \" + token}\n",
    "# config = client.Configuration(host=openshift_api_url, api_key=api_key)\n",
    "# config.verify_ssl = False\n",
    "# tc = TrainingClient(client_configuration=config)\n",
    "\n",
    "\n",
    "# Alternatively add edit role for user running this Notebook using oc CLI:\n",
    "# oc adm policy add-role-to-user edit system:serviceaccount:<namespace>:<workbench name> -n <namespace>\n",
    "tc = TrainingClient()\n",
    "\n",
    "tc.create_job(\n",
    "    job_kind=\"PyTorchJob\",\n",
    "    name=job_name,\n",
    "    train_func=train_func,\n",
    "    num_workers=2,\n",
    "    num_procs_per_worker=\"auto\",\n",
    "    resources_per_worker={\"gpu\": 2},\n",
    "    base_image=\"quay.io/modh/training:py311-cuda121-torch241\",\n",
    "    env_vars=[\n",
    "        V1EnvVar(name=\"HF_TOKEN\", value_from=V1EnvVarSource(secret_key_ref=V1SecretKeySelector(key=\"HF_TOKEN\", name=\"hf-token\"))),\n",
    "        V1EnvVar(name=\"NCCL_DEBUG\", value=\"INFO\"),\n",
    "        V1EnvVar(name=\"AWS_ACCESS_KEY_ID\", value_from=V1EnvVarSource(secret_key_ref=V1SecretKeySelector(key=\"AWS_ACCESS_KEY_ID\", name=aws_connection_name))),\n",
    "        V1EnvVar(name=\"AWS_S3_BUCKET\", value_from=V1EnvVarSource(secret_key_ref=V1SecretKeySelector(key=\"AWS_S3_BUCKET\", name=aws_connection_name))),\n",
    "        V1EnvVar(name=\"AWS_S3_ENDPOINT\", value_from=V1EnvVarSource(secret_key_ref=V1SecretKeySelector(key=\"AWS_S3_ENDPOINT\", name=aws_connection_name))),\n",
    "        V1EnvVar(name=\"AWS_SECRET_ACCESS_KEY\", value_from=V1EnvVarSource(secret_key_ref=V1SecretKeySelector(key=\"AWS_SECRET_ACCESS_KEY\", name=aws_connection_name))),\n",
    "    ],\n",
    "    packages_to_install=[\n",
    "        \"s3fs\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d731d9d4-d8d4-47a9-b204-497c97b86ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pod pytorch-fsdp-master-0]: ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "[Pod pytorch-fsdp-master-0]: datasets 3.3.2 requires fsspec[http]<=2024.12.0,>=2023.1.0, but you have fsspec 2025.2.0 which is incompatible.\n",
      "[Pod pytorch-fsdp-master-0]: W0225 11:56:28.146000 140310728419136 torch/distributed/run.py:779] \n",
      "[Pod pytorch-fsdp-master-0]: W0225 11:56:28.146000 140310728419136 torch/distributed/run.py:779] *****************************************\n",
      "[Pod pytorch-fsdp-master-0]: W0225 11:56:28.146000 140310728419136 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "[Pod pytorch-fsdp-master-0]: W0225 11:56:28.146000 140310728419136 torch/distributed/run.py:779] *****************************************\n",
      "Generating train split: 100%|██████████| 7473/7473 [00:00<00:00, 506529.31 examples/s]\n",
      "Generating test split: 100%|██████████| 1319/1319 [00:00<00:00, 378664.41 examples/s]\n",
      "Map: 100%|██████████| 7473/7473 [00:00<00:00, 10670.04 examples/s]\n",
      "Map: 100%|██████████| 7473/7473 [00:00<00:00, 10602.95 examples/s]\n",
      "Map: 100%|██████████| 1319/1319 [00:00<00:00, 10726.60 examples/s]\n",
      "Map: 100%|██████████| 1319/1319 [00:00<00:00, 10666.34 examples/s]\n",
      "[Pod pytorch-fsdp-master-0]: /tmp/tmp.pFhPmEUsth/ephemeral_script.py:71: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "[Pod pytorch-fsdp-master-0]:   trainer = SFTTrainer(\n",
      "Converting train dataset to ChatML: 100%|██████████| 7473/7473 [00:00<00:00, 42074.00 examples/s]\n",
      "[Pod pytorch-fsdp-master-0]: /tmp/tmp.pFhPmEUsth/ephemeral_script.py:71: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "[Pod pytorch-fsdp-master-0]:   trainer = SFTTrainer(\n",
      "Applying chat template to train dataset: 100%|██████████| 7473/7473 [00:00<00:00, 40169.31 examples/s]\n",
      "Tokenizing train dataset: 100%|██████████| 7473/7473 [00:02<00:00, 2609.05 examples/s]\n",
      "Tokenizing train dataset: 100%|██████████| 7473/7473 [00:01<00:00, 5105.47 examples/s]\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:77:77 [0] NCCL INFO Bootstrap : Using eth0:10.131.0.172<0>\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:77:77 [0] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:77:77 [0] NCCL INFO cudaDriverVersion 12040\n",
      "[Pod pytorch-fsdp-master-0]: NCCL version 2.20.5+cuda12.4\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:78:78 [1] NCCL INFO cudaDriverVersion 12040\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:78:78 [1] NCCL INFO Bootstrap : Using eth0:10.131.0.172<0>\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:78:78 [1] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:77:190 [0] NCCL INFO Failed to open libibverbs.so[.1]\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:77:190 [0] NCCL INFO NET/Socket : Using [0]eth0:10.131.0.172<0>\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:77:190 [0] NCCL INFO Using non-device net plugin version 0\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:77:190 [0] NCCL INFO Using network Socket\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:78:191 [1] NCCL INFO Failed to open libibverbs.so[.1]\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:78:191 [1] NCCL INFO NET/Socket : Using [0]eth0:10.131.0.172<0>\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:78:191 [1] NCCL INFO Using non-device net plugin version 0\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:78:191 [1] NCCL INFO Using network Socket\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:78:191 [1] NCCL INFO comm 0x55a1bfef7540 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId 8020 commId 0x2f4dc4ffbd206ce - Init START\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:77:190 [0] NCCL INFO comm 0x55d1ce60c860 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 8010 commId 0x2f4dc4ffbd206ce - Init START\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:78:191 [1] NCCL INFO Setting affinity for GPU 1 to ff,ffffffff\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:77:190 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:78:191 [1] NCCL INFO comm 0x55a1bfef7540 rank 1 nRanks 4 nNodes 2 localRanks 2 localRank 1 MNNVL 0\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:77:190 [0] NCCL INFO comm 0x55d1ce60c860 rank 0 nRanks 4 nNodes 2 localRanks 2 localRank 0 MNNVL 0\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:78:191 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:78:191 [1] NCCL INFO P2P Chunksize set to 131072\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:77:190 [0] NCCL INFO Channel 00/02 :    0   1   2   3\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:77:190 [0] NCCL INFO Channel 01/02 :    0   1   2   3\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:77:190 [0] NCCL INFO Trees [0] 1/2/-1->0->-1 [1] 1/-1/-1->0->2\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:77:190 [0] NCCL INFO P2P Chunksize set to 131072\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:77:190 [0] NCCL INFO Channel 00/0 : 3[1] -> 0[0] [receive] via NET/Socket/0\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:77:190 [0] NCCL INFO Channel 01/0 : 3[1] -> 0[0] [receive] via NET/Socket/0\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:77:190 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/CUMEM/read\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:77:190 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/CUMEM/read\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:78:191 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[0] [send] via NET/Socket/0\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:78:191 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[0] [send] via NET/Socket/0\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:78:191 [1] NCCL INFO Connected all rings\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:78:191 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:77:190 [0] NCCL INFO Connected all rings\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:78:191 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:77:190 [0] NCCL INFO Channel 00/0 : 2[0] -> 0[0] [receive] via NET/Socket/0\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:77:190 [0] NCCL INFO Channel 01/0 : 2[0] -> 0[0] [receive] via NET/Socket/0\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:77:190 [0] NCCL INFO Channel 00/0 : 0[0] -> 2[0] [send] via NET/Socket/0\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:77:190 [0] NCCL INFO Channel 01/0 : 0[0] -> 2[0] [send] via NET/Socket/0\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:77:190 [0] NCCL INFO Connected all trees\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:78:191 [1] NCCL INFO Connected all trees\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:78:191 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:78:191 [1] NCCL INFO 2 coll channels, 0 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:77:190 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:77:190 [0] NCCL INFO 2 coll channels, 0 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:77:190 [0] NCCL INFO comm 0x55d1ce60c860 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 8010 commId 0x2f4dc4ffbd206ce - Init COMPLETE\n",
      "[Pod pytorch-fsdp-master-0]: pytorch-fsdp-master-0:78:191 [1] NCCL INFO comm 0x55a1bfef7540 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId 8020 commId 0x2f4dc4ffbd206ce - Init COMPLETE\n",
      "Converting eval dataset to ChatML: 100%|██████████| 1319/1319 [00:00<00:00, 36255.42 examples/s]\n",
      "Applying chat template to eval dataset: 100%|██████████| 1319/1319 [00:00<00:00, 36839.43 examples/s]\n",
      "Tokenizing eval dataset: 100%|██████████| 1319/1319 [00:00<00:00, 2473.33 examples/s]]\n",
      "Tokenizing eval dataset: 100%|██████████| 1319/1319 [00:00<00:00, 4968.90 examples/s]]\n",
      "Tokenizing train dataset: 100%|██████████| 7473/7473 [00:03<00:00, 2225.41 examples/s]\n",
      "Tokenizing train dataset: 100%|██████████| 7473/7473 [00:01<00:00, 4732.83 examples/s]\n",
      "Tokenizing eval dataset:  56%|█████▌    | 733/1319 [00:00<00:00, 2442.68 examples/s][2025-02-25 12:01:11,419] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Tokenizing eval dataset:  75%|███████▍  | 989/1319 [00:00<00:00, 2485.71 examples/s]df: /opt/app-root/src/.triton/autotune: No such file or directory\n",
      "Tokenizing eval dataset: 100%|██████████| 1319/1319 [00:00<00:00, 2217.58 examples/s]\n",
      "Tokenizing eval dataset: 100%|██████████| 1319/1319 [00:00<00:00, 4596.16 examples/s]\n",
      "[Pod pytorch-fsdp-master-0]: Device set to use cuda:0\n",
      "[Pod pytorch-fsdp-master-0]: [2025-02-25 12:01:12,651] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[Pod pytorch-fsdp-master-0]: Query:\n",
      "[Pod pytorch-fsdp-master-0]: Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\n",
      "[Pod pytorch-fsdp-master-0]: Original Answer:\n",
      "[Pod pytorch-fsdp-master-0]: Janet sells 16 - 3 - 4 = <<16-3-4=9>>9 duck eggs a day.\n",
      "[Pod pytorch-fsdp-master-0]: She makes 9 * 2 = $<<9*2=18>>18 every day at the farmer’s market.\n",
      "[Pod pytorch-fsdp-master-0]: #### 18\n",
      "[Pod pytorch-fsdp-master-0]: Generated Answer:\n",
      "[Pod pytorch-fsdp-master-0]: To find out how much Janet makes at the farmers' market, we need to calculate the number of eggs she sells and then multiply that by the price per egg.\n",
      "[Pod pytorch-fsdp-master-0]: Janet lays 16 eggs per day. She eats 3 for breakfast, so she has 16 - 3 = 13 eggs left.\n",
      "[Pod pytorch-fsdp-master-0]: She bakes muffins for 4 eggs per day. So, she has 13 - 4 = 9 eggs left.\n",
      "[Pod pytorch-fsdp-master-0]: She sells the remaining 9 eggs at the farmers' market for $2 per egg. So, she makes 9 x $2 = $18 per day.\n",
      "[Pod pytorch-fsdp-master-0]: Therefore, Janet makes $18 every day at the farmers' market.\n"
     ]
    }
   ],
   "source": [
    "logs, _ = tc.get_job_logs(job_name, follow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2cb4d19f-b172-45b7-bf30-418bd2922982",
   "metadata": {},
   "outputs": [],
   "source": [
    "tc.delete_job(name=job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b31d3d9-fa48-4a6b-bd3d-66f133ab821b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import s3fs\n",
    "import os\n",
    "\n",
    "# Download trained model into local filesystem\n",
    "s3 = s3fs.S3FileSystem()\n",
    "s3_path = os.environ[\"AWS_S3_BUCKET\"] + '/saved-model'\n",
    "s3.get(s3_path, \"./saved-model\", recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d00747cb-7409-4e16-b21f-713d07c7c864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95bff9895c114ecb838fbb458b2cd6a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\n",
      "Original Answer:\n",
      "Janet sells 16 - 3 - 4 = <<16-3-4=9>>9 duck eggs a day.\n",
      "She makes 9 * 2 = $<<9*2=18>>18 every day at the farmer’s market.\n",
      "#### 18\n",
      "Generated Answer:\n",
      "She eats 3 eggs for breakfast every morning, so she eats 3 eggs/day * 16 eggs/day = <<3*16=48>>48 eggs/day\n",
      "She bakes muffins for her friends every day with 4 muffins, so she bakes 4 muffins/day * 16 muffins/day = <<4*16=64>>64 muffins/day\n",
      "She sells the remainder at the farmers' market daily for $2 per fresh duck egg, so she sells 64 muffins/day - 48 eggs/day = <<64-48=16>>16 muffins/day\n",
      "She sells 16 muffins/day at the farmers' market for $2 per muffin, so she makes 16 muffins/day * $2/muffin = $<<16*2=32>>32/day\n",
      "#### 32\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    pipeline,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "\n",
    "# infer sample test result\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"./saved-model\",\n",
    ").to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"./saved-model\",\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "dataset = load_dataset(\"openai/gsm8k\", \"main\")\n",
    "\n",
    "def infer_answer(model, question):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": question},\n",
    "    ]\n",
    "    pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0, temperature = 0.01)\n",
    "    prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    outputs = pipe(messages, max_new_tokens=256)\n",
    "    return outputs[0]['generated_text'][-1]['content']\n",
    "\n",
    "test_prompt_answer = infer_answer(model, dataset['test']['question'][0])\n",
    "print(f\"Query:\\n{dataset['test']['question'][0]}\")\n",
    "print(f\"Original Answer:\\n{dataset['test']['answer'][0]}\")\n",
    "print(f\"Generated Answer:\\n{test_prompt_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "644fd89b-1d58-498d-b83d-80613c7c1474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unload the model from GPU memory\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "del model\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79039eb0-ed85-453c-8c61-e8534f392d29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
