{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5cde943-a5da-4d7a-a333-c32341266447",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b89c3c-a969-46fb-8314-b57f47061f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qqU datasets s3fs diffusers peft transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6697b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Install the YAML magic\n",
    "%pip install -qqU yamlmagic\n",
    "%load_ext yamlmagic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b2ca86-d333-4c09-b420-11b3517a9bc0",
   "metadata": {},
   "source": [
    "## Training configuration\n",
    "Edit the following training parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc84c36-55ce-4ce3-be20-8c387bf5a4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%yaml parameters\n",
    "\n",
    "# Dataset specification and description\n",
    "name_of_your_concept: 'ccorgi'\n",
    "type_of_thing: 'dog'\n",
    "dataset_id: 'diffusers/dog-example'\n",
    "dataset_train_split: 'train'\n",
    "# Model definitions\n",
    "model_id: 'CompVis/stable-diffusion-v1-4'\n",
    "model_text_encoder_subfolder: 'text_encoder'\n",
    "model_vae_subfolder: 'vae'\n",
    "model_unet_subfolder: 'unet'\n",
    "model_tokenizer_subfolder: 'tokenizer'\n",
    "feature_extractor: 'openai/clip-vit-base-patch32'\n",
    "safety_checker: 'CompVis/stable-diffusion-safety-checker'\n",
    "# Storage configuration\n",
    "local_trained_model_folder: './dreambooth'    # Notebook location where will the trained model be stored once downloaded from s3\n",
    "# Model training configuration\n",
    "learning_rate: 2.0e-06\n",
    "max_train_steps: 300\n",
    "train_batch_size: 1\n",
    "gradient_accumulation_steps: 1                # Increase this if you want to lower memory usage\n",
    "max_grad_norm: 1.0\n",
    "gradient_checkpointing: True                  # Set this to True to lower the memory usage\n",
    "use_8bit_adam: True                           # Use 8bit optimizer from bitsandbytes\n",
    "seed: 3434554\n",
    "output_dir: \"dreambooth\"                      # Where to save the trained pipeline in PyTorchJob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb03bffa-7369-4d5f-aae1-ec7455f3a669",
   "metadata": {},
   "source": [
    "Load and show dataset images used for training dreambooth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48982f8f-5fa9-49b1-b0f5-bc5fe75eef88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def image_grid(imgs, rows, cols):\n",
    "    assert len(imgs) == rows * cols\n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new(\"RGB\", size=(cols * w, rows * h))\n",
    "    grid_w, grid_h = grid.size\n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i % cols * w, i // cols * h))\n",
    "    return grid\n",
    "\n",
    "\n",
    "num_samples = 4\n",
    "dataset = load_dataset(parameters['dataset_id'], split=parameters['dataset_train_split'])\n",
    "image_grid(dataset[\"image\"][:num_samples], rows=1, cols=num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd30cf6d-e7df-4e0e-b1b2-67adad612429",
   "metadata": {},
   "source": [
    "# Distributed training\n",
    "Training function to be ran on all distributed training Pods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef56b48-ae1e-492d-a276-4b06d1cc050a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_function(parameters):\n",
    "    import math\n",
    "    import torch\n",
    "    import os\n",
    "\n",
    "    import torch.nn.functional as F\n",
    "    from accelerate import Accelerator\n",
    "    from accelerate.utils import set_seed\n",
    "    from diffusers import DDPMScheduler, PNDMScheduler, StableDiffusionPipeline, AutoencoderKL, UNet2DConditionModel\n",
    "    from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n",
    "    from torch.utils.data import DataLoader, Dataset\n",
    "    from tqdm.auto import tqdm\n",
    "    from transformers import CLIPFeatureExtractor, CLIPTextModel, CLIPTokenizer\n",
    "    from torchvision import transforms\n",
    "    from datasets import load_dataset\n",
    "    import s3fs\n",
    "\n",
    "    name_of_your_concept = parameters['name_of_your_concept']\n",
    "    type_of_thing = parameters['type_of_thing']\n",
    "    instance_prompt = f\"a photo of {name_of_your_concept} {type_of_thing}\"\n",
    "\n",
    "    # PyTorch Dataset object that implements the __len__ and __getitem__ methods\n",
    "    class DreamBoothDataset(Dataset):\n",
    "        def __init__(self, dataset, instance_prompt, tokenizer, size=512):\n",
    "            self.dataset = dataset\n",
    "            self.instance_prompt = instance_prompt\n",
    "            self.tokenizer = tokenizer\n",
    "            self.size = size\n",
    "            self.transforms = transforms.Compose(\n",
    "                [\n",
    "                    transforms.Resize(size),\n",
    "                    transforms.CenterCrop(size),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize([0.5], [0.5]),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.dataset)\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "            example = {}\n",
    "            image = self.dataset[index][\"image\"]\n",
    "            example[\"instance_images\"] = self.transforms(image)\n",
    "            example[\"instance_prompt_ids\"] = self.tokenizer(\n",
    "                self.instance_prompt,\n",
    "                padding=\"do_not_pad\",\n",
    "                truncation=True,\n",
    "                max_length=self.tokenizer.model_max_length,\n",
    "            ).input_ids\n",
    "            return example\n",
    "\n",
    "    # The Stable Diffusion checkpoint we'll fine-tune\n",
    "    model_id = parameters['model_id']\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(\n",
    "        model_id,\n",
    "        subfolder=parameters['model_tokenizer_subfolder'],\n",
    "    )\n",
    "\n",
    "    dataset = load_dataset(parameters['dataset_id'], split=parameters['dataset_train_split'])\n",
    "    train_dataset = DreamBoothDataset(dataset, instance_prompt, tokenizer)\n",
    "\n",
    "    # Data collator function to couple prompt with image\n",
    "    def collate_fn(examples):\n",
    "        input_ids = [example[\"instance_prompt_ids\"] for example in examples]\n",
    "        pixel_values = [example[\"instance_images\"] for example in examples]\n",
    "        pixel_values = torch.stack(pixel_values)\n",
    "        pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n",
    "\n",
    "        input_ids = tokenizer.pad({\"input_ids\": input_ids}, padding=True, return_tensors=\"pt\").input_ids\n",
    "\n",
    "        batch = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"pixel_values\": pixel_values,\n",
    "        }\n",
    "        return batch\n",
    "\n",
    "    text_encoder = CLIPTextModel.from_pretrained(model_id, subfolder=parameters['model_text_encoder_subfolder'])\n",
    "    vae = AutoencoderKL.from_pretrained(model_id, subfolder=parameters['model_vae_subfolder'])\n",
    "    unet = UNet2DConditionModel.from_pretrained(model_id, subfolder=parameters['model_unet_subfolder'])\n",
    "    feature_extractor = CLIPFeatureExtractor.from_pretrained(parameters['feature_extractor'])\n",
    "\n",
    "    accelerator = Accelerator(\n",
    "        gradient_accumulation_steps=parameters['gradient_accumulation_steps'],\n",
    "    )\n",
    "\n",
    "    set_seed(parameters['seed'])\n",
    "\n",
    "    if parameters['gradient_checkpointing']:\n",
    "        unet.enable_gradient_checkpointing()\n",
    "\n",
    "    # Use 8-bit Adam for lower memory usage or to fine-tune the model in 16GB GPUs\n",
    "    if parameters['use_8bit_adam']:\n",
    "        import bitsandbytes as bnb\n",
    "\n",
    "        optimizer_class = bnb.optim.AdamW8bit\n",
    "    else:\n",
    "        optimizer_class = torch.optim.AdamW\n",
    "\n",
    "    optimizer = optimizer_class(\n",
    "        unet.parameters(),  # Only optimize unet\n",
    "        lr=parameters['learning_rate'],\n",
    "    )\n",
    "\n",
    "    noise_scheduler = DDPMScheduler(\n",
    "        beta_start=0.00085,\n",
    "        beta_end=0.012,\n",
    "        beta_schedule=\"scaled_linear\",\n",
    "        num_train_timesteps=1000,\n",
    "    )\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=parameters['train_batch_size'],\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "\n",
    "    unet, optimizer, train_dataloader = accelerator.prepare(unet, optimizer, train_dataloader)\n",
    "\n",
    "    # Move text_encode and vae to gpu\n",
    "    text_encoder.to(accelerator.device)\n",
    "    vae.to(accelerator.device)\n",
    "\n",
    "    # We need to recalculate our total training steps as the size of the training dataloader may have changed\n",
    "    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / parameters['gradient_accumulation_steps'])\n",
    "    num_train_epochs = math.ceil(parameters['max_train_steps'] / num_update_steps_per_epoch)\n",
    "\n",
    "    # Train!\n",
    "    # Only show the progress bar once on each machine\n",
    "    progress_bar = tqdm(range(parameters['max_train_steps']), disable=not accelerator.is_local_main_process)\n",
    "    progress_bar.set_description(\"Steps\")\n",
    "    global_step = 0\n",
    "\n",
    "    for epoch in range(num_train_epochs):\n",
    "        unet.train()\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            with accelerator.accumulate(unet):\n",
    "                # Convert images to latent space\n",
    "                with torch.no_grad():\n",
    "                    latents = vae.encode(batch[\"pixel_values\"]).latent_dist.sample()\n",
    "                    latents = latents * 0.18215\n",
    "\n",
    "                # Sample noise that we'll add to the latents\n",
    "                noise = torch.randn(latents.shape).to(latents.device)\n",
    "                bsz = latents.shape[0]\n",
    "                # Sample a random timestep for each image\n",
    "                timesteps = torch.randint(\n",
    "                    0,\n",
    "                    noise_scheduler.config.num_train_timesteps,\n",
    "                    (bsz,),\n",
    "                    device=latents.device,\n",
    "                ).long()\n",
    "\n",
    "                # Add noise to the latents according to the noise magnitude at each timestep\n",
    "                # (this is the forward diffusion process)\n",
    "                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "                # Get the text embedding for conditioning\n",
    "                with torch.no_grad():\n",
    "                    encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\n",
    "\n",
    "                # Predict the noise residual\n",
    "                noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
    "                loss = F.mse_loss(noise_pred, noise, reduction=\"none\").mean([1, 2, 3]).mean()\n",
    "\n",
    "                accelerator.backward(loss)\n",
    "                if accelerator.sync_gradients:\n",
    "                    accelerator.clip_grad_norm_(unet.parameters(), parameters['max_grad_norm'])\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            # Checks if the accelerator has performed an optimization step behind the scenes\n",
    "            if accelerator.sync_gradients:\n",
    "                progress_bar.update(1)\n",
    "                global_step += 1\n",
    "\n",
    "            logs = {\"loss\": loss.detach().item()}\n",
    "            progress_bar.set_postfix(**logs)\n",
    "\n",
    "            if global_step >= parameters['max_train_steps']:\n",
    "                break\n",
    "\n",
    "        accelerator.wait_for_everyone()\n",
    "\n",
    "    # Create the pipeline using the trained modules and save it\n",
    "    if accelerator.is_main_process:\n",
    "        output_dir = parameters['output_dir']\n",
    "        print(f\"Loading pipeline and saving to {output_dir}...\")\n",
    "        scheduler = PNDMScheduler(\n",
    "            beta_start=0.00085,\n",
    "            beta_end=0.012,\n",
    "            beta_schedule=\"scaled_linear\",\n",
    "            skip_prk_steps=True,\n",
    "            steps_offset=1,\n",
    "        )\n",
    "        pipeline = StableDiffusionPipeline(\n",
    "            text_encoder=text_encoder,\n",
    "            vae=vae,\n",
    "            unet=accelerator.unwrap_model(unet),\n",
    "            tokenizer=tokenizer,\n",
    "            scheduler=scheduler,\n",
    "            safety_checker=StableDiffusionSafetyChecker.from_pretrained(parameters['safety_checker']),\n",
    "            feature_extractor=feature_extractor,\n",
    "        )\n",
    "        pipeline.save_pretrained(output_dir)\n",
    "\n",
    "        # Store trained model on AWS S3 bucket\n",
    "        s3 = s3fs.S3FileSystem()\n",
    "        s3_path = os.environ[\"AWS_S3_BUCKET\"]\n",
    "        s3.put(output_dir, s3_path, recursive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4625006d-13e2-41ff-9f41-8a04a3b44f37",
   "metadata": {},
   "source": [
    "Invoke training function on PyTorchJob Pods, provide packages to be installed and AWS S3 configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9c5c55-f2ab-4ceb-9dd2-db3aa34253ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubeflow.training import TrainingClient\n",
    "from kubernetes import client\n",
    "from kubernetes.client import (\n",
    "    V1EnvVar,\n",
    "    V1EnvVarSource,\n",
    "    V1SecretKeySelector\n",
    ")\n",
    "\n",
    "job_name = \"dreambooth\"\n",
    "\n",
    "# aws_connection_name value should be the same as connection name in Data science project where the Workbench is running\n",
    "aws_connection_name = \"workbench-aws\"\n",
    "\n",
    "# Provide URL and token with all required permissions\n",
    "# On OpenShift, you can retrieve the token by running `oc whoami -t`,\n",
    "# and the server with `oc cluster-info`.\n",
    "\n",
    "token = \"\"\n",
    "openshift_api_url = \"\"\n",
    "\n",
    "api_key = {\"authorization\": \"Bearer \" + token}\n",
    "config = client.Configuration(host=openshift_api_url, api_key=api_key)\n",
    "# Un-comment if your cluster API server uses a self-signed certificate or an un-trusted CA\n",
    "# config.verify_ssl = False\n",
    "tc = TrainingClient(client_configuration=config)\n",
    "\n",
    "\n",
    "# Alternatively add edit role for user running this Notebook using oc CLI:\n",
    "# oc adm policy add-role-to-user edit system:serviceaccount:<namespace>:<workbench name> -n <namespace>\n",
    "# tc = TrainingClient()\n",
    "\n",
    "tc.create_job(\n",
    "    job_kind=\"PyTorchJob\",\n",
    "    name=job_name,\n",
    "    train_func=training_function,\n",
    "    num_workers=1,\n",
    "    num_procs_per_worker=\"auto\",\n",
    "    resources_per_worker={\"gpu\": 2},\n",
    "    base_image=\"quay.io/modh/training:py311-cuda121-torch241\",\n",
    "    parameters=parameters,\n",
    "    # labels={\"kueue.x-k8s.io/queue-name\": \"<LOCAL_QUEUE_NAME>\"}, # Optional: Add local queue name and uncomment these lines if using Kueue for resource management\n",
    "    env_vars=[\n",
    "        V1EnvVar(name=\"AWS_ACCESS_KEY_ID\", value_from=V1EnvVarSource(secret_key_ref=V1SecretKeySelector(key=\"AWS_ACCESS_KEY_ID\", name=aws_connection_name))),\n",
    "        V1EnvVar(name=\"AWS_S3_BUCKET\", value_from=V1EnvVarSource(secret_key_ref=V1SecretKeySelector(key=\"AWS_S3_BUCKET\", name=aws_connection_name))),\n",
    "        V1EnvVar(name=\"AWS_S3_ENDPOINT\", value_from=V1EnvVarSource(secret_key_ref=V1SecretKeySelector(key=\"AWS_S3_ENDPOINT\", name=aws_connection_name))),\n",
    "        V1EnvVar(name=\"AWS_SECRET_ACCESS_KEY\", value_from=V1EnvVarSource(secret_key_ref=V1SecretKeySelector(key=\"AWS_SECRET_ACCESS_KEY\", name=aws_connection_name))),\n",
    "    ],\n",
    "    packages_to_install=[\n",
    "        \"diffusers\",\n",
    "        \"torchvision\",\n",
    "        \"s3fs\",\n",
    "        \"bitsandbytes\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b915aa-7e9e-4d21-aaee-fe31f1dc0689",
   "metadata": {},
   "source": [
    "Once training Pods start you can watch training progress using following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08126214-47c7-4f39-a0de-af704fb76859",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs, _ = tc.get_job_logs(job_name, follow=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5057aa6-5fa2-49bb-8b98-566a50ef7a32",
   "metadata": {},
   "source": [
    "# Local inference in Workbench\n",
    "Once training finishes download trained model from S3 to Workbench folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b136e0b-6afb-43f0-809e-2b4c690fef81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "import os\n",
    "\n",
    "# Download trained model into local filesystem\n",
    "s3 = s3fs.S3FileSystem()\n",
    "s3_path = os.environ[\"AWS_S3_BUCKET\"] + \"/\" + parameters['output_dir']\n",
    "_ = s3.get(s3_path, parameters['local_trained_model_folder'], recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1d2641-fd29-4045-b828-1849282e5f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "import torch\n",
    "\n",
    "# Setup stable diffusion pipeline with downloaded model\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    parameters['local_trained_model_folder'],\n",
    "    torch_dtype=torch.float16,\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a29b675-8986-47c7-93b2-9ec7941eee9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a prompt for image generation using trained tokens\n",
    "name_of_your_concept = parameters['name_of_your_concept']\n",
    "type_of_thing = parameters['type_of_thing']\n",
    "prompt = f\"a photo of {name_of_your_concept} {type_of_thing} in the Acropolis\"\n",
    "\n",
    "# Tune the guidance to control how closely the generations follow the prompt\n",
    "# Values between 7-11 usually work best\n",
    "guidance_scale = 7\n",
    "\n",
    "num_cols = 2\n",
    "all_images = []\n",
    "for _ in range(num_cols):\n",
    "    images = pipe(prompt, guidance_scale=guidance_scale).images\n",
    "    all_images.extend(images)\n",
    "\n",
    "image_grid(all_images, 1, num_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcee586-1788-4e1f-a490-cad57a352220",
   "metadata": {},
   "source": [
    "# Cleaning Up\n",
    "Delete PyTorchJob to clean up OpenShift environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cdad40-a96e-4e48-a847-0c04782fd26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tc.delete_job(name=job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba11e79a-df63-46ce-b6e9-0a890bb6e086",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
