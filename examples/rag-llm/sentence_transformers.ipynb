{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b29cb9f2-e3c0-44cc-8327-7757c5add287",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Install all needed dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff9c793-7ca5-4f3b-8353-b55d3acb3b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet --upgrade transformers sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ef69a7-c616-4b06-b1ad-d3cb98abe7df",
   "metadata": {},
   "source": [
    "#  Simple RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0abf06-145f-4644-b25d-823c6ffc58af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "embedder_model = \"ibm-granite/granite-embedding-30m-english\"\n",
    "generator_model = \"ibm-granite/granite-3.2-2b-instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1ee0e6-105a-438b-9265-fedf919ed690",
   "metadata": {},
   "source": [
    "Prepare chunk database. In this case database is represented by a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb54a2f0-aef6-4308-a8b4-07e9c7cca23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "link = \"https://huggingface.co/ngxson/demo_simple_rag_py/raw/main/cat-facts.txt\"\n",
    "dataset = []\n",
    "\n",
    "# Retrieve knowledge from provided link, use every line as a separate chunk.\n",
    "for line in urllib.request.urlopen(link):\n",
    "  dataset.append(line.decode('utf-8'))\n",
    "\n",
    "print(f'Loaded {len(dataset)} entries')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5bb3cd-9785-43fa-b1c4-e16e78b69073",
   "metadata": {},
   "source": [
    "**Specify user query here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc40487-bf1c-49ed-9106-0dc46e38820c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_query = \"tell me about cat mummies\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677c95fe-1d36-4dfe-bf0d-1283857e5ee7",
   "metadata": {},
   "source": [
    "Encode user query and chunks into embeddings (vector representations). Use semantic_search to find 5 chunks which are most similar to the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c61b305-5f62-4f99-8577-0708ba5e5f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import semantic_search\n",
    "\n",
    "embedder = SentenceTransformer(embedder_model)\n",
    "\n",
    "query_embedding = embedder.encode(input_query, convert_to_tensor=True)\n",
    "dataset_embedding = embedder.encode(dataset, convert_to_tensor=True)\n",
    "\n",
    "# Get 5 chunk embeddings which are most similar to the query embedding.\n",
    "# semantic_search returns list of dictionaries with embedding index (corpus_id) and similarity score.\n",
    "retrieved_knowledge = semantic_search(query_embeddings=query_embedding, corpus_embeddings=dataset_embedding, top_k=5)\n",
    "\n",
    "print('Retrieved knowledge:')\n",
    "for corpus in retrieved_knowledge[0]:\n",
    "  print(f' - (similarity: {corpus[\"score\"]:.2f}) {dataset[corpus[\"corpus_id\"]]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10554313-eca4-4a81-a1b7-3363632095c1",
   "metadata": {},
   "source": [
    "Prepare inference pipeline using transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cb29a0-50ff-4111-a1c3-5fb7e2d18bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(generator_model)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "pipeline = transformers.pipeline(\"text-generation\", model=generator_model, tokenizer=tokenizer, device=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52d2349-02aa-47d6-a5d0-783e8361feee",
   "metadata": {},
   "source": [
    "Run inference using provided user prompt and system prompt containing knowledge chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780ba23e-2dda-4cb8-814c-66886ddd3ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct system prompt for inference providing retrieved chunks as context.\n",
    "instruction_prompt = f'''You are a helpful chatbot.\n",
    "Use only the following pieces of context to answer the question. Don't make up any new information:\n",
    "{''.join([f' - {dataset[corpus[\"corpus_id\"]]}' for corpus in retrieved_knowledge[0]])}\n",
    "'''\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": instruction_prompt,\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": input_query,\n",
    "    }\n",
    "]\n",
    "\n",
    "outputs = pipeline(messages, max_new_tokens=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f58e9e1-1096-447b-90a3-c3819e4982a9",
   "metadata": {},
   "source": [
    "Print result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a948777-5abd-4f57-a8ac-9edaadf41b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "output = \"\"\n",
    "for turn in outputs:\n",
    "    for item in turn[\"generated_text\"]:\n",
    "        output += f\"# {item['role']}\\n\\n{item['content']}\\n\\n\"\n",
    "\n",
    "display(Markdown(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894204cb-0e7e-4324-bec6-5f4eeeb295bf",
   "metadata": {},
   "source": [
    "# Cleaning Up\n",
    "\n",
    "Delete pipeline and associated model from GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afef942-a76e-4af4-b8da-32967ad8eb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "del pipeline\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ae1899-a50e-4cd9-92ff-0f093c092979",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
