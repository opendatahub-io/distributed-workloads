{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install datasets --quiet\n",
    "# pip Install kubeflow SDK from main branch for testing\n",
    "%pip install git+https://github.com/opendatahub-io/kubeflow-sdk.git@main --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubernetes import client as k8s, config as k8s_config\n",
    "# Edit to match your specific settings\n",
    "api_server = os.getenv(\"OPENSHIFT_API_URL\")\n",
    "token = os.getenv(\"NOTEBOOK_USER_TOKEN\")\n",
    "if not api_server or not token:\n",
    "    raise RuntimeError(\"OPENSHIFT_API_URL and NOTEBOOK_USER_TOKEN environment variables are required\")\n",
    "PVC_NAME = os.getenv(\"SHARED_PVC_NAME\", \"\")\n",
    "\n",
    "if not PVC_NAME:\n",
    "   raise RuntimeError(\"SHARED_PVC_NAME environment variable is required\")\n",
    "\n",
    "configuration = k8s.Configuration()\n",
    "configuration.host = api_server\n",
    "# Un-comment if your cluster API server uses a self-signed certificate or an un-trusted CA\n",
    "configuration.verify_ssl = False\n",
    "configuration.api_key = {\"authorization\": f\"Bearer {token}\"}\n",
    "api_client = k8s.ApiClient(configuration)\n",
    "\n",
    "PVC_MOUNT_PATH = \"/opt/app-root/src\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the format of the intial messages.\n",
    "def convert_to_messages(example):\n",
    "    \"\"\"\n",
    "    Convert a sql-create-context example to chat template format.\n",
    "    \n",
    "    The user provides the database schema and question.\n",
    "    The assistant responds with the SQL query.\n",
    "    \"\"\"\n",
    "    user_message = f\"\"\"Given the following database schema:\n",
    "\n",
    "{example['context']}\n",
    "\n",
    "Write a SQL query to answer this question: {example['question']}\"\"\"\n",
    "    \n",
    "    assistant_message = example['answer']\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": user_message},\n",
    "            {\"role\": \"assistant\", \"content\": assistant_message}\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import shutil\n",
    "import socket\n",
    "\n",
    "import boto3\n",
    "from botocore.config import Config as BotoConfig\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# --- Global networking safety net: cap all socket operations ---\n",
    "socket.setdefaulttimeout(10)  # seconds\n",
    "\n",
    "# Notebook's PVC mount path (per Notebook CR). Training pods will mount the same PVC at /opt/app-root/src\n",
    "PVC_NOTEBOOK_PATH = \"/opt/app-root/src/\"\n",
    "DATASET_ROOT_NOTEBOOK = PVC_NOTEBOOK_PATH\n",
    "TXT_SQL_DIR = os.path.join(DATASET_ROOT_NOTEBOOK, \"txt-sql-data\", \"train\")\n",
    "MODEL_DIR = os.path.join(DATASET_ROOT_NOTEBOOK, \"Qwen\", \"Qwen2.5-1.5B-Instruct\")\n",
    "os.makedirs(TXT_SQL_DIR, exist_ok=True)\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# Env config for S3/MinIO\n",
    "s3_endpoint = os.getenv(\"AWS_DEFAULT_ENDPOINT\", \"\")\n",
    "s3_access_key = os.getenv(\"AWS_ACCESS_KEY_ID\", \"\")\n",
    "s3_secret_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\", \"\")\n",
    "s3_bucket = os.getenv(\"AWS_STORAGE_BUCKET\", \"\")\n",
    "s3_prefix = os.getenv(\"AWS_STORAGE_BUCKET_LORA_DIR\", \"\")  \n",
    "\n",
    "data_download_successful = False\n",
    "\n",
    "def stream_download(s3, bucket, key, dst):\n",
    "    \"\"\"\n",
    "    Download an object from S3/MinIO using get_object and streaming reads.\n",
    "    Returns True on success, False on any error.\n",
    "    \"\"\"\n",
    "    print(f\"[notebook] STREAM download s3://{bucket}/{key} -> {dst}\")\n",
    "    t0 = time.time()\n",
    "\n",
    "    try:\n",
    "        resp = s3.get_object(Bucket=bucket, Key=key)\n",
    "    except ClientError as e:\n",
    "        err = e.response.get(\"Error\", {})\n",
    "        print(f\"[notebook] CLIENT ERROR (get_object) for {key}: {err}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"[notebook] OTHER ERROR (get_object) for {key}: {e}\")\n",
    "        return False\n",
    "\n",
    "    body = resp[\"Body\"]\n",
    "    try:\n",
    "        with open(dst, \"wb\") as f:\n",
    "            while True:\n",
    "                try:\n",
    "                    chunk = body.read(1024 * 1024)  # 1MB per chunk\n",
    "                except socket.timeout as e:\n",
    "                    print(f\"[notebook] socket.timeout while reading {key}: {e}\")\n",
    "                    return False\n",
    "                if not chunk:\n",
    "                    break\n",
    "                f.write(chunk)\n",
    "    except Exception as e:\n",
    "        print(f\"[notebook] ERROR writing to {dst} for {key}: {e}\")\n",
    "        return False\n",
    "\n",
    "    t1 = time.time()\n",
    "    print(f\"[notebook] DONE  stream {key} in {t1 - t0:.2f}s\")\n",
    "    return True\n",
    "\n",
    "\n",
    "if s3_endpoint and s3_bucket:\n",
    "    try:\n",
    "        # Normalize endpoint URL\n",
    "        endpoint_url = (\n",
    "            s3_endpoint\n",
    "            if s3_endpoint.startswith(\"http\")\n",
    "            else f\"https://{s3_endpoint}\"\n",
    "        )\n",
    "        prefix = (s3_prefix or \"\").strip(\"/\")\n",
    "\n",
    "        print(\n",
    "            f\"[notebook] S3 configured: \"\n",
    "            f\"endpoint={endpoint_url}, bucket={s3_bucket}, prefix={prefix or '<root>'}\"\n",
    "        )\n",
    "\n",
    "        # Boto config: single attempt, reasonable connect/read timeouts\n",
    "        boto_cfg = BotoConfig(\n",
    "            signature_version=\"s3v4\",\n",
    "            s3={\"addressing_style\": \"path\"},\n",
    "            retries={\"max_attempts\": 1, \"mode\": \"standard\"},\n",
    "            connect_timeout=5,\n",
    "            read_timeout=10,\n",
    "        )\n",
    "\n",
    "        # Create S3/MinIO client\n",
    "        s3 = boto3.client(\n",
    "            \"s3\",\n",
    "            endpoint_url=endpoint_url,\n",
    "            aws_access_key_id=s3_access_key,\n",
    "            aws_secret_access_key=s3_secret_key,\n",
    "            config=boto_cfg,\n",
    "            verify=False,\n",
    "        )\n",
    "\n",
    "        # List and download all objects under the prefix\n",
    "        paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "        pulled_any = False\n",
    "        file_count = 0\n",
    "        \n",
    "        print(f\"[notebook] Starting S3 download from prefix: {prefix}\")\n",
    "        for page in paginator.paginate(Bucket=s3_bucket, Prefix=prefix or \"\"):\n",
    "            contents = page.get(\"Contents\", [])\n",
    "            if not contents:\n",
    "                print(f\"[notebook] No contents found in this page\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"[notebook] Found {len(contents)} objects in this page\")\n",
    "\n",
    "            for obj in contents:\n",
    "                key = obj[\"Key\"]\n",
    "                file_count += 1\n",
    "\n",
    "                # Skip \"directory markers\"\n",
    "                if key.endswith(\"/\"):\n",
    "                    print(f\"[notebook] Skipping directory marker: {key}\")\n",
    "                    continue\n",
    "\n",
    "                # Determine relative path under prefix for local storage\n",
    "                rel = key[len(prefix):].lstrip(\"/\") if prefix else key\n",
    "                print(f\"[notebook] Processing key={key}, rel={rel}\")\n",
    "                \n",
    "                # Route to appropriate directory based on content type\n",
    "                if \"table-gpt\" in rel.lower() or rel.endswith(\".jsonl\"):\n",
    "                    dst = os.path.join(TXT_SQL_DIR, os.path.basename(rel))\n",
    "                    print(f\"[notebook] Routing to dataset dir: {dst}\")\n",
    "                elif \"qwen\" in rel.lower() or any(rel.endswith(ext) for ext in [\".bin\", \".json\", \".model\", \".safetensors\", \".txt\"]):\n",
    "                    # Preserve directory structure for model files\n",
    "                    dst = os.path.join(MODEL_DIR, rel.split(\"Qwen2.5-1.5B-Instruct/\")[-1] if \"Qwen2.5-1.5B-Instruct\" in rel else os.path.basename(rel))\n",
    "                    print(f\"[notebook] Routing to model dir: {dst}\")\n",
    "                else:\n",
    "                    # Default: use the relative path as-is\n",
    "                    dst = os.path.join(DATASET_ROOT_NOTEBOOK, rel)\n",
    "                    print(f\"[notebook] Routing to default dir: {dst}\")\n",
    "                \n",
    "                os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
    "\n",
    "                # Download only if missing\n",
    "                if not os.path.exists(dst):\n",
    "                    ok = stream_download(s3, s3_bucket, key, dst)\n",
    "                    if not ok:\n",
    "                        print(f\"[notebook] Download failed for {key}\")\n",
    "                        continue\n",
    "                    pulled_any = True\n",
    "                else:\n",
    "                    print(f\"[notebook] Skipping existing file {dst}\")\n",
    "                    pulled_any = True\n",
    "\n",
    "                # If the file is .gz, decompress and remove the .gz\n",
    "                if dst.endswith(\".gz\") and os.path.exists(dst):\n",
    "                    out_path = os.path.splitext(dst)[0]\n",
    "                    if not os.path.exists(out_path):\n",
    "                        print(f\"[notebook] Decompressing {dst} -> {out_path}\")\n",
    "                        try:\n",
    "                            with gzip.open(dst, \"rb\") as f_in, open(out_path, \"wb\") as f_out:\n",
    "                                shutil.copyfileobj(f_in, f_out)\n",
    "                        except Exception as e:\n",
    "                            print(f\"[notebook] Failed to decompress {dst}: {e}\")\n",
    "                        else:\n",
    "                            try:\n",
    "                                os.remove(dst)\n",
    "                            except Exception:\n",
    "                                pass\n",
    "\n",
    "        if pulled_any:\n",
    "            print(f\"[notebook] ✓ S3 download successful. Processed {file_count} files\")\n",
    "            data_download_successful = True\n",
    "        else:\n",
    "            print(f\"[notebook] ✗ S3 download found no files to download\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[notebook] ✗ S3 fetch failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        print(\"[notebook] Will attempt HuggingFace fallback...\")\n",
    "else:\n",
    "    print(\"[notebook] S3 not configured (missing endpoint or bucket env vars)\")\n",
    "\n",
    "# Fallback to HuggingFace if S3 was not configured or failed (requires internet)\n",
    "if not data_download_successful:\n",
    "    print(\"[notebook] Attempting HuggingFace dataset download (requires internet)...\")\n",
    "    try:\n",
    "        import json\n",
    "        import random\n",
    "        from datasets import load_dataset\n",
    "\n",
    "        # Load the Table-GPT dataset\n",
    "        print(\"[notebook] Loading Table-GPT dataset from HuggingFace...\")\n",
    "        # Load the dataset\n",
    "        dataset = load_dataset(\"b-mc2/sql-create-context\", split=\"train\")\n",
    "\n",
    "        TRAIN_SIZE = 100  # Adjust based on your time/compute budget\n",
    "\n",
    "        # Shuffle and select a subset\n",
    "        train_dataset = dataset.shuffle(seed=42).select(range(min(TRAIN_SIZE, len(dataset))))\n",
    "\n",
    "        # Convert to messages format\n",
    "        train_data = [convert_to_messages(example) for example in train_dataset]\n",
    "\n",
    "        # Save the subset to a JSONL file\n",
    "        output_file = os.path.join(TXT_SQL_DIR, \"train_All_100.jsonl\")\n",
    "        with open(output_file, \"w\") as f:\n",
    "            for example in train_data:\n",
    "                f.write(json.dumps(example) + \"\\n\")\n",
    "\n",
    "        print(f\"[notebook] ✓ HuggingFace download successful. Subset saved to {output_file}\")\n",
    "        data_download_successful = True\n",
    "\n",
    "    except Exception as hf_error:\n",
    "        print(f\"[notebook] ✗ HuggingFace download failed: {hf_error}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise RuntimeError(\n",
    "            \"Failed to download dataset from both S3 and HuggingFace. \"\n",
    "            \"In disconnected environments, ensure S3/MinIO is configured with the required data. \"\n",
    "            \"In connected environments, check your internet connection and credentials.\"\n",
    "        ) from hf_error\n",
    "\n",
    "# Verify dataset file exists\n",
    "dataset_file = os.path.join(TXT_SQL_DIR, \"train_All_100.jsonl\")\n",
    "if os.path.exists(dataset_file):\n",
    "    print(f\"[notebook] ✓ Dataset ready: {dataset_file}\")\n",
    "else:\n",
    "    raise RuntimeError(f\"Dataset file not found: {dataset_file}\")\n",
    "\n",
    "# Verify model directory has files (model will be downloaded during training if not present)\n",
    "if os.path.exists(MODEL_DIR) and os.listdir(MODEL_DIR):\n",
    "    print(f\"[notebook] ✓ Model files ready in: {MODEL_DIR}\")\n",
    "    print(f\"[notebook] Model files: {os.listdir(MODEL_DIR)[:5]}...\")  # Show first 5 files\n",
    "else:\n",
    "    print(f\"[notebook] Note: Model directory is empty: {MODEL_DIR}\")\n",
    "    print(\"[notebook] Training will download model from HuggingFace during execution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model download - use S3 if available, otherwise HuggingFace\n",
    "if os.path.exists(MODEL_DIR) and os.listdir(MODEL_DIR):\n",
    "    model_path = MODEL_DIR\n",
    "    print(f\"✓ Using local model from S3: {model_path}\")\n",
    "else:\n",
    "    # Download from HuggingFace\n",
    "    print(\"[notebook] Model not found in S3, downloading from HuggingFace...\")\n",
    "    from huggingface_hub import snapshot_download\n",
    "    \n",
    "    token = os.getenv(\"HUGGINGFACE_HUB_TOKEN\")\n",
    "    model_path = snapshot_download(\n",
    "        repo_id=\"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "        local_dir=MODEL_DIR,\n",
    "        token=token,\n",
    "        resume_download=True,\n",
    "        local_dir_use_symlinks=False,\n",
    "    )\n",
    "    print(f\"✓ Model downloaded to: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "\n",
    "# You can also try these alternatives:\n",
    "# MODEL_NAME = \"Qwen/Qwen2.5-3B-Instruct\"    # Larger, more capable\n",
    "# MODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\"  # Smaller, faster training\n",
    "# MODEL_NAME = \"meta-llama/Llama-3.2-1B-Instruct\"  # Alternative architecture\n",
    "\n",
    "# LoRA configuration\n",
    "LORA_R = 16          # Rank - start small, increase if needed\n",
    "LORA_ALPHA = 32      # Alpha - typically 2x rank\n",
    "LORA_DROPOUT = 0.0   # Dropout - 0.0 is optimized for Unsloth\n",
    "\n",
    "# Training configuration\n",
    "NUM_EPOCHS = 2            # More epochs = better learning, longer training\n",
    "LEARNING_RATE = 2e-4        # Standard LoRA learning rate\n",
    "MAX_SEQ_LEN = 1024          # Maximum sequence length\n",
    "MICRO_BATCH_SIZE = 16       # Batch size per GPU (reduce if OOM)\n",
    "GRADIENT_ACCUMULATION = 4   # Effective batch = micro_batch * grad_accum\n",
    "\n",
    "# QLoRA settings (set to True to enable 4-bit quantization)\n",
    "USE_QLORA = True  # Set to True if you have limited GPU memory\n",
    "\n",
    "params = {\n",
    "        # Model and data path\n",
    "        'model_path': MODEL_NAME,\n",
    "        'data_path': \"/opt/app-root/src/txt-sql-data/train/train_All_100.jsonl\",\n",
    "        'ckpt_output_dir': \"/opt/app-root/src/checkpoints-logs-dir\",\n",
    "        'data_output_path': \"/opt/app-root/src/lora-json/_data\",\n",
    "        # Important for LORA\n",
    "        'lr_scheduler': \"cosine\",\n",
    "        'warmup_steps': 0,\n",
    "        'seed': 42,\n",
    "        # LoRA configuration\n",
    "        'lora_r': LORA_R,\n",
    "        'lora_alpha': LORA_ALPHA,\n",
    "        'lora_dropout': LORA_DROPOUT,\n",
    "\n",
    "        # Training configuration\n",
    "        'num_epochs': NUM_EPOCHS,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'micro_batch_size': MICRO_BATCH_SIZE,\n",
    "        'max_seq_len': MAX_SEQ_LEN,\n",
    "        'gradient_accumulation_steps': GRADIENT_ACCUMULATION,\n",
    "\n",
    "        # Dataset format\n",
    "        'dataset_type' : \"chat_template\",\n",
    "        'field_messages' : \"messages\",\n",
    "        # Quantization\n",
    "        'load_in_4bit': USE_QLORA,\n",
    "        #GPU configuration\n",
    "        'nproc_per_node' : 2,\n",
    "        'nnodes' : 2,\n",
    "        # Logging\n",
    "        'logging_steps': 10,\n",
    "        'save_steps': 200,\n",
    "        'save_total_limit': 3,\n",
    "\n",
    "        # Model Checkpointing\n",
    "        'save_final_checkpoint': True,\n",
    "        'checkpoint_at_epoch': 2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubeflow.trainer import TrainerClient\n",
    "from kubeflow.trainer.rhai import TrainingHubAlgorithms\n",
    "from kubeflow.trainer.rhai import TrainingHubTrainer\n",
    "from kubeflow.common.types import KubernetesBackendConfig\n",
    "\n",
    "backend_cfg = KubernetesBackendConfig(client_configuration=api_client.configuration)\n",
    "client = TrainerClient(backend_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_runtime_name = os.getenv(\"TRAINING_RUNTIME\")\n",
    "\n",
    "if not training_runtime_name:\n",
    "    raise RuntimeError(\"TRAINING_RUNTIME environment variable is required\")\n",
    "\n",
    "th_runtime = None\n",
    "for runtime in client.list_runtimes():\n",
    "    if runtime.name == training_runtime_name:\n",
    "        th_runtime = runtime\n",
    "        print(\"Found runtime: \" + str(th_runtime))\n",
    "        break\n",
    "\n",
    "if th_runtime is None:\n",
    "    raise RuntimeError(f\"Required runtime '{training_runtime_name}' not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubeflow.trainer.options.kubernetes import (\n",
    "    PodTemplateOverrides,\n",
    "    PodTemplateOverride,\n",
    "    PodSpecOverride,\n",
    "    ContainerOverride,\n",
    ")\n",
    "\n",
    "cache_root = \"/opt/app-root/src/.cache/huggingface\"\n",
    "triton_cache = \"/opt/app-root/src/.triton\"\n",
    "\n",
    "job_name = client.train(\n",
    "    trainer=TrainingHubTrainer(\n",
    "        algorithm=TrainingHubAlgorithms.LORA_SFT,\n",
    "        func_args=params,\n",
    "        env={\n",
    "            \"HF_HOME\": cache_root,\n",
    "            \"TRITON_CACHE_DIR\": triton_cache,\n",
    "            \"XDG_CACHE_HOME\": \"/opt/app-root/src/.cache\",\n",
    "            \"NCCL_DEBUG\": \"INFO\",\n",
    "        },\n",
    "        resources_per_node={\n",
    "            \"cpu\": 4,\n",
    "            \"memory\": \"32Gi\",\n",
    "            \"nvidia.com/gpu\": 1\n",
    "        },\n",
    "    ),\n",
    "    options=[\n",
    "        PodTemplateOverrides(\n",
    "            PodTemplateOverride(\n",
    "                target_jobs=[\"node\"],\n",
    "                spec=PodSpecOverride(\n",
    "                    volumes=[\n",
    "                        {\"name\": \"work\", \"persistentVolumeClaim\": {\"claimName\": PVC_NAME}},\n",
    "                    ],\n",
    "                    containers=[\n",
    "                        ContainerOverride(\n",
    "                            name=\"node\",\n",
    "                            volume_mounts=[\n",
    "                                {\"name\": \"work\", \"mountPath\": \"/opt/app-root/src\", \"readOnly\": False},\n",
    "                            ],\n",
    "                        )\n",
    "                    ],\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "    ],\n",
    "    runtime=th_runtime,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for the running status, then wait for completion or failure\n",
    "# Using reasonable timeout for LORA training\n",
    "client.wait_for_job_status(name=job_name, status={\"Running\"}, timeout=300)\n",
    "client.wait_for_job_status(name=job_name, status={\"Complete\", \"Failed\"}, timeout=1800)  # 30 minutes for training\n",
    "\n",
    "# Get job details and logs\n",
    "job = client.get_job(name=job_name)\n",
    "pod_logs = client.get_job_logs(name=job_name, follow=False)\n",
    "\n",
    "# Collect all log lines from the generator into a list\n",
    "logs = list(pod_logs)\n",
    "log_text = \"\\n\".join(str(line) for line in logs)\n",
    "\n",
    "print(f\"Training job final status: {job.status}\")\n",
    "\n",
    "# Check 1: Job status must not be \"Failed\"  \n",
    "if job.status == \"Failed\":\n",
    "    print(f\"ERROR: Training job '{job_name}' has Failed status\")\n",
    "    print(\"Last 30 lines of logs:\")\n",
    "    for line in logs[-30:]:\n",
    "        print(line)\n",
    "    raise RuntimeError(f\"Training job '{job_name}' failed\")\n",
    "\n",
    "# Check 2: Look for the training completion message in logs\n",
    "# This is critical because the training script may catch exceptions and exit 0\n",
    "if \"[PY] LORA_SFT training complete. Result=\" not in log_text:\n",
    "    print(f\"ERROR: Training completion message not found in logs\")\n",
    "    print(\"Last 50 lines of logs:\")\n",
    "    for line in logs[-50:]:\n",
    "        print(line)\n",
    "    raise RuntimeError(f\"Training did not complete successfully - missing completion message\")\n",
    "\n",
    "print(f\"✓ Training job '{job_name}' completed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in client.get_job(name=job_name).steps:\n",
    "    print(f\"Step: {c.name}, Status: {c.status}, Devices: {c.device} x {c.device_count}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = client.get_job_logs(name=job_name, follow=False)\n",
    "\n",
    "# Collect all log lines from the generator into a list\n",
    "logs = list(pod_logs)\n",
    "log_text = \"\\n\".join(str(line) for line in logs)\n",
    "print(log_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.delete_job(job_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
