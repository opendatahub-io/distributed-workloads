{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc710e7a-3a8c-4a4e-b18a-45e370772cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install datasets transformers accelerate bitsandbytes huggingface_hub\n",
    "\n",
    "# pip Install kubeflow SDK from main branch for testing\n",
    "%pip install git+https://github.com/opendatahub-io/kubeflow-sdk.git@main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zolfezkdo1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from kubernetes import client as k8s, config as k8s_config\n",
    "\n",
    "# Edit to match your specific settings\n",
    "api_server = os.getenv(\"OPENSHIFT_API_URL\")\n",
    "token = os.getenv(\"NOTEBOOK_USER_TOKEN\")\n",
    "if not api_server or not token:\n",
    "    raise RuntimeError(\"OPENSHIFT_API_URL and NOTEBOOK_USER_TOKEN environment variables are required\")\n",
    "PVC_NAME = os.getenv(\"SHARED_PVC_NAME\", \"shared\")\n",
    "\n",
    "configuration = k8s.Configuration()\n",
    "configuration.host = api_server\n",
    "# Un-comment if your cluster API server uses a self-signed certificate or an un-trusted CA\n",
    "configuration.verify_ssl = False\n",
    "configuration.api_key = {\"authorization\": f\"Bearer {token}\"}\n",
    "api_client = k8s.ApiClient(configuration)\n",
    "\n",
    "PVC_MOUNT_PATH = \"/opt/app-root/src\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e19419-5b07-42a4-9735-d520f6aadc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Kubeflow Trainer Clients, Configs\n",
    "import json\n",
    "\n",
    "from kubeflow.trainer import TrainerClient\n",
    "from kubeflow.trainer.rhai import TrainingHubAlgorithms, TrainingHubTrainer\n",
    "from kubeflow.common.types import KubernetesBackendConfig\n",
    "\n",
    "backend_cfg = KubernetesBackendConfig(\n",
    "    client_configuration=api_client.configuration,\n",
    ")\n",
    "\n",
    "client = TrainerClient(backend_cfg)\n",
    "print(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0f43c4-8685-409c-814e-84688725d7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3/MinIO and dataset download\n",
    "import os\n",
    "import gzip\n",
    "import shutil\n",
    "import socket\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "\n",
    "import boto3\n",
    "from botocore.config import Config as BotoConfig\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# --- Global networking safety net: cap all socket operations ---\n",
    "socket.setdefaulttimeout(10)  # seconds\n",
    "\n",
    "# Notebook's PVC mount path\n",
    "PVC_NOTEBOOK_PATH = \"/opt/app-root/src\"\n",
    "DATASET_ROOT_NOTEBOOK = PVC_NOTEBOOK_PATH\n",
    "TABLE_GPT_DIR = os.path.join(DATASET_ROOT_NOTEBOOK, \"table-gpt-data\", \"train\")\n",
    "MODEL_DIR = os.path.join(DATASET_ROOT_NOTEBOOK, \"Qwen\", \"Qwen2.5-1.5B-Instruct\")\n",
    "os.makedirs(TABLE_GPT_DIR, exist_ok=True)\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# Env config for S3/MinIO\n",
    "s3_endpoint = os.getenv(\"AWS_DEFAULT_ENDPOINT\", \"\")\n",
    "s3_access_key = os.getenv(\"AWS_ACCESS_KEY_ID\", \"\")\n",
    "s3_secret_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\", \"\")\n",
    "s3_bucket = os.getenv(\"AWS_STORAGE_BUCKET\", \"\")\n",
    "s3_prefix = os.getenv(\"AWS_STORAGE_BUCKET_SFT_DIR\", \"\")\n",
    "\n",
    "data_download_successful = False\n",
    "\n",
    "def stream_download(s3, bucket, key, dst):\n",
    "    \"\"\"Download an object from S3/MinIO using streaming reads.\"\"\"\n",
    "    print(f\"[notebook] STREAM download s3://{bucket}/{key} -> {dst}\")\n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        resp = s3.get_object(Bucket=bucket, Key=key)\n",
    "    except ClientError as e:\n",
    "        print(f\"[notebook] CLIENT ERROR for {key}: {e.response.get('Error', {})}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"[notebook] OTHER ERROR for {key}: {e}\")\n",
    "        return False\n",
    "\n",
    "    body = resp[\"Body\"]\n",
    "    try:\n",
    "        with open(dst, \"wb\") as f:\n",
    "            while True:\n",
    "                try:\n",
    "                    chunk = body.read(1024 * 1024)\n",
    "                except socket.timeout as e:\n",
    "                    print(f\"[notebook] socket.timeout while reading {key}: {e}\")\n",
    "                    return False\n",
    "                if not chunk:\n",
    "                    break\n",
    "                f.write(chunk)\n",
    "    except Exception as e:\n",
    "        print(f\"[notebook] ERROR writing to {dst}: {e}\")\n",
    "        return False\n",
    "\n",
    "    t1 = time.time()\n",
    "    print(f\"[notebook] DONE stream {key} in {t1 - t0:.2f}s\")\n",
    "    return True\n",
    "\n",
    "# Try S3 download first, fall back to HuggingFace if not configured or fails\n",
    "if s3_endpoint and s3_bucket:\n",
    "    try:\n",
    "        endpoint_url = s3_endpoint if s3_endpoint.startswith(\"http\") else f\"https://{s3_endpoint}\"\n",
    "        prefix = (s3_prefix or \"\").strip(\"/\")\n",
    "        \n",
    "        print(f\"[notebook] S3 configured: endpoint={endpoint_url}, bucket={s3_bucket}, prefix={prefix or '<root>'}\")\n",
    "        \n",
    "        boto_cfg = BotoConfig(\n",
    "            signature_version=\"s3v4\",\n",
    "            s3={\"addressing_style\": \"path\"},\n",
    "            retries={\"max_attempts\": 1, \"mode\": \"standard\"},\n",
    "            connect_timeout=5,\n",
    "            read_timeout=10,\n",
    "        )\n",
    "        \n",
    "        # Create S3/MinIO client\n",
    "        s3 = boto3.client(\n",
    "            \"s3\",\n",
    "            endpoint_url=endpoint_url,\n",
    "            aws_access_key_id=s3_access_key,\n",
    "            aws_secret_access_key=s3_secret_key,\n",
    "            config=boto_cfg,\n",
    "            verify=False,\n",
    "        )\n",
    "        \n",
    "        paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "        pulled_any = False\n",
    "        file_count = 0\n",
    "        \n",
    "        print(f\"[notebook] Starting S3 download from prefix: {prefix}\")\n",
    "        for page in paginator.paginate(Bucket=s3_bucket, Prefix=prefix or \"\"):\n",
    "            contents = page.get(\"Contents\", [])\n",
    "            if not contents:\n",
    "                print(f\"[notebook] No contents found in this page\")\n",
    "                continue\n",
    "                \n",
    "            print(f\"[notebook] Found {len(contents)} objects in this page\")\n",
    "            \n",
    "            for obj in contents:\n",
    "                key = obj[\"Key\"]\n",
    "                file_count += 1\n",
    "                \n",
    "                # Skip \"directory markers\"\n",
    "                if key.endswith(\"/\"):\n",
    "                    print(f\"[notebook] Skipping directory marker: {key}\")\n",
    "                    continue\n",
    "                \n",
    "                # Determine relative path under prefix for local storage\n",
    "                rel = key[len(prefix):].lstrip(\"/\") if prefix else key\n",
    "                print(f\"[notebook] Processing key={key}, rel={rel}\")\n",
    "                \n",
    "                # Route to appropriate directory based on content type\n",
    "                if \"table-gpt\" in rel.lower() or rel.endswith(\".jsonl\"):\n",
    "                    dst = os.path.join(TABLE_GPT_DIR, os.path.basename(rel))\n",
    "                    print(f\"[notebook] Routing to dataset dir: {dst}\")\n",
    "                elif \"qwen\" in rel.lower() or any(rel.endswith(ext) for ext in [\".bin\", \".json\", \".model\", \".safetensors\", \".txt\"]):\n",
    "                    dst = os.path.join(MODEL_DIR, rel.split(\"Qwen2.5-1.5B-Instruct/\")[-1] if \"Qwen2.5-1.5B-Instruct\" in rel else os.path.basename(rel))\n",
    "                    print(f\"[notebook] Routing to model dir: {dst}\")\n",
    "                else:\n",
    "                    dst = os.path.join(DATASET_ROOT_NOTEBOOK, rel)\n",
    "                    print(f\"[notebook] Routing to default dir: {dst}\")\n",
    "                \n",
    "                os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
    "                \n",
    "                # Download only if missing\n",
    "                if not os.path.exists(dst):\n",
    "                    ok = stream_download(s3, s3_bucket, key, dst)\n",
    "                    if not ok:\n",
    "                        print(f\"[notebook] Download failed for {key}\")\n",
    "                        continue\n",
    "                    pulled_any = True\n",
    "                else:\n",
    "                    print(f\"[notebook] Skipping existing file {dst}\")\n",
    "                    pulled_any = True\n",
    "                \n",
    "                # If the file is .gz, decompress and remove the .gz\n",
    "                if dst.endswith(\".gz\") and os.path.exists(dst):\n",
    "                    out_path = os.path.splitext(dst)[0]\n",
    "                    if not os.path.exists(out_path):\n",
    "                        print(f\"[notebook] Decompressing {dst} -> {out_path}\")\n",
    "                        try:\n",
    "                            with gzip.open(dst, \"rb\") as f_in, open(out_path, \"wb\") as f_out:\n",
    "                                shutil.copyfileobj(f_in, f_out)\n",
    "                        except Exception as e:\n",
    "                            print(f\"[notebook] Failed to decompress {dst}: {e}\")\n",
    "                        else:\n",
    "                            try:\n",
    "                                os.remove(dst)\n",
    "                            except Exception:\n",
    "                                pass\n",
    "        \n",
    "        if pulled_any:\n",
    "            print(f\"[notebook] ‚úì S3 download successful. Processed {file_count} files\")\n",
    "            data_download_successful = True\n",
    "        else:\n",
    "            print(f\"[notebook] ‚úó S3 download found no files to download\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"[notebook] ‚úó S3 fetch failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        print(\"[notebook] Will attempt HuggingFace fallback...\")\n",
    "else:\n",
    "    print(\"[notebook] S3 not configured (missing endpoint or bucket env vars)\")\n",
    "\n",
    "# Fallback to HuggingFace if S3 was not configured or failed (requires internet)\n",
    "if not data_download_successful:\n",
    "    print(\"[notebook] Attempting HuggingFace dataset download (requires internet)...\")\n",
    "    try:\n",
    "        from datasets import load_dataset\n",
    "        \n",
    "        print(\"[notebook] Loading Table-GPT dataset from HuggingFace...\")\n",
    "        dataset = load_dataset(\"LipengCS/Table-GPT\", \"All\")\n",
    "        \n",
    "        train_data = dataset[\"train\"]\n",
    "        print(f\"[notebook] Original training set size: {len(train_data)}\")\n",
    "        \n",
    "        # Create a random subset of 100 samples\n",
    "        random.seed(42)\n",
    "        subset_indices = random.sample(range(len(train_data)), min(100, len(train_data)))\n",
    "        subset_data = train_data.select(subset_indices)\n",
    "        \n",
    "        print(f\"[notebook] Subset size: {len(subset_data)}\")\n",
    "        \n",
    "        # Save the subset to a JSONL file\n",
    "        output_file = os.path.join(TABLE_GPT_DIR, \"train_All_100.jsonl\")\n",
    "        with open(output_file, \"w\") as f:\n",
    "            for example in subset_data:\n",
    "                f.write(json.dumps(example) + \"\\n\")\n",
    "        \n",
    "        print(f\"[notebook] ‚úì HuggingFace download successful. Subset saved to {output_file}\")\n",
    "        data_download_successful = True\n",
    "        \n",
    "    except Exception as hf_error:\n",
    "        print(f\"[notebook] ‚úó HuggingFace download failed: {hf_error}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise RuntimeError(\n",
    "            \"Failed to download dataset from both S3 and HuggingFace. \"\n",
    "            \"In disconnected environments, ensure S3/MinIO is configured with the required data. \"\n",
    "            \"In connected environments, check your internet connection and credentials.\"\n",
    "        ) from hf_error\n",
    "\n",
    "# Verify dataset file exists\n",
    "dataset_file = os.path.join(TABLE_GPT_DIR, \"train_All_100.jsonl\")\n",
    "if os.path.exists(dataset_file):\n",
    "    print(f\"[notebook] ‚úì Dataset ready: {dataset_file}\")\n",
    "else:\n",
    "    raise RuntimeError(f\"Dataset file not found: {dataset_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dce5f4-2bfc-4021-ac94-f19a8b491292",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "from huggingface_hub.utils import HfHubHTTPError, RepositoryNotFoundError\n",
    "\n",
    "\n",
    "def download_model_snapshot(\n",
    "    model_id: str,\n",
    "    output_dir: str,\n",
    "    revision: str = \"main\",\n",
    "    token: str | bool | None = None,\n",
    "    allow_patterns: list[str] | None = None,\n",
    "    ignore_patterns: list[str] | None = None,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Downloads a model snapshot from the Hugging Face Hub using print statements.\n",
    "\n",
    "    Args:\n",
    "        model_id (str): The repository ID of the model (e.g., \"meta-llama/Meta-Llama-3-8B\").\n",
    "        output_dir (str): The local directory to save the model files.\n",
    "        revision (str): The model revision (branch, tag, or commit hash).\n",
    "        token (str | bool | None): Hugging Face auth token.\n",
    "            - If None (default): Uses HUGGINGFACE_HUB_TOKEN env var or login.\n",
    "            - If True: Explicitly uses the stored token.\n",
    "            - If str: Uses the provided token string.\n",
    "        allow_patterns (list[str], optional): List of glob patterns to include.\n",
    "        ignore_patterns (list[str], optional): List of glob patterns to exclude.\n",
    "\n",
    "    Returns:\n",
    "        str: The path to the downloaded snapshot directory.\n",
    "\n",
    "    Raises:\n",
    "        RepositoryNotFoundError: If the model is not found.\n",
    "        HfHubHTTPError: For authentication errors (e.g., 401) or other HTTP issues.\n",
    "        OSError: If the output directory cannot be created.\n",
    "        Exception: For other unexpected errors.\n",
    "    \"\"\"\n",
    "    print(f\"Attempting to download repository: {model_id}\")\n",
    "    print(f\"Target directory: {output_dir}\")\n",
    "    print(f\"Revision: {revision}\")\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    try:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    except OSError as e:\n",
    "        print(f\"Failed to create output directory {output_dir}: {e}\")\n",
    "        raise e  # Re-raise the exception\n",
    "\n",
    "    # Handle token for gated/private models\n",
    "    if token is None:\n",
    "        token = os.getenv(\"HUGGINGFACE_HUB_TOKEN\")\n",
    "        if token:\n",
    "            print(\n",
    "                \"Using HUGGINGFACE_HUB_TOKEN environment variable for authentication.\"\n",
    "            )\n",
    "        else:\n",
    "            token = True  # Use locally cached token\n",
    "            print(\"No explicit token. Will use locally cached token (if logged in).\")\n",
    "\n",
    "    try:\n",
    "        # This is the core command.\n",
    "        snapshot_path = snapshot_download(\n",
    "            repo_id=model_id,\n",
    "            local_dir=output_dir,\n",
    "            revision=revision,\n",
    "            token=token,\n",
    "            allow_patterns=allow_patterns,\n",
    "            ignore_patterns=ignore_patterns,\n",
    "            resume_download=True,\n",
    "            local_dir_use_symlinks=False,\n",
    "        )\n",
    "\n",
    "        print(f\"‚úÖ Successfully downloaded model to: {snapshot_path}\")\n",
    "        return snapshot_path\n",
    "\n",
    "    except RepositoryNotFoundError as e:\n",
    "        print(f\"‚ùå Error: Repository '{model_id}' (revision: {revision}) not found.\")\n",
    "        print(\"Please check the model ID and revision name.\")\n",
    "        raise e\n",
    "\n",
    "    except HfHubHTTPError as e:\n",
    "        print(f\"‚ùå HTTP Error: Failed to access {model_id}.\")\n",
    "        print(f\"Status Code: {e.response.status_code}\")\n",
    "        if e.response.status_code == 401:\n",
    "            print(\"This is an 'Unauthorized' error. The model may be private or gated.\")\n",
    "            print(\"Ensure you have accepted the license terms on the model's HF page.\")\n",
    "            print(\n",
    "                \"And that you are authenticated (use `huggingface-cli login` in your terminal).\"\n",
    "            )\n",
    "        raise e\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå An unexpected error occurred: {e}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7809dab7-ca08-4bb6-8254-2c3b9aed94f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model download - use S3 if available, otherwise HuggingFace\n",
    "if os.path.exists(MODEL_DIR) and os.listdir(MODEL_DIR):\n",
    "    model_path = MODEL_DIR\n",
    "    print(f\"‚úì Using local model from S3: {model_path}\")\n",
    "else:\n",
    "    # Download from HuggingFace\n",
    "    print(\"[notebook] Model not found in S3, downloading from HuggingFace...\")\n",
    "    from huggingface_hub import snapshot_download\n",
    "    \n",
    "    token = os.getenv(\"HUGGINGFACE_HUB_TOKEN\")\n",
    "    model_path = snapshot_download(\n",
    "        repo_id=\"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "        local_dir=MODEL_DIR,\n",
    "        token=token,\n",
    "        resume_download=True,\n",
    "        local_dir_use_symlinks=False,\n",
    "    )\n",
    "    print(f\"‚úì Model downloaded to: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12126d5b-e928-47d3-80a8-6ad521f0908c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use local model path (downloaded in previous cell if not from S3)\n",
    "LOCAL_MODEL_PATH = \"/opt/app-root/src/Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "\n",
    "training_parameters = {\n",
    "    ################################################################################\n",
    "    # ü§ñ Model + Data Paths                                                        #\n",
    "    ################################################################################\n",
    "    \"model_path\": LOCAL_MODEL_PATH,  # Always use local path with actual files\n",
    "    \"data_path\": \"/opt/app-root/src/table-gpt-data/train/train_All_100.jsonl\",\n",
    "    \"ckpt_output_dir\": \"/opt/app-root/src/checkpoints\",\n",
    "    \"data_output_dir\": \"/opt/app-root/src/sft-data\",\n",
    "    ################################################################################\n",
    "    # üèãÔ∏è‚Äç‚ôÄÔ∏è Training Hyperparameters                                                  #\n",
    "    ################################################################################\n",
    "    \"effective_batch_size\": 128,\n",
    "    \"learning_rate\": 5e-6,\n",
    "    \"num_epochs\": 1,\n",
    "    \"lr_scheduler\": \"cosine\",\n",
    "    \"warmup_steps\": 0,\n",
    "    \"seed\": 42,\n",
    "    ################################################################################\n",
    "    # üèéÔ∏è Performance Hyperparameters                                               #\n",
    "    ################################################################################\n",
    "    \"max_tokens_per_gpu\": 10000,\n",
    "    \"max_seq_len\": 8192,\n",
    "    ################################################################################\n",
    "    # üíæ Checkpointing Settings                                                    #\n",
    "    ################################################################################\n",
    "    \"checkpoint_at_epoch\": False,\n",
    "    ################################################################################\n",
    "    # üöÄ Distributed Training Configuration                                        #\n",
    "    ################################################################################\n",
    "    # Override runtime defaults: 2 nodes with 1 GPU each\n",
    "    \"nnodes\": 2,\n",
    "    \"nproc_per_node\": 1,\n",
    "}\n",
    "\n",
    "print(\"‚öôÔ∏è  Training Hyperparameters\")\n",
    "print(\"=\" * 50)\n",
    "print(json.dumps(training_parameters, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab7fdad-32ae-4f90-86ec-e41edf2aacf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the TrainingHub runtime\n",
    "training_runtime_name = os.getenv(\"TRAINING_RUNTIME\")\n",
    "if not training_runtime_name:\n",
    "    raise RuntimeError(\"TRAINING_RUNTIME environment variable is required\")\n",
    "\n",
    "th_runtime = None\n",
    "for runtime in client.list_runtimes():\n",
    "    if runtime.name == training_runtime_name:\n",
    "        th_runtime = runtime\n",
    "        print(\"Found runtime: \" + str(th_runtime))\n",
    "        break\n",
    "\n",
    "if th_runtime is None:\n",
    "    raise RuntimeError(f\"Required runtime '{training_runtime_name}' not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ltl994l22wj",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubeflow.trainer.options.kubernetes import (\n",
    "    PodTemplateOverrides,\n",
    "    PodTemplateOverride,\n",
    "    PodSpecOverride,\n",
    "    ContainerOverride,\n",
    ")\n",
    "\n",
    "cache_root = \"/opt/app-root/src/.cache/huggingface\"\n",
    "triton_cache = \"/opt/app-root/src/.triton\"\n",
    "\n",
    "job_name = client.train(\n",
    "    trainer=TrainingHubTrainer(\n",
    "        algorithm=TrainingHubAlgorithms.SFT,\n",
    "        func_args=training_parameters,\n",
    "        env={ \n",
    "            \"HF_HOME\": cache_root,\n",
    "            \"TRITON_CACHE_DIR\": triton_cache,\n",
    "            \"XDG_CACHE_HOME\": \"/opt/app-root/src/.cache\",\n",
    "            \"NCCL_DEBUG\": \"INFO\",\n",
    "        },\n",
    "        resources_per_node={\n",
    "            \"cpu\": 4,\n",
    "            \"memory\": \"32Gi\",\n",
    "            \"nvidia.com/gpu\": 1\n",
    "        },\n",
    "    ),\n",
    "    options=[\n",
    "        PodTemplateOverrides(\n",
    "            PodTemplateOverride(\n",
    "                target_jobs=[\"node\"],\n",
    "                spec=PodSpecOverride(\n",
    "                    volumes=[\n",
    "                        {\"name\": \"work\", \"persistentVolumeClaim\": {\"claimName\": PVC_NAME}},\n",
    "                    ],\n",
    "                    containers=[\n",
    "                        ContainerOverride(\n",
    "                            name=\"node\", \n",
    "                            volume_mounts=[\n",
    "                                {\"name\": \"work\", \"mountPath\": \"/opt/app-root/src\", \"readOnly\": False},\n",
    "                            ],\n",
    "                        )\n",
    "                    ],\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "    ],\n",
    "    runtime=th_runtime,\n",
    ")\n",
    "\n",
    "print(f\"Training job created: {job_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ca62a4-7f3e-48b3-94a4-144f1d92cb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for the running status, then wait for completion or failure\n",
    "client.wait_for_job_status(name=job_name, status={\"Running\"}, timeout=300)\n",
    "client.wait_for_job_status(name=job_name, status={\"Complete\", \"Failed\"}, timeout=3600)  # 1 hour for SFT training\n",
    "\n",
    "# Get job details and logs\n",
    "job = client.get_job(name=job_name)\n",
    "pod_logs = client.get_job_logs(job_name, follow=False)\n",
    "\n",
    "# Flatten all pod logs into a single list of lines\n",
    "logs = []\n",
    "for log_line in pod_logs:\n",
    "    logs.extend(str(log_line).splitlines())\n",
    "\n",
    "log_text = \"\\n\".join(logs)\n",
    "\n",
    "print(f\"Training job final status: {job.status}\")\n",
    "\n",
    "# Check 1: Job status must not be \"Failed\"  \n",
    "if job.status == \"Failed\":\n",
    "    print(f\"ERROR: Training job '{job_name}' has Failed status\")\n",
    "    print(\"Last 30 lines of logs:\")\n",
    "    for line in logs[-30:]:\n",
    "        print(line)\n",
    "    raise RuntimeError(f\"Training job '{job_name}' failed\")\n",
    "\n",
    "# Check 2: Look for the training completion message in logs\n",
    "# This is critical because the training script may catch exceptions and exit 0\n",
    "if \"[PY] SFT training complete. Result=\" not in log_text:\n",
    "    print(f\"ERROR: Training completion message not found in logs\")\n",
    "    print(\"Last 50 lines of logs:\")\n",
    "    for line in logs[-50:]:\n",
    "        print(line)\n",
    "    raise RuntimeError(f\"Training did not complete successfully - missing completion message\")\n",
    "\n",
    "print(f\"‚úì Training job '{job_name}' completed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39871e5c-d0ff-4b89-9d31-75e2d71bae21",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in client.get_job(name=job_name).steps:\n",
    "    print(f\"Step: {c.name}, Status: {c.status}, Devices: {c.device} x {c.device_count}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sovsqgypat",
   "metadata": {},
   "outputs": [],
   "source": [
    "for logline in client.get_job_logs(job_name, follow=False):\n",
    "    print(logline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1edf2f-6cf1-4cba-8b61-58955eed4543",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.delete_job(job_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
