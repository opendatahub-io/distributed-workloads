{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubernetes import client as k8s, config as k8s_config\n",
    "# Edit to match your specific settings\n",
    "api_server = os.getenv(\"OPENSHIFT_API_URL\")\n",
    "token = os.getenv(\"NOTEBOOK_USER_TOKEN\")\n",
    "if not api_server or not token:\n",
    "    raise RuntimeError(\"OPENSHIFT_API_URL and NOTEBOOK_USER_TOKEN environment variables are required\")\n",
    "PVC_NAME = os.getenv(\"SHARED_PVC_NAME\", \"shared\")\n",
    "\n",
    "configuration = k8s.Configuration()\n",
    "configuration.host = api_server\n",
    "# Un-comment if your cluster API server uses a self-signed certificate or an un-trusted CA\n",
    "configuration.verify_ssl = False\n",
    "configuration.api_key = {\"authorization\": f\"Bearer {token}\"}\n",
    "api_client = k8s.ApiClient(configuration)\n",
    "\n",
    "PVC_MOUNT_PATH = \"/opt/app-root/src\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport gzip\nimport shutil\nimport socket\n\nimport boto3\nfrom botocore.config import Config as BotoConfig\nfrom botocore.exceptions import ClientError\n\n# --- Global networking safety net: cap all socket operations ---\nsocket.setdefaulttimeout(10)  # seconds\n\n# Notebook's PVC mount path (per Notebook CR). Training pods will mount the same PVC at /opt/app-root/src\nPVC_NOTEBOOK_PATH = \"/opt/app-root/src\"\nDATASET_ROOT_NOTEBOOK = PVC_NOTEBOOK_PATH\nTABLE_GPT_DIR = os.path.join(DATASET_ROOT_NOTEBOOK, \"table-gpt-data\", \"train\")\nMODEL_DIR = os.path.join(DATASET_ROOT_NOTEBOOK, \"Qwen\", \"Qwen2.5-1.5B-Instruct\")\nos.makedirs(TABLE_GPT_DIR, exist_ok=True)\nos.makedirs(MODEL_DIR, exist_ok=True)\n\n# Env config for S3/MinIO\ns3_endpoint = os.getenv(\"AWS_DEFAULT_ENDPOINT\", \"\")\ns3_access_key = os.getenv(\"AWS_ACCESS_KEY_ID\", \"\")\ns3_secret_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\", \"\")\ns3_bucket = os.getenv(\"AWS_STORAGE_BUCKET\", \"\")\ns3_prefix = os.getenv(\"AWS_STORAGE_BUCKET_LORA_SFT_DIR\", \"\")\n\ndata_download_successful = False\n\ndef stream_download(s3, bucket, key, dst):\n    \"\"\"Download an object from S3/MinIO using streaming reads.\"\"\"\n    print(f\"[notebook] STREAM download s3://{bucket}/{key} -> {dst}\")\n    t0 = time.time()\n    try:\n        resp = s3.get_object(Bucket=bucket, Key=key)\n    except ClientError as e:\n        print(f\"[notebook] CLIENT ERROR for {key}: {e.response.get('Error', {})}\")\n        return False\n    except Exception as e:\n        print(f\"[notebook] OTHER ERROR for {key}: {e}\")\n        return False\n\n    body = resp[\"Body\"]\n    try:\n        with open(dst, \"wb\") as f:\n            while True:\n                try:\n                    chunk = body.read(1024 * 1024)\n                except socket.timeout as e:\n                    print(f\"[notebook] socket.timeout while reading {key}: {e}\")\n                    return False\n                if not chunk:\n                    break\n                f.write(chunk)\n    except Exception as e:\n        print(f\"[notebook] ERROR writing to {dst}: {e}\")\n        return False\n\n    t1 = time.time()\n    print(f\"[notebook] DONE stream {key} in {t1 - t0:.2f}s\")\n    return True\n\n# Try S3 download first, fall back to HuggingFace if not configured or fails\nif s3_endpoint and s3_bucket:\n    try:\n        endpoint_url = s3_endpoint if s3_endpoint.startswith(\"http\") else f\"https://{s3_endpoint}\"\n        prefix = (s3_prefix or \"\").strip(\"/\")\n        \n        print(f\"[notebook] S3 configured: endpoint={endpoint_url}, bucket={s3_bucket}, prefix={prefix or '<root>'}\")\n        \n        boto_cfg = BotoConfig(\n            signature_version=\"s3v4\",\n            s3={\"addressing_style\": \"path\"},\n            retries={\"max_attempts\": 1, \"mode\": \"standard\"},\n            connect_timeout=5,\n            read_timeout=10,\n        )\n        \n        # Create S3/MinIO client\n        s3 = boto3.client(\n            \"s3\",\n            endpoint_url=endpoint_url,\n            aws_access_key_id=s3_access_key,\n            aws_secret_access_key=s3_secret_key,\n            config=boto_cfg,\n            verify=False,\n        )\n        \n        paginator = s3.get_paginator(\"list_objects_v2\")\n        pulled_any = False\n        file_count = 0\n        \n        print(f\"[notebook] Starting S3 download from prefix: {prefix}\")\n        for page in paginator.paginate(Bucket=s3_bucket, Prefix=prefix or \"\"):\n            contents = page.get(\"Contents\", [])\n            if not contents:\n                print(f\"[notebook] No contents found in this page\")\n                continue\n                \n            print(f\"[notebook] Found {len(contents)} objects in this page\")\n            \n            for obj in contents:\n                key = obj[\"Key\"]\n                file_count += 1\n                \n                # Skip \"directory markers\"\n                if key.endswith(\"/\"):\n                    print(f\"[notebook] Skipping directory marker: {key}\")\n                    continue\n                \n                # Determine relative path under prefix for local storage\n                rel = key[len(prefix):].lstrip(\"/\") if prefix else key\n                print(f\"[notebook] Processing key={key}, rel={rel}\")\n                \n                # Route to appropriate directory based on content type\n                if \"table-gpt\" in rel.lower() or rel.endswith(\".jsonl\"):\n                    dst = os.path.join(TABLE_GPT_DIR, os.path.basename(rel))\n                    print(f\"[notebook] Routing to dataset dir: {dst}\")\n                elif \"qwen\" in rel.lower() or any(rel.endswith(ext) for ext in [\".bin\", \".json\", \".model\", \".safetensors\", \".txt\"]):\n                    dst = os.path.join(MODEL_DIR, rel.split(\"Qwen2.5-1.5B-Instruct/\")[-1] if \"Qwen2.5-1.5B-Instruct\" in rel else os.path.basename(rel))\n                    print(f\"[notebook] Routing to model dir: {dst}\")\n                else:\n                    dst = os.path.join(DATASET_ROOT_NOTEBOOK, rel)\n                    print(f\"[notebook] Routing to default dir: {dst}\")\n                \n                os.makedirs(os.path.dirname(dst), exist_ok=True)\n                \n                # Download only if missing\n                if not os.path.exists(dst):\n                    ok = stream_download(s3, s3_bucket, key, dst)\n                    if not ok:\n                        print(f\"[notebook] Download failed for {key}\")\n                        continue\n                    pulled_any = True\n                else:\n                    print(f\"[notebook] Skipping existing file {dst}\")\n                    pulled_any = True\n                \n                # If the file is .gz, decompress and remove the .gz\n                if dst.endswith(\".gz\") and os.path.exists(dst):\n                    out_path = os.path.splitext(dst)[0]\n                    if not os.path.exists(out_path):\n                        print(f\"[notebook] Decompressing {dst} -> {out_path}\")\n                        try:\n                            with gzip.open(dst, \"rb\") as f_in, open(out_path, \"wb\") as f_out:\n                                shutil.copyfileobj(f_in, f_out)\n                        except Exception as e:\n                            print(f\"[notebook] Failed to decompress {dst}: {e}\")\n                        else:\n                            try:\n                                os.remove(dst)\n                            except Exception:\n                                pass\n        \n        if pulled_any:\n            print(f\"[notebook] \u2713 S3 download successful. Processed {file_count} files\")\n            data_download_successful = True\n        else:\n            print(f\"[notebook] \u2717 S3 download found no files to download\")\n        \n    except Exception as e:\n        print(f\"[notebook] \u2717 S3 fetch failed: {e}\")\n        import traceback\n        traceback.print_exc()\n        print(\"[notebook] Will attempt HuggingFace fallback...\")\nelse:\n    print(\"[notebook] S3 not configured (missing endpoint or bucket env vars)\")\n\n# Fallback to HuggingFace if S3 was not configured or failed (requires internet)\nif not data_download_successful:\n    print(\"[notebook] Attempting HuggingFace dataset download (requires internet)...\")\n    try:\n        import json\n        import random\n        from datasets import load_dataset\n        \n        print(\"[notebook] Loading Table-GPT dataset from HuggingFace...\")\n        dataset = load_dataset(\"LipengCS/Table-GPT\", \"All\")\n        \n        train_data = dataset[\"train\"]\n        print(f\"[notebook] Original training set size: {len(train_data)}\")\n        \n        # Create a random subset of 100 samples\n        random.seed(42)\n        subset_indices = random.sample(range(len(train_data)), min(100, len(train_data)))\n        subset_data = train_data.select(subset_indices)\n        \n        print(f\"[notebook] Subset size: {len(subset_data)}\")\n        \n        # Save the subset to a JSONL file\n        output_file = os.path.join(TABLE_GPT_DIR, \"train_All_100.jsonl\")\n        with open(output_file, \"w\") as f:\n            for example in subset_data:\n                f.write(json.dumps(example) + \"\\n\")\n        \n        print(f\"[notebook] \u2713 HuggingFace download successful. Subset saved to {output_file}\")\n        data_download_successful = True\n        \n    except Exception as hf_error:\n        print(f\"[notebook] \u2717 HuggingFace download failed: {hf_error}\")\n        import traceback\n        traceback.print_exc()\n        raise RuntimeError(\n            \"Failed to download dataset from both S3 and HuggingFace. \"\n            \"In disconnected environments, ensure S3/MinIO is configured with the required data. \"\n            \"In connected environments, check your internet connection and credentials.\"\n        ) from hf_error\n\n# Verify dataset file exists\ndataset_file = os.path.join(TABLE_GPT_DIR, \"train_All_100.jsonl\")\nif os.path.exists(dataset_file):\n    print(f\"[notebook] \u2713 Dataset ready: {dataset_file}\")\nelse:\n    raise RuntimeError(f\"Dataset file not found: {dataset_file}\")\n\n# Verify model directory has files (model will be downloaded during training if not present)\nif os.path.exists(MODEL_DIR) and os.listdir(MODEL_DIR):\n    print(f\"[notebook] \u2713 Model files ready in: {MODEL_DIR}\")\n    print(f\"[notebook] Model files: {os.listdir(MODEL_DIR)[:5]}...\")  # Show first 5 files\nelse:\n    print(f\"[notebook] Note: Model directory is empty: {MODEL_DIR}\")\n    print(\"[notebook] Training will download model from HuggingFace during execution\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine model path based on whether S3 download succeeded\n",
    "import os\n",
    "LOCAL_MODEL_PATH = \"/opt/app-root/src/Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "HUGGINGFACE_MODEL_ID = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "\n",
    "# Check if model was downloaded from S3\n",
    "model_downloaded = os.path.exists(LOCAL_MODEL_PATH) and len(os.listdir(LOCAL_MODEL_PATH)) > 0\n",
    "\n",
    "if model_downloaded:\n",
    "    model_path_to_use = LOCAL_MODEL_PATH\n",
    "    print(f\"\u2713 Using local model from S3: {model_path_to_use}\")\n",
    "else:\n",
    "    model_path_to_use = HUGGINGFACE_MODEL_ID  \n",
    "    print(f\"\u2713 Using HuggingFace model ID: {model_path_to_use}\")\n",
    "\n",
    "params = {\n",
    "    ###########################################################################\n",
    "    # \ud83e\udd16 Model + Data Paths                                                   #\n",
    "    ###########################################################################\n",
    "    \"model_path\": model_path_to_use,\n",
    "    \"data_path\": \"/opt/app-root/src/table-gpt-data/train/train_All_100.jsonl\",\n",
    "    \"ckpt_output_dir\": \"/opt/app-root/src/checkpoints-logs-dir\",\n",
    "    \"data_output_path\": \"/opt/app-root/src/lora-sft-json/_data\",\n",
    "    ############################################################################\n",
    "    # \ud83c\udfcb\ufe0f\u200d\u2640\ufe0f Training Hyperparameters                                              #\n",
    "    ############################################################################\n",
    "    \"effective_batch_size\": 128,\n",
    "    \"learning_rate\": 1.0e-4,  # LoRA typically uses higher learning rate than full fine-tuning\n",
    "    \"num_epochs\": 1,\n",
    "    \"lr_scheduler\": \"cosine\",\n",
    "    \"warmup_steps\": 0,\n",
    "    \"seed\": 42,\n",
    "    ############################################################################\n",
    "    # \ud83d\udd27 LoRA Configuration                                                    #\n",
    "    ############################################################################\n",
    "    \"lora_r\": 32,              # LoRA rank (from lora_example.py default)\n",
    "    \"lora_alpha\": 64,          # LoRA alpha parameter (from lora_example.py default)\n",
    "    \"lora_dropout\": 0.0,       # LoRA dropout (optimized for Unsloth)\n",
    "    ###########################################################################\n",
    "    # \ud83c\udfce\ufe0f Performance Hyperparameters                                          #\n",
    "    ###########################################################################\n",
    "    \"max_tokens_per_gpu\": 32000,\n",
    "    \"max_seq_len\": 2048,\n",
    "    ############################################################################\n",
    "    # \ud83d\udcbe Checkpointing Settings                                                #\n",
    "    ############################################################################\n",
    "    \"save_final_checkpoint\": True,\n",
    "    \"checkpoint_at_epoch\": False,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubeflow.trainer import TrainerClient\n",
    "from kubeflow.trainer.rhai import TrainingHubAlgorithms\n",
    "from kubeflow.trainer.rhai import TrainingHubTrainer\n",
    "from kubeflow_trainer_api import models\n",
    "from kubeflow.common.types import KubernetesBackendConfig\n",
    "\n",
    "backend_cfg = KubernetesBackendConfig(\n",
    "    client_configuration=api_client.configuration,   # <\u2014 key part\n",
    ")\n",
    "\n",
    "client = TrainerClient(backend_cfg)\n",
    "print(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "th_runtime = None\n",
    "for runtime in client.list_runtimes():\n",
    "    if runtime.name == \"training-hub03-cuda128-torch28-py312\":\n",
    "        th_runtime = runtime\n",
    "        print(\"Found runtime: \" + str(th_runtime))\n",
    "        break\n",
    "\n",
    "if th_runtime is None:\n",
    "    raise RuntimeError(\"Required runtime 'training-hub03-cuda128-torch28-py312' not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubeflow.trainer.options.kubernetes import (\n",
    "    PodTemplateOverrides,\n",
    "    PodTemplateOverride,\n",
    "    PodSpecOverride,\n",
    "    ContainerOverride,\n",
    ")\n",
    "\n",
    "cache_root = \"/opt/app-root/src/.cache/huggingface\"\n",
    "triton_cache = \"/opt/app-root/src/.triton\"\n",
    "\n",
    "job_name = client.train(\n",
    "    trainer=TrainingHubTrainer(\n",
    "        algorithm=TrainingHubAlgorithms.LORA_SFT,\n",
    "        func_args=params,\n",
    "        env={ \n",
    "            \"HF_HOME\": cache_root,\n",
    "            \"TRITON_CACHE_DIR\": triton_cache,\n",
    "            \"XDG_CACHE_HOME\": \"/opt/app-root/src/.cache\",\n",
    "            \"NCCL_DEBUG\": \"INFO\",\n",
    "        },\n",
    "    ),\n",
    "    options=[\n",
    "        PodTemplateOverrides(\n",
    "            PodTemplateOverride(\n",
    "                target_jobs=[\"node\"],\n",
    "                spec=PodSpecOverride(\n",
    "                    volumes=[\n",
    "                        {\"name\": \"work\", \"persistentVolumeClaim\": {\"claimName\": PVC_NAME}},\n",
    "                    ],\n",
    "                    containers=[\n",
    "                        ContainerOverride(\n",
    "                            name=\"node\", \n",
    "                            volume_mounts=[\n",
    "                                {\"name\": \"work\", \"mountPath\": \"/opt/app-root/src\", \"readOnly\": False},\n",
    "                            ],\n",
    "                        )\n",
    "                    ],\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "    ],\n",
    "    runtime=th_runtime,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for the running status, then wait for completion or failure\n",
    "# Using reasonable timeout for LORA-SFT training\n",
    "client.wait_for_job_status(name=job_name, status={\"Running\"}, timeout=300)\n",
    "client.wait_for_job_status(name=job_name, status={\"Complete\", \"Failed\"}, timeout=1800)  # 30 minutes for training\n",
    "\n",
    "# Get job details and logs\n",
    "job = client.get_job(name=job_name)\n",
    "pod_logs, _ = client.get_job_logs(name=job_name, follow=False)\n",
    "\n",
    "# Flatten all pod logs into a single list of lines\n",
    "logs = []\n",
    "for log_text_content in pod_logs.values():\n",
    "    logs.extend(str(log_text_content).splitlines())\n",
    "\n",
    "log_text = \"\\n\".join(logs)\n",
    "\n",
    "print(f\"Training job final status: {job.status}\")\n",
    "\n",
    "# Check 1: Job status must not be \\\"Failed\\\"  \n",
    "if job.status == \"Failed\":\n",
    "    print(f\"ERROR: Training job '{job_name}' has Failed status\")\n",
    "    print(\"Last 30 lines of logs:\")\n",
    "    for line in logs[-30:]:\n",
    "        print(line)\n",
    "    raise RuntimeError(f\"Training job '{job_name}' failed\")\n",
    "\n",
    "# Check 2: Look for the training completion message in logs\n",
    "# This is critical because the training script may catch exceptions and exit 0\n",
    "if \"[PY] LORA_SFT training complete. Result=\" not in log_text:\n",
    "    print(f\"ERROR: Training completion message not found in logs\")\n",
    "    print(\"Last 50 lines of logs:\")\n",
    "    for line in logs[-50:]:\n",
    "        print(line)\n",
    "    raise RuntimeError(f\"Training did not complete successfully - missing completion message\")\n",
    "\n",
    "print(f\"\u2713 Training job '{job_name}' completed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in client.get_job(name=job_name).steps:\n",
    "    print(f\"Step: {c.name}, Status: {c.status}, Devices: {c.device} x {c.device_count}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for logline in client.get_job_logs(job_name, follow=False):\n",
    "    print(logline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.delete_job(job_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}