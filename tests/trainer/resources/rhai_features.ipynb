{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RHAI Features Test - TrainJob Submission\n",
        "\n",
        "This notebook submits a TrainJob with RHAI features (progression tracking).\n",
        "Uses shared PVC for HuggingFace cache across distributed training nodes.\n",
        "Assertions are handled in the Go test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_bloom():\n",
        "    \"\"\"Training function for distributed training.\n",
        "    \n",
        "    Auto-detects data source:\n",
        "    - If local data exists on shared PVC (pre-downloaded by notebook) > use it\n",
        "    - Otherwise > download from HuggingFace Hub\n",
        "    \n",
        "    For disconnected environments:\n",
        "    - The notebook pre-downloads model/dataset from S3 to shared PVC\n",
        "    - Training pods just load from local paths (no S3 access needed)\n",
        "    \"\"\"\n",
        "    import os\n",
        "    os.environ[\"HF_HOME\"] = \"/workspace/hf_cache\"\n",
        "    \n",
        "    import torch\n",
        "    import torch.distributed as dist\n",
        "    from datasets import load_dataset, load_from_disk\n",
        "    from transformers import (\n",
        "        AutoTokenizer, AutoModelForCausalLM, Trainer,\n",
        "        TrainingArguments, DataCollatorForLanguageModeling,\n",
        "    )\n",
        "\n",
        "    # Local paths on shared PVC (pre-downloaded by notebook in disconnected mode)\n",
        "    model_local_path = \"/workspace/models/distilgpt2\"\n",
        "    dataset_local_path = \"/workspace/datasets/alpaca-cleaned\"\n",
        "    \n",
        "    # HuggingFace model/dataset names (for connected environments)\n",
        "    model_name = \"distilgpt2\"\n",
        "    dataset_name = \"yahma/alpaca-cleaned\"\n",
        "    \n",
        "    # Auto-detect: use local data if it exists\n",
        "    use_local_data = os.path.exists(os.path.join(model_local_path, \"config.json\"))\n",
        "\n",
        "    # ========== Auto-detect accelerator and configure ==========\n",
        "    # GPU_TYPE env var overrides auto-detection (set by test framework)\n",
        "    gpu_type = os.environ.get(\"GPU_TYPE\", \"\").lower()\n",
        "    force_cpu = gpu_type == \"cpu\"\n",
        "    \n",
        "    if force_cpu:\n",
        "        print(\"CPU mode forced via GPU_TYPE=cpu\")\n",
        "        device = torch.device(\"cpu\")\n",
        "        backend = \"gloo\"\n",
        "        use_fp16 = False\n",
        "        use_bf16 = False\n",
        "        accelerator = \"CPU (forced)\"\n",
        "    elif torch.cuda.is_available() and torch.cuda.device_count() > 0:\n",
        "        # NVIDIA GPU or AMD ROCm (ROCm exposes as CUDA)\n",
        "        device = torch.device(\"cuda\")\n",
        "        backend = \"nccl\"\n",
        "        use_fp16 = True\n",
        "        use_bf16 = False\n",
        "        # Check for bf16 support (Ampere+ or ROCm)\n",
        "        if torch.cuda.is_bf16_supported():\n",
        "            use_fp16 = False\n",
        "            use_bf16 = True\n",
        "        accelerator = f\"CUDA ({torch.cuda.get_device_name(0)})\"\n",
        "    elif hasattr(torch, \"xpu\") and torch.xpu.is_available():\n",
        "        # Intel XPU\n",
        "        device = torch.device(\"xpu\")\n",
        "        backend = \"ccl\"\n",
        "        use_fp16 = True\n",
        "        use_bf16 = False\n",
        "        accelerator = \"Intel XPU\"\n",
        "    else:\n",
        "        # CPU fallback\n",
        "        device = torch.device(\"cpu\")\n",
        "        backend = \"gloo\"\n",
        "        use_fp16 = False\n",
        "        use_bf16 = False\n",
        "        accelerator = \"CPU\"\n",
        "\n",
        "    print(f\"Detected accelerator: {accelerator}\")\n",
        "    print(f\"Using backend: {backend}, fp16={use_fp16}, bf16={use_bf16}\")\n",
        "\n",
        "    # ========== Initialize distributed ==========\n",
        "    dist.init_process_group(backend=backend)\n",
        "    rank = dist.get_rank()\n",
        "    world_size = dist.get_world_size()\n",
        "    local_rank = int(os.environ.get(\"LOCAL_RANK\", 0))\n",
        "    \n",
        "    print(f\"Distributed: WORLD_SIZE={world_size}, RANK={rank}, LOCAL_RANK={local_rank}\")\n",
        "\n",
        "    # Set device for this process (for multi-GPU)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.set_device(local_rank)\n",
        "\n",
        "    # ========== Load model and dataset ==========\n",
        "    if use_local_data:\n",
        "        # Local mode: load from shared PVC (pre-downloaded by notebook)\n",
        "        print(f\"Local mode: loading from shared PVC\")\n",
        "        print(f\"Loading model from: {model_local_path}\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_local_path)\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_local_path)\n",
        "        \n",
        "        print(f\"Loading dataset from: {dataset_local_path}\")\n",
        "        dataset = load_from_disk(dataset_local_path)\n",
        "        # load_from_disk returns DatasetDict, get train split\n",
        "        if hasattr(dataset, \"keys\") and \"train\" in dataset.keys():\n",
        "            dataset = dataset[\"train\"]\n",
        "        # Take only first 100 samples for testing\n",
        "        dataset = dataset.select(range(min(100, len(dataset))))\n",
        "    else:\n",
        "        # HuggingFace mode: download from internet\n",
        "        print(f\"HuggingFace mode: model={model_name}, dataset={dataset_name}\")\n",
        "        \n",
        "        if rank == 0:\n",
        "            print(f\"Downloading model from HuggingFace: {model_name}\")\n",
        "            tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "            model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "            print(\"Model downloaded\")\n",
        "        dist.barrier()\n",
        "        \n",
        "        if rank != 0:\n",
        "            tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "            model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "        \n",
        "        if rank == 0:\n",
        "            print(f\"Downloading dataset from HuggingFace: {dataset_name}\")\n",
        "            dataset = load_dataset(dataset_name, split=\"train[:100]\")\n",
        "            print(\"Dataset downloaded\")\n",
        "        dist.barrier()\n",
        "        \n",
        "        if rank != 0:\n",
        "            dataset = load_dataset(dataset_name, split=\"train[:100]\")\n",
        "    \n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    \n",
        "    dataset = dataset.train_test_split(test_size=0.2, shuffle=False)\n",
        "    train_ds = dataset[\"train\"]\n",
        "    eval_ds = dataset[\"test\"]\n",
        "\n",
        "    def tokenize_function(examples):\n",
        "        texts = [f\"### Instruction:\\n{i}\\n\\n### Response:\\n{o}\"\n",
        "                 for i, o in zip(examples[\"instruction\"], examples[\"output\"])]\n",
        "        return tokenizer(texts, padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "    train_tokenized = train_ds.map(tokenize_function, batched=True, remove_columns=train_ds.column_names)\n",
        "    eval_tokenized = eval_ds.map(tokenize_function, batched=True, remove_columns=eval_ds.column_names)\n",
        "\n",
        "    # ========== Configure training arguments based on accelerator ==========\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"/workspace/checkpoints\",\n",
        "        num_train_epochs=5,\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=2,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"no\",\n",
        "        logging_steps=5,\n",
        "        report_to=\"none\",\n",
        "        # Auto-configured based on accelerator\n",
        "        fp16=use_fp16,\n",
        "        bf16=use_bf16,\n",
        "        # Only enable gradient checkpointing on GPU (saves VRAM)\n",
        "        gradient_checkpointing=torch.cuda.is_available(),\n",
        "        # Disable pin_memory on CPU to avoid warning\n",
        "        dataloader_pin_memory=torch.cuda.is_available(),\n",
        "        # Fix for distributed checkpoint loading\n",
        "        save_safetensors=True,  # Avoid pickle device mapping issues\n",
        "        save_only_model=True,   # Skip optimizer state (avoids CPU device tag issue)\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_tokenized,\n",
        "        eval_dataset=eval_tokenized,\n",
        "        data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
        "    )\n",
        "    \n",
        "    print(f\"Starting training on {accelerator}...\")\n",
        "    trainer.train()\n",
        "\n",
        "    dist.barrier()\n",
        "    if rank == 0:\n",
        "        print(\"Training is finished\")\n",
        "    dist.destroy_process_group()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import warnings\n",
        "import urllib3\n",
        "\n",
        "# Suppress SSL warnings for self-signed certs in disconnected environments\n",
        "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
        "warnings.filterwarnings(\"ignore\", message=\".*Unverified HTTPS.*\")\n",
        "warnings.filterwarnings(\"ignore\", category=urllib3.exceptions.InsecureRequestWarning)\n",
        "\n",
        "from kubernetes import client as k8s_client\n",
        "from kubeflow.trainer import TrainerClient\n",
        "from kubeflow.common.types import KubernetesBackendConfig\n",
        "\n",
        "openshift_api_url = os.getenv(\"OPENSHIFT_API_URL\", \"\")\n",
        "token = os.getenv(\"NOTEBOOK_TOKEN\", \"\")\n",
        "namespace = os.getenv(\"NOTEBOOK_NAMESPACE\", \"default\")\n",
        "shared_pvc_name = os.getenv(\"SHARED_PVC_NAME\", \"shared-pvc\")\n",
        "\n",
        "print(f\"API: {openshift_api_url}\")\n",
        "print(f\"Namespace: {namespace}\")\n",
        "print(f\"PVC: {shared_pvc_name}\")\n",
        "\n",
        "cfg = k8s_client.Configuration()\n",
        "cfg.host = openshift_api_url\n",
        "cfg.verify_ssl = False\n",
        "cfg.api_key = {\"authorization\": f\"Bearer {token}\"}\n",
        "\n",
        "api_client = k8s_client.ApiClient(cfg)\n",
        "\n",
        "backend_cfg = KubernetesBackendConfig(\n",
        "    client_configuration=api_client.configuration,\n",
        ")\n",
        "\n",
        "trainer_client = TrainerClient(backend_cfg)\n",
        "print(\"TrainerClient initialized\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_runtime_name = os.getenv(\"TRAINING_RUNTIME\")\n",
        "if not training_runtime_name:\n",
        "    raise RuntimeError(\"TRAINING_RUNTIME environment variable is required\")\n",
        "\n",
        "torch_runtime = trainer_client.get_runtime(training_runtime_name)\n",
        "if torch_runtime is None:\n",
        "    raise RuntimeError(f\"Required runtime '{training_runtime_name}' not found\")\n",
        "print(f\"Got runtime: {torch_runtime.name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pre-download model/dataset from S3 to shared PVC (disconnected environments only)\n",
        "# Training pods will then load from these local paths\n",
        "#\n",
        "# NOTE: Shared PVC is mounted at different paths:\n",
        "#   - Notebook pod: /opt/app-root/src (we write here)\n",
        "#   - Training pods: /workspace (they read from here)\n",
        "# Same PVC, different mount points - data is shared!\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Clean up any leftover checkpoints from previous runs to avoid resume conflicts\n",
        "# (Notebook path = /opt/app-root/src, training pods see it as /workspace)\n",
        "checkpoints_path = \"/opt/app-root/src/checkpoints\"\n",
        "if os.path.exists(checkpoints_path):\n",
        "    print(f\"Cleaning up old checkpoints at {checkpoints_path}\")\n",
        "    shutil.rmtree(checkpoints_path)\n",
        "    print(\"  ✅ Old checkpoints removed\")\n",
        "\n",
        "s3_endpoint = os.getenv(\"AWS_DEFAULT_ENDPOINT\", \"\")\n",
        "s3_access_key = os.getenv(\"AWS_ACCESS_KEY_ID\", \"\")\n",
        "s3_secret_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\", \"\")\n",
        "s3_bucket = os.getenv(\"AWS_STORAGE_BUCKET\", \"\")\n",
        "model_s3_prefix = os.getenv(\"MODEL_S3_PREFIX\", \"models/distilgpt2\")\n",
        "dataset_s3_prefix = os.getenv(\"DATASET_S3_PREFIX\", \"alpaca-cleaned-datasets\")\n",
        "\n",
        "# Notebook writes to /opt/app-root/src (its PVC mount point)\n",
        "# Training pods will read from /workspace (their PVC mount point)\n",
        "# Same underlying storage!\n",
        "notebook_pvc_path = \"/opt/app-root/src\"\n",
        "model_local_path = f\"{notebook_pvc_path}/models/distilgpt2\"\n",
        "dataset_local_path = f\"{notebook_pvc_path}/datasets/alpaca-cleaned\"\n",
        "\n",
        "use_s3 = bool(s3_endpoint and s3_bucket and s3_access_key and s3_secret_key)\n",
        "\n",
        "if use_s3:\n",
        "    print(f\"S3 mode: downloading to shared PVC for training pods\")\n",
        "    print(f\"  Endpoint: {s3_endpoint}\")\n",
        "    print(f\"  Bucket: {s3_bucket}\")\n",
        "    \n",
        "    import boto3\n",
        "    from botocore.config import Config\n",
        "    from pathlib import Path\n",
        "    \n",
        "    config = Config(\n",
        "        signature_version=\"s3v4\",\n",
        "        s3={\"addressing_style\": \"path\"},\n",
        "    )\n",
        "    \n",
        "    endpoint_url = s3_endpoint if s3_endpoint.startswith(\"http\") else f\"https://{s3_endpoint}\"\n",
        "    s3_client = boto3.client(\n",
        "        \"s3\",\n",
        "        endpoint_url=endpoint_url,\n",
        "        aws_access_key_id=s3_access_key,\n",
        "        aws_secret_access_key=s3_secret_key,\n",
        "        config=config,\n",
        "        verify=False,\n",
        "    )\n",
        "    \n",
        "    def download_from_s3(s3_prefix: str, local_path: str):\n",
        "        \"\"\"Download files from S3/MinIO to local path.\"\"\"\n",
        "        print(f\"  Downloading s3://{s3_bucket}/{s3_prefix}/ -> {local_path}\")\n",
        "        Path(local_path).mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "        paginator = s3_client.get_paginator(\"list_objects_v2\")\n",
        "        count = 0\n",
        "        for page in paginator.paginate(Bucket=s3_bucket, Prefix=s3_prefix):\n",
        "            for obj in page.get(\"Contents\", []):\n",
        "                key = obj[\"Key\"]\n",
        "                rel_path = key[len(s3_prefix):].lstrip(\"/\")\n",
        "                if not rel_path:\n",
        "                    continue\n",
        "                local_file = os.path.join(local_path, rel_path)\n",
        "                os.makedirs(os.path.dirname(local_file), exist_ok=True)\n",
        "                s3_client.download_file(s3_bucket, key, local_file)\n",
        "                count += 1\n",
        "        print(f\"  ✅ Downloaded {count} files to {local_path}\")\n",
        "    \n",
        "    # Download model if not already present\n",
        "    if not os.path.exists(os.path.join(model_local_path, \"config.json\")):\n",
        "        download_from_s3(model_s3_prefix, model_local_path)\n",
        "    else:\n",
        "        print(f\"  Model already exists at {model_local_path}, skipping\")\n",
        "    \n",
        "    # Download dataset if not already present\n",
        "    if not os.path.exists(os.path.join(dataset_local_path, \"dataset_dict.json\")):\n",
        "        download_from_s3(dataset_s3_prefix, dataset_local_path)\n",
        "    else:\n",
        "        print(f\"  Dataset already exists at {dataset_local_path}, skipping\")\n",
        "    \n",
        "    print(\"✅ S3 download complete - training pods will load from shared PVC\")\n",
        "else:\n",
        "    print(\"HuggingFace mode: training pods will download directly from HF Hub\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from kubeflow.trainer.rhai.transformers import TransformersTrainer\n",
        "from kubeflow.trainer.options import PodTemplateOverrides, PodTemplateOverride, PodSpecOverride, ContainerOverride\n",
        "import os\n",
        "\n",
        "# Read feature flags from environment\n",
        "enable_progression = os.getenv(\"ENABLE_PROGRESSION_TRACKING\", \"true\").lower() == \"true\"\n",
        "enable_checkpoint = os.getenv(\"ENABLE_JIT_CHECKPOINT\", \"false\").lower() == \"true\"\n",
        "checkpoint_output_dir = os.getenv(\"CHECKPOINT_OUTPUT_DIR\", \"/workspace/checkpoints\")\n",
        "checkpoint_save_strategy = os.getenv(\"CHECKPOINT_SAVE_STRATEGY\", \"epoch\")\n",
        "checkpoint_save_total_limit = int(os.getenv(\"CHECKPOINT_SAVE_TOTAL_LIMIT\", \"3\"))\n",
        "\n",
        "print(f\"Progression tracking: {enable_progression}\")\n",
        "print(f\"JIT Checkpoint: {enable_checkpoint}\")\n",
        "\n",
        "\n",
        "\n",
        "# Read GPU and multi-node config from environment (passed from Go test)\n",
        "gpu_resource_label = os.environ.get(\"GPU_RESOURCE_LABEL\", \"\")\n",
        "gpu_type = os.environ.get(\"GPU_TYPE\", \"cpu\")\n",
        "num_nodes = int(os.environ.get(\"NUM_NODES\", \"2\"))\n",
        "num_gpus_per_node = int(os.environ.get(\"NUM_GPUS_PER_NODE\", \"1\"))\n",
        "\n",
        "print(f\"Training config: num_nodes={num_nodes}, num_gpus_per_node={num_gpus_per_node}, gpu_type={gpu_type}\")\n",
        "\n",
        "# Configure resources - GPU label tells k8s to schedule on GPU node\n",
        "if gpu_resource_label:\n",
        "    resources_per_node = {\n",
        "        \"cpu\": 2, \n",
        "        \"memory\": \"8Gi\",\n",
        "        gpu_resource_label: num_gpus_per_node  # e.g., \"nvidia.com/gpu\": 2\n",
        "    }\n",
        "    print(f\"GPU mode: requesting {gpu_resource_label}: {num_gpus_per_node}\")\n",
        "else:\n",
        "    resources_per_node = {\"cpu\": 2, \"memory\": \"8Gi\"}\n",
        "    print(\"CPU mode: no GPU requested\")\n",
        "\n",
        "# Build env vars to pass to training pods (GPU_TYPE tells train_bloom() to force CPU mode)\n",
        "training_env = {\"GPU_TYPE\": gpu_type}\n",
        "\n",
        "# Build trainer config\n",
        "trainer_kwargs = {\n",
        "    \"func\": train_bloom,\n",
        "    \"num_nodes\": num_nodes,\n",
        "    \"resources_per_node\": resources_per_node,\n",
        "    \"env\": training_env,\n",
        "}\n",
        "\n",
        "# Add progression tracking config (must explicitly set to False to disable SDK default)\n",
        "trainer_kwargs[\"enable_progression_tracking\"] = enable_progression\n",
        "if enable_progression:\n",
        "    trainer_kwargs[\"metrics_port\"] = 28080\n",
        "    trainer_kwargs[\"metrics_poll_interval_seconds\"] = 8\n",
        "\n",
        "# Add checkpointing if enabled\n",
        "if enable_checkpoint:\n",
        "    from kubeflow.trainer.rhai.transformers import PeriodicCheckpointConfig\n",
        "    trainer_kwargs[\"enable_jit_checkpoint\"] = True\n",
        "    trainer_kwargs[\"output_dir\"] = checkpoint_output_dir\n",
        "    trainer_kwargs[\"periodic_checkpoint_config\"] = PeriodicCheckpointConfig(\n",
        "        save_strategy=checkpoint_save_strategy,\n",
        "        save_total_limit=checkpoint_save_total_limit\n",
        "    )\n",
        "\n",
        "job_name = trainer_client.train(\n",
        "    trainer=TransformersTrainer(**trainer_kwargs),\n",
        "    runtime=torch_runtime,\n",
        "    options=[\n",
        "        PodTemplateOverrides(\n",
        "            PodTemplateOverride(\n",
        "                target_jobs=[\"node\"],\n",
        "                spec=PodSpecOverride(\n",
        "                    volumes=[{\"name\": \"workspace\", \"persistentVolumeClaim\": {\"claimName\": shared_pvc_name}}],\n",
        "                    containers=[ContainerOverride(\n",
        "                        name=\"node\",\n",
        "                        volume_mounts=[{\"name\": \"workspace\", \"mountPath\": \"/workspace\"}]\n",
        "                    )]\n",
        "                )\n",
        "            )\n",
        "        )\n",
        "    ]\n",
        ")\n",
        "print(f\"TRAINJOB_NAME: {job_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Wait for job completion\n",
        "trainer_client.wait_for_job_status(name=job_name, status={\"Running\"}, timeout=600)\n",
        "trainer_client.wait_for_job_status(name=job_name, status={\"Complete\", \"Failed\"}, timeout=1200)\n",
        "\n",
        "job = trainer_client.get_job(name=job_name)\n",
        "print(f\"Training job final status: {job.status}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print job steps\n",
        "for step in trainer_client.get_job(name=job_name).steps:\n",
        "    print(f\"Step: {step.name}, Status: {step.status}, Devices: {step.device} x {step.device_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print job logs\n",
        "for logline in trainer_client.get_job_logs(job_name, follow=False):\n",
        "    print(logline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Notebook completed - Go test handles assertions\n",
        "print(\"NOTEBOOK_STATUS: SUCCESS\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
