{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RHAI Features Test - TrainJob Submission\n",
        "\n",
        "This notebook submits a TrainJob with RHAI features (progression tracking).\n",
        "Uses shared PVC for HuggingFace cache across distributed training nodes.\n",
        "Assertions are handled in the Go test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_bloom():\n",
        "    import os\n",
        "    os.environ[\"HF_HOME\"] = \"/workspace/hf_cache\"\n",
        "    \n",
        "    import torch\n",
        "    import torch.distributed as dist\n",
        "    from datasets import load_dataset\n",
        "    from transformers import (\n",
        "        AutoTokenizer, AutoModelForCausalLM, Trainer,\n",
        "        TrainingArguments, DataCollatorForLanguageModeling,\n",
        "    )\n",
        "\n",
        "    # ========== Auto-detect accelerator and configure ==========\n",
        "    if torch.cuda.is_available():\n",
        "        # NVIDIA GPU or AMD ROCm (ROCm exposes as CUDA)\n",
        "        device = torch.device(\"cuda\")\n",
        "        backend = \"nccl\"\n",
        "        use_fp16 = True\n",
        "        use_bf16 = False\n",
        "        # Check for bf16 support (Ampere+ or ROCm)\n",
        "        if torch.cuda.is_bf16_supported():\n",
        "            use_fp16 = False\n",
        "            use_bf16 = True\n",
        "        accelerator = f\"CUDA ({torch.cuda.get_device_name(0)})\"\n",
        "    elif hasattr(torch, \"xpu\") and torch.xpu.is_available():\n",
        "        # Intel XPU\n",
        "        device = torch.device(\"xpu\")\n",
        "        backend = \"ccl\"\n",
        "        use_fp16 = True\n",
        "        use_bf16 = False\n",
        "        accelerator = \"Intel XPU\"\n",
        "    else:\n",
        "        # CPU fallback\n",
        "        device = torch.device(\"cpu\")\n",
        "        backend = \"gloo\"\n",
        "        use_fp16 = False\n",
        "        use_bf16 = False\n",
        "        accelerator = \"CPU\"\n",
        "\n",
        "    print(f\"Detected accelerator: {accelerator}\")\n",
        "    print(f\"Using backend: {backend}, fp16={use_fp16}, bf16={use_bf16}\")\n",
        "\n",
        "    # ========== Initialize distributed ==========\n",
        "    dist.init_process_group(backend=backend)\n",
        "    rank = dist.get_rank()\n",
        "    world_size = dist.get_world_size()\n",
        "    local_rank = int(os.environ.get(\"LOCAL_RANK\", 0))\n",
        "    \n",
        "    print(f\"Distributed: WORLD_SIZE={world_size}, RANK={rank}, LOCAL_RANK={local_rank}\")\n",
        "\n",
        "    # Set device for this process (for multi-GPU)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.set_device(local_rank)\n",
        "\n",
        "    # ========== Load model and tokenizer ==========\n",
        "    model_name = \"distilgpt2\"\n",
        "    \n",
        "    if rank == 0:\n",
        "        print(f\"Downloading model: {model_name}\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "        print(\"Model downloaded\")\n",
        "    dist.barrier()\n",
        "    \n",
        "    if rank != 0:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "    \n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # ========== Load and prepare dataset ==========\n",
        "    dataset_name = \"yahma/alpaca-cleaned\"\n",
        "    \n",
        "    if rank == 0:\n",
        "        print(f\"Downloading dataset: {dataset_name}\")\n",
        "        dataset = load_dataset(dataset_name, split=\"train[:100]\")\n",
        "        print(\"Dataset downloaded\")\n",
        "    dist.barrier()\n",
        "    \n",
        "    if rank != 0:\n",
        "        dataset = load_dataset(dataset_name, split=\"train[:100]\")\n",
        "    \n",
        "    dataset = dataset.train_test_split(test_size=0.2, shuffle=False)\n",
        "    train_ds = dataset[\"train\"]\n",
        "    eval_ds = dataset[\"test\"]\n",
        "\n",
        "    def tokenize_function(examples):\n",
        "        texts = [f\"### Instruction:\\n{i}\\n\\n### Response:\\n{o}\"\n",
        "                 for i, o in zip(examples[\"instruction\"], examples[\"output\"])]\n",
        "        return tokenizer(texts, padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "    train_tokenized = train_ds.map(tokenize_function, batched=True, remove_columns=train_ds.column_names)\n",
        "    eval_tokenized = eval_ds.map(tokenize_function, batched=True, remove_columns=eval_ds.column_names)\n",
        "\n",
        "    # ========== Configure training arguments based on accelerator ==========\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"/workspace/checkpoints\",\n",
        "        num_train_epochs=5,\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=2,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"no\",\n",
        "        logging_steps=5,\n",
        "        report_to=\"none\",\n",
        "        # Auto-configured based on accelerator\n",
        "        fp16=use_fp16,\n",
        "        bf16=use_bf16,\n",
        "        # Only enable gradient checkpointing on GPU (saves VRAM)\n",
        "        gradient_checkpointing=torch.cuda.is_available(),\n",
        "        # Disable pin_memory on CPU to avoid warning\n",
        "        dataloader_pin_memory=torch.cuda.is_available(),\n",
        "        # Fix for distributed checkpoint loading\n",
        "        save_safetensors=True,  # Avoid pickle device mapping issues\n",
        "        save_only_model=True,   # Skip optimizer state (avoids CPU device tag issue)\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_tokenized,\n",
        "        eval_dataset=eval_tokenized,\n",
        "        data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
        "    )\n",
        "    \n",
        "    print(f\"Starting training on {accelerator}...\")\n",
        "    trainer.train()\n",
        "\n",
        "    dist.barrier()\n",
        "    if rank == 0:\n",
        "        print(\"Training is finished\")\n",
        "    dist.destroy_process_group()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from kubernetes import client as k8s_client\n",
        "from kubeflow.trainer import TrainerClient\n",
        "from kubeflow.common.types import KubernetesBackendConfig\n",
        "\n",
        "openshift_api_url = os.getenv(\"OPENSHIFT_API_URL\", \"\")\n",
        "token = os.getenv(\"NOTEBOOK_TOKEN\", \"\")\n",
        "namespace = os.getenv(\"NOTEBOOK_NAMESPACE\", \"default\")\n",
        "shared_pvc_name = os.getenv(\"SHARED_PVC_NAME\", \"shared-pvc\")\n",
        "\n",
        "print(f\"API: {openshift_api_url}\")\n",
        "print(f\"Namespace: {namespace}\")\n",
        "print(f\"PVC: {shared_pvc_name}\")\n",
        "\n",
        "cfg = k8s_client.Configuration()\n",
        "cfg.host = openshift_api_url\n",
        "cfg.verify_ssl = False\n",
        "cfg.api_key = {\"authorization\": f\"Bearer {token}\"}\n",
        "\n",
        "api_client = k8s_client.ApiClient(cfg)\n",
        "\n",
        "backend_cfg = KubernetesBackendConfig(\n",
        "    client_configuration=api_client.configuration,\n",
        ")\n",
        "\n",
        "trainer_client = TrainerClient(backend_cfg)\n",
        "print(\"TrainerClient initialized\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_runtime_name = os.getenv(\"TRAINING_RUNTIME\", \"torch-distributed\")\n",
        "torch_runtime = trainer_client.get_runtime(training_runtime_name)\n",
        "print(f\"Got runtime: {torch_runtime.name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from kubeflow.trainer.rhai.transformers import TransformersTrainer\n",
        "from kubeflow.trainer.options import PodTemplateOverrides, PodTemplateOverride, PodSpecOverride, ContainerOverride\n",
        "import os\n",
        "\n",
        "# Read feature flags from environment\n",
        "enable_progression = os.getenv(\"ENABLE_PROGRESSION_TRACKING\", \"true\").lower() == \"true\"\n",
        "enable_checkpoint = os.getenv(\"ENABLE_JIT_CHECKPOINT\", \"false\").lower() == \"true\"\n",
        "checkpoint_output_dir = os.getenv(\"CHECKPOINT_OUTPUT_DIR\", \"/workspace/checkpoints\")\n",
        "checkpoint_save_strategy = os.getenv(\"CHECKPOINT_SAVE_STRATEGY\", \"epoch\")\n",
        "checkpoint_save_total_limit = int(os.getenv(\"CHECKPOINT_SAVE_TOTAL_LIMIT\", \"3\"))\n",
        "\n",
        "print(f\"Progression tracking: {enable_progression}\")\n",
        "print(f\"JIT Checkpoint: {enable_checkpoint}\")\n",
        "\n",
        "# Read GPU config from environment (passed from Go test)\n",
        "gpu_resource_label = os.environ.get(\"GPU_RESOURCE_LABEL\", \"\")\n",
        "\n",
        "# Configure resources - GPU label tells k8s to schedule on GPU node\n",
        "if gpu_resource_label:\n",
        "    resources_per_node = {\n",
        "        \"cpu\": 2, \n",
        "        \"memory\": \"8Gi\",\n",
        "        gpu_resource_label: 1  # e.g., \"nvidia.com/gpu\": 1, \"amd.com/gpu\": 1\n",
        "    }\n",
        "    print(f\"GPU mode: requesting {gpu_resource_label}: 1\")\n",
        "else:\n",
        "    resources_per_node = {\"cpu\": 2, \"memory\": \"8Gi\"}\n",
        "    print(\"CPU mode: no GPU requested\")\n",
        "\n",
        "# Build trainer config\n",
        "trainer_kwargs = {\n",
        "    \"func\": train_bloom,\n",
        "    \"func_args\": {},\n",
        "    \"num_nodes\": 2,\n",
        "    \"resources_per_node\": resources_per_node, \n",
        "}\n",
        "\n",
        "# Add progression tracking config (must explicitly set to False to disable SDK default)\n",
        "trainer_kwargs[\"enable_progression_tracking\"] = enable_progression\n",
        "if enable_progression:\n",
        "    trainer_kwargs[\"metrics_port\"] = 28080\n",
        "    trainer_kwargs[\"metrics_poll_interval_seconds\"] = 8\n",
        "\n",
        "# Add checkpointing if enabled\n",
        "if enable_checkpoint:\n",
        "    from kubeflow.trainer.rhai.transformers import PeriodicCheckpointConfig\n",
        "    trainer_kwargs[\"enable_jit_checkpoint\"] = True\n",
        "    trainer_kwargs[\"output_dir\"] = checkpoint_output_dir\n",
        "    trainer_kwargs[\"periodic_checkpoint_config\"] = PeriodicCheckpointConfig(\n",
        "        save_strategy=checkpoint_save_strategy,\n",
        "        save_total_limit=checkpoint_save_total_limit\n",
        "    )\n",
        "\n",
        "job_name = trainer_client.train(\n",
        "    trainer=TransformersTrainer(**trainer_kwargs),\n",
        "    runtime=torch_runtime,\n",
        "    options=[\n",
        "        PodTemplateOverrides(\n",
        "            PodTemplateOverride(\n",
        "                target_jobs=[\"node\"],\n",
        "                spec=PodSpecOverride(\n",
        "                    volumes=[{\"name\": \"workspace\", \"persistentVolumeClaim\": {\"claimName\": shared_pvc_name}}],\n",
        "                    containers=[ContainerOverride(\n",
        "                        name=\"node\",\n",
        "                        volume_mounts=[{\"name\": \"workspace\", \"mountPath\": \"/workspace\"}]\n",
        "                    )]\n",
        "                )\n",
        "            )\n",
        "        )\n",
        "    ]\n",
        ")\n",
        "print(f\"TRAINJOB_NAME: {job_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Wait for job completion\n",
        "trainer_client.wait_for_job_status(name=job_name, status={\"Running\"}, timeout=600)\n",
        "trainer_client.wait_for_job_status(name=job_name, status={\"Complete\", \"Failed\"}, timeout=1200)\n",
        "\n",
        "job = trainer_client.get_job(name=job_name)\n",
        "print(f\"Training job final status: {job.status}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print job steps\n",
        "for step in trainer_client.get_job(name=job_name).steps:\n",
        "    print(f\"Step: {step.name}, Status: {step.status}, Devices: {step.device} x {step.device_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print job logs\n",
        "for logline in trainer_client.get_job_logs(job_name, follow=False):\n",
        "    print(logline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Notebook completed - Go test handles assertions\n",
        "print(\"NOTEBOOK_STATUS: SUCCESS\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
