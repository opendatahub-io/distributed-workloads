{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T13:19:46.917723Z",
     "iopub.status.busy": "2025-09-03T13:19:46.917308Z",
     "iopub.status.idle": "2025-09-03T13:19:46.935181Z",
     "shell.execute_reply": "2025-09-03T13:19:46.934697Z",
     "shell.execute_reply.started": "2025-09-03T13:19:46.917698Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_fashion_mnist():\n",
    "    import os\n",
    "\n",
    "    import torch\n",
    "    import torch.distributed as dist\n",
    "    import torch.nn.functional as F\n",
    "    from torch import nn\n",
    "    from torch.utils.data import DataLoader, DistributedSampler, Dataset\n",
    "    import numpy as np\n",
    "    import struct\n",
    "\n",
    "    # Define the PyTorch CNN model to be trained\n",
    "    class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Net, self).__init__()\n",
    "            self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "            self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "            self.fc1 = nn.Linear(4 * 4 * 50, 500)\n",
    "            self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.conv1(x))\n",
    "            x = F.max_pool2d(x, 2, 2)\n",
    "            x = F.relu(self.conv2(x))\n",
    "            x = F.max_pool2d(x, 2, 2)\n",
    "            x = x.view(-1, 4 * 4 * 50)\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = self.fc2(x)\n",
    "            return F.log_softmax(x, dim=1)\n",
    "\n",
    "    # Force CPU-only for this test to avoid accidental NCCL/GPU usage\n",
    "    backend = \"gloo\"\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(f\"Using Device: cpu, Backend: {backend}\")\n",
    "\n",
    "    # Setup PyTorch distributed.\n",
    "    local_rank = int(os.getenv(\"LOCAL_RANK\") or os.getenv(\"PET_NODE_RANK\") or 0)\n",
    "    dist.init_process_group(backend=backend)\n",
    "    print(\n",
    "        \"Distributed Training for WORLD_SIZE: {}, RANK: {}, LOCAL_RANK: {}\".format(\n",
    "            dist.get_world_size(),\n",
    "            dist.get_rank(),\n",
    "            local_rank,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Create the model and load it into the device.\n",
    "    model = nn.parallel.DistributedDataParallel(Net().to(device))\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "\n",
    "    # Prefer shared PVC if present; else fallback to internet download (rank 0 only)\n",
    "    from urllib.parse import urlparse\n",
    "    import gzip, shutil\n",
    "\n",
    "    pvc_root = \"/mnt/shared\"\n",
    "    pvc_raw = os.path.join(pvc_root, \"FashionMNIST\", \"raw\")\n",
    "\n",
    "    use_pvc = os.path.isdir(pvc_raw) and any(os.scandir(pvc_raw))\n",
    "\n",
    "    if not use_pvc:\n",
    "        raise RuntimeError(\"Shared PVC not mounted or empty at /mnt/shared/FashionMNIST/raw; this test requires a pre-populated RWX PVC\")\n",
    "\n",
    "    print(\"Using dataset from shared PVC at /mnt/shared\")\n",
    "\n",
    "    def _read_idx_images(path):\n",
    "        with open(path, \"rb\") as f:\n",
    "            magic, num, rows, cols = struct.unpack(\">IIII\", f.read(16))\n",
    "            if magic != 2051:\n",
    "                raise RuntimeError(f\"Unexpected images magic: {magic}\")\n",
    "            data = f.read()\n",
    "        return np.frombuffer(data, dtype=np.uint8).reshape(num, rows, cols)\n",
    "\n",
    "    def _read_idx_labels(path):\n",
    "        with open(path, \"rb\") as f:\n",
    "            magic, num = struct.unpack(\">II\", f.read(8))\n",
    "            if magic != 2049:\n",
    "                raise RuntimeError(f\"Unexpected labels magic: {magic}\")\n",
    "            data = f.read()\n",
    "        return np.frombuffer(data, dtype=np.uint8)\n",
    "\n",
    "    class MnistIdxDataset(Dataset):\n",
    "        def __init__(self, images_path: str, labels_path: str):\n",
    "            self.images = _read_idx_images(images_path)\n",
    "            self.labels = _read_idx_labels(labels_path)\n",
    "            if len(self.images) != len(self.labels):\n",
    "                raise RuntimeError(\"Images and labels count mismatch\")\n",
    "        def __len__(self):\n",
    "            return len(self.labels)\n",
    "        def __getitem__(self, idx: int):\n",
    "            import torch as _torch\n",
    "            img = _torch.from_numpy(self.images[idx][None, ...].astype(\"float32\") / 255.0)\n",
    "            label = int(self.labels[idx])\n",
    "            return img, label\n",
    "\n",
    "    images_path = os.path.join(pvc_root, \"FashionMNIST\", \"raw\", \"train-images-idx3-ubyte\")\n",
    "    labels_path = os.path.join(pvc_root, \"FashionMNIST\", \"raw\", \"train-labels-idx1-ubyte\")\n",
    "\n",
    "    dataset = MnistIdxDataset(images_path, labels_path)\n",
    "    train_loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=100,\n",
    "        sampler=DistributedSampler(dataset)\n",
    "    )\n",
    "\n",
    "    dist.barrier()\n",
    "    for epoch in range(1, 3):\n",
    "        model.train()\n",
    "\n",
    "        # Iterate over mini-batches from the training set\n",
    "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "            # Move the data to the selected device\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = F.nll_loss(outputs, labels)\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx % 10 == 0 and dist.get_rank() == 0:\n",
    "                print(\n",
    "                    \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                        epoch,\n",
    "                        batch_idx * len(inputs),\n",
    "                        len(train_loader.dataset),\n",
    "                        100.0 * batch_idx / len(train_loader),\n",
    "                        loss.item(),\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    # Wait for the distributed training to complete\n",
    "    dist.barrier()\n",
    "    if dist.get_rank() == 0:\n",
    "        print(\"Training is finished\")\n",
    "\n",
    "    # Finally clean up PyTorch distributed\n",
    "    dist.destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import shutil\n",
    "import socket\n",
    "import time\n",
    "\n",
    "import boto3\n",
    "from botocore.config import Config as BotoConfig\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# --- Global networking safety net: cap all socket operations ---\n",
    "# Any blocking socket operation (like reading S3 object data) will raise\n",
    "# socket.timeout after this many seconds instead of hanging forever.\n",
    "socket.setdefaulttimeout(10)  # seconds\n",
    "\n",
    "# Notebook's PVC mount path (per Notebook CR). Training pods will mount the same PVC at /mnt/shared\n",
    "PVC_NOTEBOOK_PATH = \"/opt/app-root/src\"\n",
    "DATASET_ROOT_NOTEBOOK = PVC_NOTEBOOK_PATH  # place FashionMNIST under this root\n",
    "FASHION_RAW_DIR = os.path.join(DATASET_ROOT_NOTEBOOK, \"FashionMNIST\", \"raw\")\n",
    "os.makedirs(FASHION_RAW_DIR, exist_ok=True)\n",
    "\n",
    "# Env config for S3/MinIO\n",
    "s3_endpoint = os.getenv(\"AWS_DEFAULT_ENDPOINT\", \"\")\n",
    "s3_access_key = os.getenv(\"AWS_ACCESS_KEY_ID\", \"\")\n",
    "s3_secret_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\", \"\")\n",
    "s3_bucket = os.getenv(\"AWS_STORAGE_BUCKET\", \"\")\n",
    "s3_prefix = os.getenv(\"AWS_STORAGE_BUCKET_MNIST_DIR\", \"\")  # e.g. \"data\"\n",
    "\n",
    "def stream_download(s3, bucket, key, dst):\n",
    "    \"\"\"\n",
    "    Download an object from S3/MinIO using get_object and streaming reads.\n",
    "    This bypasses boto3's TransferManager / download_file entirely.\n",
    "    Returns True on success, False on any error (including timeouts).\n",
    "    \"\"\"\n",
    "    print(f\"[notebook] STREAM download s3://{bucket}/{key} -> {dst}\")\n",
    "    t0 = time.time()\n",
    "\n",
    "    try:\n",
    "        # Metadata / headers fetch — should be quick or fail clearly\n",
    "        resp = s3.get_object(Bucket=bucket, Key=key)\n",
    "    except ClientError as e:\n",
    "        err = e.response.get(\"Error\", {})\n",
    "        print(f\"[notebook] CLIENT ERROR (get_object) for {key}: {err}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"[notebook] OTHER ERROR (get_object) for {key}: {e}\")\n",
    "        return False\n",
    "\n",
    "    body = resp[\"Body\"]\n",
    "    try:\n",
    "        with open(dst, \"wb\") as f:\n",
    "            while True:\n",
    "                try:\n",
    "                    # Each read is bounded by socket.setdefaulttimeout(...)\n",
    "                    chunk = body.read(1024 * 1024)  # 1MB per chunk\n",
    "                except socket.timeout as e:\n",
    "                    print(f\"[notebook] socket.timeout while reading {key}: {e}\")\n",
    "                    return False\n",
    "                if not chunk:\n",
    "                    break\n",
    "                f.write(chunk)\n",
    "    except Exception as e:\n",
    "        print(f\"[notebook] ERROR writing to {dst} for {key}: {e}\")\n",
    "        return False\n",
    "\n",
    "    t1 = time.time()\n",
    "    print(f\"[notebook] DONE  stream {key} in {t1 - t0:.2f}s\")\n",
    "    return True\n",
    "\n",
    "\n",
    "if s3_endpoint and s3_bucket:\n",
    "    try:\n",
    "        # Normalize endpoint URL\n",
    "        endpoint_url = (\n",
    "            s3_endpoint\n",
    "            if s3_endpoint.startswith(\"http\")\n",
    "            else f\"https://{s3_endpoint}\"\n",
    "        )\n",
    "        prefix = (s3_prefix or \"\").strip(\"/\")\n",
    "\n",
    "        print(\n",
    "            f\"S3 configured (boto3, notebook): \"\n",
    "            f\"endpoint={endpoint_url}, bucket={s3_bucket}, prefix={prefix or '<root>'}\"\n",
    "        )\n",
    "\n",
    "        # Boto config: single attempt, reasonable connect/read timeouts.\n",
    "        boto_cfg = BotoConfig(\n",
    "            signature_version=\"s3v4\",\n",
    "            s3={\"addressing_style\": \"path\"},\n",
    "            retries={\"max_attempts\": 1, \"mode\": \"standard\"},\n",
    "            connect_timeout=5,\n",
    "            read_timeout=10,\n",
    "        )\n",
    "\n",
    "        # Create S3/MinIO client\n",
    "        s3 = boto3.client(\n",
    "            \"s3\",\n",
    "            endpoint_url=endpoint_url,\n",
    "            aws_access_key_id=s3_access_key,\n",
    "            aws_secret_access_key=s3_secret_key,\n",
    "            config=boto_cfg,\n",
    "            verify=False,\n",
    "        )\n",
    "\n",
    "        # Optional: quick debug HEAD of the problematic key\n",
    "        # (will just log if there's an access or existence problem)\n",
    "        test_key = \"data/t10k-labels-idx1-ubyte.gz\"\n",
    "        try:\n",
    "            print(f\"[debug] HEAD s3://{s3_bucket}/{test_key}\")\n",
    "            meta = s3.head_object(Bucket=s3_bucket, Key=test_key)\n",
    "            print(f\"[debug] HEAD OK: size={meta.get('ContentLength')}\")\n",
    "        except ClientError as e:\n",
    "            print(f\"[debug] HEAD ERROR for {test_key}: {e.response.get('Error')}\")\n",
    "\n",
    "        # List and download all objects under the prefix\n",
    "        paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "        pulled_any = False\n",
    "\n",
    "        for page in paginator.paginate(Bucket=s3_bucket, Prefix=prefix or \"\"):\n",
    "            contents = page.get(\"Contents\", [])\n",
    "            if not contents:\n",
    "                continue\n",
    "\n",
    "            for obj in contents:\n",
    "                key = obj[\"Key\"]\n",
    "\n",
    "                # Skip \"directory markers\"\n",
    "                if key.endswith(\"/\"):\n",
    "                    continue\n",
    "\n",
    "                # Determine relative path under prefix for local storage\n",
    "                rel = key[len(prefix):].lstrip(\"/\") if prefix else key\n",
    "                dst = os.path.join(FASHION_RAW_DIR, rel)\n",
    "                os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
    "\n",
    "                # Download only if missing\n",
    "                if not os.path.exists(dst):\n",
    "                    ok = stream_download(s3, s3_bucket, key, dst)\n",
    "                    if not ok:\n",
    "                        # Skip decompression and move on to next object\n",
    "                        continue\n",
    "                else:\n",
    "                    print(f\"[notebook] Skipping existing file {dst}\")\n",
    "\n",
    "                # If the file is .gz, decompress and remove the .gz\n",
    "                if dst.endswith(\".gz\") and os.path.exists(dst):\n",
    "                    out_path = os.path.splitext(dst)[0]\n",
    "                    if not os.path.exists(out_path):\n",
    "                        print(f\"[notebook] Decompressing {dst} -> {out_path}\")\n",
    "                        try:\n",
    "                            with gzip.open(dst, \"rb\") as f_in, open(out_path, \"wb\") as f_out:\n",
    "                                shutil.copyfileobj(f_in, f_out)\n",
    "                        except Exception as e:\n",
    "                            print(f\"[notebook] Failed to decompress {dst}: {e}\")\n",
    "                        else:\n",
    "                            try:\n",
    "                                os.remove(dst)\n",
    "                            except Exception:\n",
    "                                # Not critical if we can't delete the gzip\n",
    "                                pass\n",
    "\n",
    "                pulled_any = True\n",
    "\n",
    "        print(f\"[notebook] S3 pulled_any={pulled_any}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[notebook] S3 fetch failed: {e}\")\n",
    "else:\n",
    "    print(\"[notebook] S3 not configured: missing endpoint or bucket env vars\")\n",
    "\n",
    "# Check if we have data; if not, try downloading from internet\n",
    "files_needed = [\n",
    "    \"train-images-idx3-ubyte\",\n",
    "    \"train-labels-idx1-ubyte\",\n",
    "    \"t10k-images-idx3-ubyte\",\n",
    "    \"t10k-labels-idx1-ubyte\",\n",
    "]\n",
    "files_present = all(os.path.exists(os.path.join(FASHION_RAW_DIR, f)) for f in files_needed)\n",
    "\n",
    "if not files_present:\n",
    "    print(\"[notebook] Dataset not complete, attempting internet download...\")\n",
    "    try:\n",
    "        import urllib.request\n",
    "        \n",
    "        base_url = \"http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/\"\n",
    "        gz_files = [\n",
    "            \"train-images-idx3-ubyte.gz\",\n",
    "            \"train-labels-idx1-ubyte.gz\",\n",
    "            \"t10k-images-idx3-ubyte.gz\",\n",
    "            \"t10k-labels-idx1-ubyte.gz\",\n",
    "        ]\n",
    "        \n",
    "        for gz_file in gz_files:\n",
    "            url = base_url + gz_file\n",
    "            dst_gz = os.path.join(FASHION_RAW_DIR, gz_file)\n",
    "            dst = os.path.splitext(dst_gz)[0]\n",
    "            \n",
    "            if os.path.exists(dst):\n",
    "                print(f\"[notebook] Already have {dst}, skipping download\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"[notebook] Downloading {url} ...\")\n",
    "            urllib.request.urlretrieve(url, dst_gz)\n",
    "            \n",
    "            print(f\"[notebook] Decompressing {dst_gz} ...\")\n",
    "            with gzip.open(dst_gz, \"rb\") as f_in, open(dst, \"wb\") as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "            \n",
    "            os.remove(dst_gz)\n",
    "            print(f\"[notebook] Done: {dst}\")\n",
    "        \n",
    "        print(\"[notebook] Internet download completed successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"[notebook] Internet download failed: {e}\")\n",
    "        raise RuntimeError(\"Internet download failed; aborting test\")\n",
    "else:\n",
    "    print(\"[notebook] Dataset files already present, skipping download\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T13:19:49.832393Z",
     "iopub.status.busy": "2025-09-03T13:19:49.832117Z",
     "iopub.status.idle": "2025-09-03T13:19:51.924613Z",
     "shell.execute_reply": "2025-09-03T13:19:51.924264Z",
     "shell.execute_reply.started": "2025-09-03T13:19:49.832371Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Init SDK client with user token/API URL (no Backend types import)\n",
    "import os\n",
    "from kubernetes import client as k8s_client\n",
    "from kubeflow.trainer import TrainerClient\n",
    "from kubeflow.common.types import KubernetesBackendConfig\n",
    "\n",
    "openshift_api_url = os.getenv(\"OPENSHIFT_API_URL\", \"\")\n",
    "token = os.getenv(\"NOTEBOOK_TOKEN\", \"\")\n",
    "\n",
    "cfg = k8s_client.Configuration()\n",
    "cfg.host = openshift_api_url\n",
    "cfg.verify_ssl = False\n",
    "cfg.api_key = {\"authorization\": f\"Bearer {token}\"}\n",
    "\n",
    "api_client = k8s_client.ApiClient(cfg)\n",
    "\n",
    "backend_cfg = KubernetesBackendConfig(\n",
    "    client_configuration=api_client.configuration,\n",
    ")\n",
    "\n",
    "client = TrainerClient(backend_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    torch_runtime = client.get_runtime(\"torch-distributed\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"Runtime 'torch-distributed' not found or not accessible\") from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T13:19:56.525591Z",
     "iopub.status.busy": "2025-09-03T13:19:56.524936Z",
     "iopub.status.idle": "2025-09-03T13:19:56.721404Z",
     "shell.execute_reply": "2025-09-03T13:19:56.720565Z",
     "shell.execute_reply.started": "2025-09-03T13:19:56.525536Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from kubeflow.trainer import CustomTrainer\n",
    "from kubeflow.trainer.options import PodTemplateOverrides, PodTemplateOverride, PodSpecOverride, ContainerOverride\n",
    "\n",
    "pvc_name = os.getenv(\"SHARED_PVC_NAME\", \"\")\n",
    "print(f\"[notebook] Using PVC: {pvc_name}\")\n",
    "\n",
    "job_name = client.train(\n",
    "    trainer=CustomTrainer(\n",
    "        func=train_fashion_mnist,\n",
    "        num_nodes=2,\n",
    "        resources_per_node={\n",
    "            \"cpu\": 2,\n",
    "            \"memory\": \"8Gi\",\n",
    "        },\n",
    "    ),\n",
    "    runtime=torch_runtime,\n",
    "    options=[\n",
    "        PodTemplateOverrides(\n",
    "            PodTemplateOverride(\n",
    "                target_jobs=[\"node\"],\n",
    "                spec=PodSpecOverride(\n",
    "                    volumes=[\n",
    "                        {\n",
    "                            \"name\": \"work\",\n",
    "                            \"persistentVolumeClaim\": {\"claimName\": pvc_name},\n",
    "                        }\n",
    "                    ],\n",
    "                    containers=[\n",
    "                        ContainerOverride(\n",
    "                            name=\"node\",\n",
    "                            volume_mounts=[\n",
    "                                {\"name\": \"work\", \"mountPath\": \"/mnt/shared\", \"readOnly\": False}\n",
    "                            ],\n",
    "                        )\n",
    "                    ],\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(f\"[notebook] Job submitted: {job_name}\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T13:20:01.378158Z",
     "iopub.status.busy": "2025-09-03T13:20:01.377707Z",
     "iopub.status.idle": "2025-09-03T13:20:12.713960Z",
     "shell.execute_reply": "2025-09-03T13:20:12.713295Z",
     "shell.execute_reply.started": "2025-09-03T13:20:01.378130Z"
    }
   },
   "outputs": [],
   "source": [
    "# Wait for the running status, then wait for completion or failure\n",
    "client.wait_for_job_status(name=job_name, status={\"Running\"}, timeout=300)\n",
    "client.wait_for_job_status(name=job_name, status={\"Complete\", \"Failed\"}, timeout=900)\n",
    "\n",
    "# Get job details and logs\n",
    "job = client.get_job(name=job_name)\n",
    "pod_logs = client.get_job_logs(name=job_name, follow=False)\n",
    "\n",
    "# Collect all log lines from the generator into a list\n",
    "logs = list(pod_logs)\n",
    "log_text = \"\\n\".join(str(line) for line in logs)\n",
    "\n",
    "\n",
    "print(f\"Training job final status: {job.status}\")\n",
    "\n",
    "# Check 1: Job status must not be \"Failed\"  \n",
    "if job.status == \"Failed\":\n",
    "    print(f\"ERROR: Training job '{job_name}' has Failed status\")\n",
    "    print(\"Last 30 lines of logs:\")\n",
    "    for line in logs[-30:]:\n",
    "        print(line)\n",
    "    raise RuntimeError(f\"Training job '{job_name}' failed\")\n",
    "\n",
    "# Check 2: Look for the training completion message in logs\n",
    "# This is critical because the training script may catch exceptions and exit 0\n",
    "if \"Training is finished\" not in log_text:\n",
    "    print(f\"ERROR: Training completion message not found in logs\")\n",
    "    print(\"Last 50 lines of logs:\")\n",
    "    for line in logs[-50:]:\n",
    "        print(line)\n",
    "    raise RuntimeError(f\"Training did not complete successfully - missing completion message\")\n",
    "\n",
    "print(f\"✓ Training job '{job_name}' completed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T13:20:24.045774Z",
     "iopub.status.busy": "2025-09-03T13:20:24.045480Z",
     "iopub.status.idle": "2025-09-03T13:20:24.772877Z",
     "shell.execute_reply": "2025-09-03T13:20:24.772178Z",
     "shell.execute_reply.started": "2025-09-03T13:20:24.045755Z"
    }
   },
   "outputs": [],
   "source": [
    "for c in client.get_job(name=job_name).steps:\n",
    "    print(f\"Step: {c.name}, Status: {c.status}, Devices: {c.device} x {c.device_count}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T13:20:26.729486Z",
     "iopub.status.busy": "2025-09-03T13:20:26.728951Z",
     "iopub.status.idle": "2025-09-03T13:20:29.596510Z",
     "shell.execute_reply": "2025-09-03T13:20:29.594741Z",
     "shell.execute_reply.started": "2025-09-03T13:20:26.729446Z"
    }
   },
   "outputs": [],
   "source": [
    "for logline in client.get_job_logs(job_name, follow=True):\n",
    "    print(logline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.delete_job(job_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
