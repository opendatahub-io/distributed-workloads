# Model
model_id_or_path: Meta-Llama/Meta-Llama-3.1-8B-Instruct
#tokenizer_name_or_path: Meta-Llama/Meta-Llama-3.1-8B-Instruct
#model_revision: main
torch_dtype: bfloat16
#attn_implementation: flash_attention_2
#use_liger: true
bf16: true                                # use bfloat16 precision
tf32: true                                # use tf32 precision

# Quantization / BitsAndBytes
use_bnb: false
load_in_4bit: true

# LoRA / PEFT
use_peft: true
lora_target_modules: "all-linear"
lora_modules_to_save: ["lm_head", "embed_tokens"]
lora_r: 16
lora_alpha: 8
lora_dropout: 0.05

# SFT
dataset_id_or_path: gsm8k                 # id or path to the dataset
dataset_config_name: main                 # name of the dataset configuration
#dataset_batch_size: 64                    # mini batch size
max_seq_length: 512                       # max sequence length for model and packing of the dataset
packing: true

# FSDP
fsdp: "full_shard auto_wrap offload"      # remove offload if enough GPU memory
fsdp_config:
  backward_prefetch: "backward_pre"
  forward_prefetch: "false"
  use_orig_params: "false"

# Training
num_train_epochs: 20                      # number of training epochs

per_device_train_batch_size: 8            # batch size per device during training
per_device_eval_batch_size: 8             # batch size for evaluation
evaluation_strategy: epoch                # evaluate every epoch

max_grad_norm: 0.3                        # max gradient norm
gradient_accumulation_steps: 1            # number of steps before performing a backward/update pass
learning_rate: 2.0e-4                     # learning rate
lr_scheduler_type: constant               # learning rate scheduler
optim: adamw_torch                        # use torch adamw optimizer
warmup_ratio: 0.1                         # warmup ratio
seed: 42

# Checkpointing
gradient_checkpointing: true              # use gradient checkpointing to save memory
gradient_checkpointing_kwargs:
  use_reentrant: false
save_strategy: "epoch"                    # save checkpoint every epoch

# Logging
logging_strategy: steps
logging_steps: 1                          # log every 10 steps
report_to:
- tensorboard                             # report metrics to tensorboard

output_dir: /mnt/nfs/runs/Meta-Llama-3.1-8B-Instruct
