# Special packages requiring special installation
# 
# flash-attn: Included as a transitive dependency from instructlab-training[rocm]
# in pylock.toml (version 2.8.3). No separate install needed.
#
# PyTorch 2.9's built-in scaled_dot_product_attention also provides
# flash attention support via SDPA (confirmed working on MI300X)
